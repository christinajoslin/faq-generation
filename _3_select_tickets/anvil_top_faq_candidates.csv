Cluster,Subcluster_ID,Num_Summaries,Num_Resolutions,All_Summaries,All_Resolutions
0,1,91,91,"User wants to test and potentially move projects to Purdue's Anvil CPU supercomputer due to concerns about Stampede3 system resources and waiting times. ||| The user's code on Anvil supercomputer stops running after 10 iterations, and there is no output in the file. ||| The user has reported a network connectivity issue with certain operations on Anvil back-end nodes using Infiniband and 1 to 1 NAT. ||| Submitted namd2 jobs on Anvil are failing due to a network issue. ||| User is unable to submit a final report properly and is facing an issue where the system declines the request despite using up all ACCESS Credits and 90% of their allocation. The user suspects that their supplement material may be confusing the reviewer, as they selected the 'supplement' option instead of finding a proper place to upload the document. ||| Error submitting vasp job due to an invalid account or account/partition combination specified ||| The user's code that uses BLACS has a segmentation fault on the first call of a subroutine of the suite. The issue occurs on Anvil but not on Stampede2, and it seems to be related to MPI-related errors in parallel IO/GPFS. ||| User is unable to submit batch jobs on Anvil due to an error related to account or partition combination. ||| User wants to know the details about the node-local storage on Anvil HPC system at Purdue, including file system, accessibility, and pathname. ||| The Anvil OnDemand interactive apps require a minimum of 128 cores to run, making it impossible to use them with less. This issue affects Jupyter Notebook, Desktop, Rstudio Server, Matlab, and possibly other apps as well. ||| User cannot access project resources (mch230015) due to lacking account permissions. ||| User is unable to create a PVC with RWX access mode using the ""anvil-block"" storage class on Anvil Composable due to multi node access modes not being supported. ||| User needs a reservation of 50-60 nodes on the Anvil wholenode partition and is unsure if it's possible or how to request one. The user also wants to know if there are alternative options like using the 'shared' partition. ||| The ""proxy-public"" service in the ""geoedf-jupyter"" namespace of project EES220056 requires a public IP for SSL certificate and DNS name configuration. ||| User Aashish is experiencing an error when running GROMACS simulations on Anvil using 864 cores and 36 nodes with a CPU-based version of GROMACS. The simulation works fine on a GPU workstation but not on the CPU. ||| Anvil back-end nodes cannot see hosts on the outside world ||| User Yuxuan is having trouble compiling Quantum Espresso (QE) version 7.2 with EPW 5.7.0 and the compilation failed some tests shown in /anvil/projects/x-dmr200031/software/q-e-qe-7.2/test-suite/history-test. They are seeking suggestions on how to resolve this issue. ||| User is unable to compile Fortran code using specified modules in Makefile ||| Anvil composable is unable to mount PVC (volume-shin152-40purdue-2eedu and jupyterhub-shared). ||| JupyterHub in Anvil k8s namespace, geoedf-jupyter, is experiencing trouble mounting volumes on a specific node (cloud07), causing notebook launch failures due to missing Ceph filesystem drivers. ||| Unable to transfer data from ANVIL to Negishi cluster using 'scp' command ||| Postfix service was broken during Anvil maintenance last week and unable to send mail using Anvil. ||| Error with NAMD CPU job submission in Anvil due to invalid account or account/partition combination specified. ||| Aashish is unable to receive job status updates via email after maintenance on the Anvil cluster. ||| Perl script fails to execute due to missing Statistics::Normality module on Anvil cluster. ||| Part of the group's allocation on Anvil expired unexpectedly, causing a loss of CPU-hours and affecting their remaining balance. ||| The user is unable to run LESGO software on the anvil-Purdue cluster and encounters an error. Additionally, the user cannot access the anvil since last evening due to a Proxy Error. ||| Anvil Purdue job status inconsistency ||| User is unable to run a GUI application (""deformetrica"") via remote desktop on Anvil due to an error related to QT and no window opening after running ""export qt\_qpa\_platform=offscreen"". ||| Unable to run the GUI application ""deformetrica"" on Anvil due to an error related to QT not being able to initialize a platform plugin. The issue persists even after trying to set the QPA platform offscreen and reinstalling the package within the conda environment. ||| User is experiencing problems with kubectl command to anvil composable. ||| The Anvil front-ends are currently configured to allow an unlimited amount of virtual memory, causing frustration for users when running programs exceeding available RAM, and potentially leading to performance issues or crashes. ||| User wants to access more nodes from the highmem partition of Anvil for a specific period (three weeks) to run large fluid dynamics simulations. ||| User is unable to run LESGO on Anvil-Purdue cluster and cannot access Anvil since last evening. ||| The user is unable to run an AMBER molecular dynamics simulation due to a ""command not found"" error for 'pmemd.cuda' and a module load issue for 'amber/20'. ||| Unable to log in and book RCAC coffee hour consultation slot due to login issue ||| Kubernetes (k8s) node has an issue where a Pod cannot mount or release Persistent Volume Claim (PVC), preventing the termination or creation of the Pod. ||| Init container for Kubernetes pod on Anvil fails to start due to a persistent volume access mode issue. ||| User wants to share a storage volume between two Kubernetes deployments in different namespaces (geoedf and geoedf-jupyter). They currently have a PVC (geoedf-staging) in the geoedf namespace, which they would like to make available across both namespaces. ||| The user is experiencing communication issues between their JupyterHub container in the geoedf namespace on Anvil composable and other containers running on Anvil via DNS names. Connection attempts time out but the user can access other internet locations. ||| The user is encountering an error when submitting a job on the cluster using MFiX software with Anaconda module. The error message indicates that some files and commands are not found. ||| The user is experiencing issues running a NAMD simulation in Anvil using the provided submission script, and they believe incorporating mpirun might enhance performance. ||| User is unable to see SUs they recently exchanged from ACCESS credits in their Purdue Anvil server account. ||| The Anvil Composable cluster on RKE is in an error state due to a failed health check with the API server. ||| The user is unable to SSH into node a198 on the HPC cluster Anvil at Purdue University due to an ""Access denied by pam_slurm_adopt"" error, and the job they were running was abruptly dropped. Additionally, their HOME directory is close to its storage limit. ||| Issues with Anvil resource, Execute shell on devnanohub exechost, and ssh connection to multiple hosts (128.211.162.15, 128.211.162.11) ||| User cannot access Anvil cluster after account creation and is unable to log in due to ""can't find user"" error. ||| Error creating tool session pod due to missing docker directories in /tmp. ||| User unable to submit a job on Anvil due to an invalid numeric value error in sbatch command caused by an extra space after --ntasks=. ||| User unable to access Anvil through SSH keys and requires guidance for submitting LAMMPS jobs. The user also needs clarification about working directories and shared storage spaces. ||| The user is unable to use the 'terra', 'raster' and 'sf' packages in R for geospatial data analysis on Anvil due to a missing 'libproj.so.15' library and issues with the Rstudio interface. ||| User is unable to run an interactive job on ANVIL using sinteractive command due to a read error -1 Connection reset by peer and possibly due to an issue with the connection between login node and compute node while trying to use gdb debugger in interactive mode. ||| Need to configure a deployment or pod in Rancher system on Anvil Composable to be reachable via hostname instead of IP address by other pods in the same namespace. ||| The user Nikhil is experiencing an issue with a batch script not executing on the Anvil cluster. ||| User Matin Mostaan is unable to submit VASP jobs on Anvil due to a missing vasp group membership. ||| Unable to login to ANVIL using ACCESS credentials and receiving an ""Connection closed"" error when attempting ssh connection. ||| All partitions on the Anvil cluster are currently down. ||| User cannot execute an rsync command on the Anvil Cluster due to a password prompt and is seeking assistance. ||| Researcher Brendan requires access to the highmem partition of Anvil (4 nodes) with a duration of three weeks for running large fluid dynamics simulations under allocation PHY230172. ||| Nodes in the shared partition of Anvil are stuck in the ""comp"" state, making them unusable. ||| User is unable to access the Anvil Training webpage provided in their welcome email from ACCESS. ||| Macs2 for peak calling fails on Jupiter notebook (ANVIL open on demand) with a numpy incompatibility error. ||| User requires the interactive capability for Anvil on Slurm version 24.05.3 which is affected by a default behavior change introduced in version 20.11, and they are unable to use 'srun' or 'mpirun' due to this issue. ||| The user is experiencing intermittent issues with using scRNAseq RStudio on Anvil on-demand dashboard. ||| User is unable to run batch jobs on Anvil cluster using a shared node as it is unavailable. ||| Home directory missing or unavailable on Anvil OOD at Purdue University. ||| Unable to submit a job on Anvil using matlab file due to ""Unable to open file myjobsubmissionfile"" error. ||| Simulations using MFiX software package on the Anvil cluster are returning ""Floating-point exception"" errors and crashing. ||| The user does not have a Bypass code for DUO authentication on their Access ID, and they are unable to log in. ||| User is running simulations using SUs on Anvil from the Explore project, has used up half of their allocation, and needs to request additional SUs for the second half. ||| The user's exchange credit allocation balance on the Purdue Anvil CPU is not being reflected correctly on the cluster. ||| Discrepancy between mybalance report and ACCESS website balance for allocation OTH220004 ||| User compiled CAMx model but encountered segmentation fault errors during runtime. The run script is located at /anvil/projects/x-atm130003/kiarash/camx/camxv7.2/bench\_inp/runfiles/CAMx\_v7.20.36.12.20160610-11.MPICH3.job\_v2, and the compiled code is at /anvil/projects/x-atm130003/kiarash/camx/camxv7.2/built/CAMx.v7.20.openMPI.NCF4.ifort. The log for the run can be found at /anvil/projects/x-atm130003/kiarash/camx/camxv7.2/bench\_inp/runfiles/slurm-4654227.out. ||| User encountered an error while running a parallelized simulation on Anvil HPC system with code located in /home/x-jsullivan1/WORK2/arepo. The error was related to the system architecture and occurred in the directory /anvil/scratch/x-jsullivan1/CosmoArkABHRuns/Run\_02\_05\_25\_1. The user had previously successfully compiled and run the code (in October of last year) that is now failing, suggesting the issue was not with the configuration or parameter files. ||| ACCESS project allocation (cis230083-gpu) is being unintentionally consumed by user Chapparapu despite him having been removed from the project and allocated a different account (cis240662). ||| The user is experiencing repeated failures when trying to run batch jobs on the Anvil cluster, with the error 'NODE\_FAIL' being reported and no output/error file generated in the scratch directory. ||| The network connection between nanohub.org and Anvil pods is slow for tool sessions, causing delays up to 5 minutes or more. The performance has improved over the past few days due to a gradual rollout of tool sessions, but it's unclear if the issue is related to the network or the exechost being overwhelmed. ||| The user is experiencing errors with the NodeExpandVolume for PVC ""pvc-fb06da00-c68c-497f-803b-a268aa4e9d2f"" when expanding the volume on nanohub-home. The error message indicates that the CSI plugin ""kubernetes.io/csi/rook-ceph.cephfs.csi.ceph.com"" doesn't support node expansion, but the issue does not seem to impact performance significantly. ||| User experienced ""Uncorrectable ECC error"" when training on node g-008 and suspects hardware failure. ||| The user is experiencing issues with Anvil, specifically with their balance not being corrected and missing jobs on the dashboard since last Friday. The user is unsure if the problem lies with Anvil or their account. ||| The SMB passthrough with QEMU is not working for the Windows 11 application on OOD, preventing access to the Anvil Home directory. This issue has been experienced before (see ATS-6916). ||| User wants to access Tecplot software on Anvil machine using a floating license. ||| Users 'arezvaniboroujeni' and 'kxu6' are unable to access the Purdue Anvil Open OnDemand dashboard, receiving an error message that the user does not exist. They wish to test Purdue Anvil and write their own ACCESS project. ||| User needs to transfer data from Anvil scratch space to NCSA Delta before their allocation expires and is unsure about access duration. ||| The user is unable to run a Slurm script due to the executable not being found in the correct directory. The executable is installed in two locations - home subdirectory and project subdirectory. The script is trying to find the executable in the home directory instead of the project directory. ||| User needs help to modify a script for execution in Anvil (SLURM environment) ||| User, Bindesh, requests a reservation of 256 nodes on Anvil for a high-resolution simulation of magnetohydrodynamic turbulence using a pseudospectral solver. The requested reservation time is 12 days, but they are flexible with shorter durations if needed. This calculation is urgent due to positive feedback from reviewers of their paper in _Nature_. ||| The user requires a reservation on Anvil Compute with 20 nodes and GPU with 2 nodes for a workshop. They plan to use scigap community logins, shared partitions, Cybershuttle Computing Continuum, and the Amber software (preferably updated to Amber22). The user is also experiencing issues with conda-forge block causing requirement downloads for their notebooks to hang/fail. Additionally, they are facing network connectivity issues on RCAC systems. ||| User inquired about availability of ANSYS Fluent software on Anvil HPC system ||| User is unable to access the `$PROJECT` directory on Anvil CPU due to a missing permission issue. ||| Back-end nodes on Anvil cannot access non-RCAC hosts (e.g., www.purdue.edu) using ping/ssh/curl/wget commands.","To access and run code on the Anvil supercomputer, the user should submit a Supplement for additional SUs (Stored Units). Follow these steps:
   - Log in to the ACCESS Home Page at https://allocations.access-ci.org
   - Click on ""Manage Allocations."" - Within ""Manage Allocations,"" click on ""Manage My Projects."" - Go to the List of ACCESS Allocation Requests page at https://allocations.access-ci.org/requests
   - Look for the Allocation you would like to take action on and click the ""Choose New Action"" button. - Choose the option: Supplement. - Submit a Progress Report describing how the PI's current allocation was used and summarizing any findings or results. - After submission, wait for reviewer's decision. If any questions arise along the way, submit a ticket at https://access-ci.atlassian.net/servicedesk/customer/portal/2 ||| To investigate this issue, the user should recheck their job IDs (if possible) and submit them for further analysis. Additionally, they are advised to verify that their code is correctly written to print at each iteration and ensure that the correct resource, Anvil, is being utilized. If the problem persists, the user can either reopen this ticket or submit a new one for further assistance. ||| To reproduce the problem, connect to an Anvil back-end node and run the following command:

```bash
while true; do nc -N -v retool.the-examples-book.com:443 < /dev/null ; date; sleep 0.3; done
```

The user suggests that network issues could be causing or contributing to ongoing GPFS (General Parallel File System) problems. It is recommended to verify the network configuration on affected Anvil back-end nodes and ensure they are functioning properly. ||| The error appears to be related to a recent network failure on the Anvil cluster at Purdue University. It has been reported that this issue was addressed yesterday. Users are advised to keep monitoring their job status, as the resolution seems to depend on the network stabilization. ||| To submit a final report, users should not use the 'supplement' option when it seems to confuse the reviewer. Instead, users are recommended to wait until they have used up about 90% of their Allocation and then reply to this ticket for further details on moving forward with the next ACCESS Opportunity. If users wish to move up now, they can initiate the graduation process by letting the team know. It's important to remember that SUs do not get carried over to the new allocation. For future questions or support, users can visit this site (https://access-ci.atlassian.net/servicedesk/customer/portal/2) and submit a ticket. ||| To resolve the issue, verify that you have correctly specified your account and partition in the sbatch command. The correct syntax should be as follows:

```bash
sbatch [options] -A <account> -p <partition> script.sh
```

Make sure to replace `<account>` with your account name and `<partition>` with the desired partition. If you are not sure about the account or partition, please consult the system documentation or contact your local administrator for assistance. You can also find more information in the [SLURM documentation](https://slurm.schedmd.com/documentation.html). ||| To resolve this issue, follow these steps:
   - Investigate the error messages by reproducing the code execution on Anvil. - Review the user's provided makefile, flags.mk, and the relevant files (fwi_merge1a.err, fwi_merge1a.out, merge_ata_matrix_mpi_io.c) to understand the code's behavior on Anvil. - Examine any changes made to the code for the openMPI version in comparison to the IMPI version to identify potential causes of the error. - If necessary, schedule a video call with the user to aid in debugging the issue if more assistance is needed. - Keep track of progress on this ticket and follow up as needed to resolve the segmentation fault and ensure the code works correctly on Anvil. ||| The user should check their current available allocations by running the command `mybalance` in the terminal while landed on Anvil. The support team has confirmed that the change in allocation has not been propagated to Anvil yet and suggests waiting for another couple of days to see if the process can be finished. If the issue persists after this time, the user may need to contact support again. Here is an example of how the command should be run:

```bash
mybalance
``` ||| The node-local storage is accessible under `/tmp` directory. It is flash and uses xfs as its file system. Users can access it during job execution on the node. The pathname for the user visible portion of node-local disk is `/tmp`. ||| To resolve this issue, modify the ""shared"" section of ondemand.anvil:/var/www/ood/apps/name/bc\_desktop/form.yml by adding the following lines:

```yaml
data-name-num-cores: 1
data-max-num-cores: 128
```

Similar changes should be made to the gpu and highmem sections, though you may want to adjust the minimums for each of those. It is recommended to apply similar changes to all other OnDemand apps to allow users to use fewer cores when needed. ||| The user needs to request the PI, name, to add them to the mch230015 project using this link: <https://allocations.access-ci.org/manage-allocations-overview>. The user can also refer to this site for future queries and ticket submission: <https://access-ci.atlassian.net/servicedesk/customer/portal/2>. ||| To resolve the issue, use the ""anvil-filesystem"" storage class instead of ""anvil-block"". The user should replace the StorageClass parameter in their PVC creation command with ""anvil-filesystem"". Here is an example:

```bash
kubectl apply -f pvc.yaml --namespace=my-namespace
```

In the `pvc.yaml` file, update the StorageClass to be `anvil-filesystem`. ```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
  namespace: my-namespace
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: anvil-filesystem
  resources:
    requests:
      storage: 1Gi
``` ||| To secure a node reservation, users can submit a form on Bridges2, but no similar functionality was found on Anvil. As an alternative, consider using the 'shared' partition which allows up to 1 node/per job with a maximum of 6400 cores (50 running jobs with 128 cores). Each job can run up to 96 hours. For more information about different partitions on Anvil, refer to: <https://www.rcac.purdue.edu/knowledge/anvil/run/partitions> and <https://www.rcac.purdue.edu/knowledge/anvil/run/partitions|smart-link>. The user has resolved the issue by switching to using the shared queue for now. ||| To obtain a public URL for your hub, follow these steps if you're using the zero to jupyterhub Helm chart:
   - Enable Ingress by adding `ingress:` with `enabled: true` in your `values.yaml`. - Configure the host by setting it as `hosts: - .anvilcloud.rcac.purdue.edu`. This will provide a public URL for your hub. ||| To resolve this issue, Aashish can try compiling and running GROMACS for GPU acceleration on Anvil using the following script as a starting point:

```bash
#!/bin/bash
#SBATCH -A mcb180049-gpu
#SBATCH -p gpu
#SBATCH -t 8:00:00
#SBATCH -N 1
#SBATCH -n 16
#SBATCH --gpus-per-node=1
#SBATCH --job-name=gromacs_test

module --force purge
module load modtree/gpu
module load ngc
module load gromacs

gmx grompp -f md.mdp -c npt.gro -t npt.cpt -p topol.top -o md.tpr
gmx mdrun -deffnm md -ntmpi 4 -ntomp 4 -nb gpu -bonded gpu
```

Additionally, Aashish is advised to test the `{-ntmpi}` and `{-ntomp}` options for their mdrun command and adjust the number of threads (MPI threads and openmp threads) as needed. It's also recommended to consult the GROMACS documentation for further assistance. ||| The networking team has been notified and they are currently working on this issue. A workaround has been put in place by the networking technicians. Compute nodes on Anvil should now be able to see the outside world. If further issues occur, please reopen this ticket. ||| To troubleshoot the compilation issue with QE 7.2, Yuxuan can try the following steps:
   - Ensure that all the required modules, such as `gmp`, `mpfr`, `zlib`, `libfabric`, `openmpi`, `modtree`, and `fftw` are loaded. - Verify that the commands used to compile QE 7.1 manually also work for QE 7.2:
     ```bash
     module purge
     module load modtree/cpu
     module load intel-mkl
     module load fftw
     wget https://www.quantum-espresso.org/rdm-download/488/v7-2/xxxxx/qe-7.2-ReleasePack.tar.gz
     tar -xzvf qe-7.2-ReleasePack.tar.gz
     cd qe-7.2
     ./configure
     make pw
     ```
   - If the compilation still fails, it would be beneficial to share the exact error message encountered during the compile process for further troubleshooting. Additionally, if Yuxuan wants to install QE 6.7 manually on Anvil, they can use Spack:
   ```bash
   module --force purge
   module load gcc/11.2.0 openmpi/4.0.6
   module load intel-mkl/2019.5.281
   module load fftw/3.3.8
   wget https://gitlab.com/QEF/q-e/-/archive/qe-6.7MaX-Release/q-e-qe-6.7MaX-Release.tar.gz
   tar -xzvf q-e-qe-6.7MaX-Release.tar.gz
   cd q-e-qe-6.7MaX-Release/
   ./configure
   make pw
   ``` ||| If using the Intel compiler, the appropriate module to specify in the Makefile is {{FC= mpiifort}}. For the GCC compiler, it should be {{FC = mpif90}}. The user may also want to ensure their batch submit file is set up correctly by following the guidelines provided in our user guide: <https://www.rcac.purdue.edu/knowledge/anvil/run>. To load either intel or gcc compiler, and/or impi or openmpi if a MPI job is required, it is recommended to use {{module load ...}} after performing a {{module purge}} in the user's job script. ||| To resolve the issue, attempt to manually mount the volumes using the following command on the cloud07 node:

```bash
kubectl attach volume claim/volume-shin152-40purdue-2eedu --namespace=jupyterhub -it --container=<anvil-composable-container-name>
```

Repeat the command for jupyterhub-shared. If successful, you should be able to interact with the mounted volumes within the container. It is also recommended to check the status of other storage system resources and ensure they are functioning correctly. Refer to the Rancher dashboard for more details on the system's state. ||| Investigate the issues with the specific node (cloud07) mentioned in the ticket. Focus on resolving the trouble mounting volumes issue that is preventing JupyterHub from attaching and mounting volumes for the namespace, geoedf-jupyter. This problem seems to stem from missing drivers for the Ceph filesystem. Attempt to resolve this by checking the required k8s system drivers are installed on node cloud07. If not, install the necessary Ceph drivers to enable volume mounting for the JupyterHub setup. Additionally, consider monitoring the system logs on node cloud07 to identify any further potential issues that may be causing this behavior. ||| The user is experiencing issues with transferring data from ANVIL to the Negishi cluster using the command `scp -r : mailto::/anvil/scratch/x-mrahman2/CdTe\_HSE/HSE06 .` To resolve this issue, make sure that the user's ssh keys are properly configured on both ANVIL and Negishi cluster. Additionally, ensure that the path to the data being transferred is correct. If the issue persists, consult the respective documentation for each cluster regarding scp usage:
   - ANVIL: <https://www.anvil-project.org/documentation/using-scp/>
   - Negishi Cluster: <https://docs.rcac.purdue.edu/negishi/using/transferring-data> ||| Edit the /etc/postfix/main.cf file and ensure that the 'setgid\_group' parameter is set to a different group ID than postdrop and postgres. For example, if the desired group ID is 9001, update the following line in the main.cf file:

```
setgid_group = 9001
```

Save the changes and restart the Postfix service using the following commands:

```
sudo systemctl restart postfix
```

After applying these steps, you should be able to send emails using Anvil again. If the issue persists, please reopen this ticket or create a new one for further investigation. ||| Add the following line to your job script: `{{#SBATCH -p wholenode}}` This command specifies that you wish to use a whole node, which may be required for certain partitions. ||| The issue was escalated to the engineering team for a thorough examination and a fix has been implemented. The ticket will be resolved. ||| To resolve the issue, install the missing Perl module (Statistics::Normality) in your home directory or project space on the Anvil cluster. You can refer to the official installation guide at this link: [Perl Module Installation Guide](https://www.cpan.org/modules/INSTALL.html). If you have further questions, please do not hesitate to contact us again. ||| The support team extended the part that expired on December 31, 2023 until March 31, 2024. To verify the updated allocation for the account phy130027, run the following command `mybalance x-btripathi`. This command displays the current allocation details for the specified user. The updated allocation type name limit and balance should now be as follows:

   - Allocation Type: CPU
   - name Limit: 62464243.9
   - name Usage: 61926637.1
   - name Usage: 37335080.0 (updated)
   - Imbalance: 537606.8
   - Account (account): phy130027 (user): Btripathi ||| To resolve the issue with LESGO not running, examine the attached log file for error messages. Investigate the compiler settings used during the software compilation to ensure they are compatible with the cluster environment. If necessary, recompile LESGO with updated or alternative compiler options. For assistance with this step, consult the LESGO documentation or seek help from the cluster support team. To address the Proxy Error preventing access to the anvil-Purdue cluster, check your proxy configuration settings to ensure they are correctly configured for the current environment. If you have recently updated your proxy configuration, verify that it is functioning properly. If needed, contact your system administrator or IT department for assistance with your proxy setup. Include relevant log files, job scripts, and any other pertinent information in your ticket response to help expedite the resolution process. ||| The user experienced an issue where a submitted molecular dynamics simulation job on Anvil Purdue was showing an inconsistent status. It appeared to have started only two hours after the initial submission, despite being run for over nine hours the previous day. This inconsistency occurred due to job manager issues on Wednesday, 2/27. The problem has been resolved by our engineers. ||| Reinstall the ""deformetrica"" package within the user's conda environment or check if there are missing libraries that need to be installed for the application to work properly. Additionally, consult the manual of the application for any discussions regarding QT plugin. Here are the commands needed to launch the software without its GUI:
```bash
module load anaconda
conda activate deformetrica
deformetrica command_to_run_without_GUI
``` ||| It is recommended to ensure that all necessary libraries required for the QT platform are present in the environment. If not, they should be installed. However, if the GUI is not a requirement, the software can still be run without it. If the problem persists after installing the required libraries, re-installing the ""deformetrica"" package within the conda environment might resolve the issue. It is also advised to check the manual of the application for any discussion about QT plugin issues. ||| The issue was caused by an intermittent problem with one of the API servers. It should have been resolved now. No action required for the user. ||| To address this issue, it is recommended to limit the amount of virtual memory available to users. To test this change, you can run the command `stress -m 1 -t 30 --vm-bytes 150G` and observe the system behavior using tools like `top` and `vmstat`. This command should cap at 12GB of RAM and page furiously. To fix the issue, systemd needs to be restarted for the new limits to take effect. However, it seems that this was supposedly addressed during the last maintenance. If the problem persists, please report it again for further investigation. ||| The user has been granted access to the QOS 'wide-highmem' which allows them to request up to 16 nodes from the highmem partition. To use this QOS, the user needs to add `{-q wide-highmem}` into any of their Slurm options. It is also mentioned that a maximum time limit of 2 days per run applies on this partition. The user can check this QOS limit using the following command:

   ```
   $ sacctmgr show qos wide-highmem format=Name%,GrpSubmit,MaxTRES%20,MaxTRESPerNode%20,MaxWall,MaxTRESPU,MaxJobsPU,MaxSubmitPU,MaxTRESPA,MinTRES
   ```

   The output of the command will be:

   ```
   Name               GrpSubmit MaxTRES       MaxTRESPerNode     MaxWall  MaxTRESPU MaxJobsPU MaxSubmitPU MaxTRESPA MinTRES
   wide-highmem        cpu=2048,node=16     2-00:00:00 cpu=2048,nod+16    32 name
   ``` ||| The user was advised to try running LESGO with ""mpirun"" instead of ""srun"". It is also recommended to check the compilation method used for the application and the modules loaded, as this information might be relevant in resolving the issue. The user reported that they have resolved the error by themselves after trying with ""mpirun"". ||| To resolve this issue, the user should check if the GPU-enabled version of AMBER (amber/20) is correctly loaded using the following command:

```bash
module load amber/20
```

After loading the module, they can try to execute the 'pmemd.cuda' command again. If the issue persists, it may be necessary to verify that the GPU-enabled version of AMBER is available on the system and properly configured for use with SLURM. For more information, consult the documentation or contact support for further assistance. ||| To access the RCAC coffee hour, it is required to use Purdue credentials. However, if you require a consultation meeting, consider registering for the Anvil Support Hour at this link: [Anvil Support Hour Registration](https://www.rcac.purdue.edu/anvil/anvil-support-hour) |smart-link This is designed for users of the Purdue community clusters (not including Anvil). If you are an Anvil user, this would be more suitable for your needs. ||| To resolve the issue, it is necessary to investigate the specific composable name mentioned in the ticket message. Here are some steps that can be taken:

a) Examine the event logs provided in the attachment for error messages related to the PVC mounting or unmounting process. These logs might provide clues about the underlying problem, such as permissions issues, incorrect configuration settings, or software bugs. b) Verify if there are any known issues with the Kubernetes version being used on the affected node by checking its release notes or consulting the relevant documentation. If a known issue exists, follow the provided workarounds to resolve it. c) Ensure that the PVC is correctly configured and bound to the Pod. Check if the correct access modes (ReadWriteOnce, ReadOnlyMany, etc.) have been specified for the PVC and the Pod's volume mount. Make sure that the storage class associated with the PVC supports the desired access mode. d) If needed, consult the official Kubernetes documentation for guidance on troubleshooting issues related to mounting and unmounting PVCs: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes

e) Perform a test to recreate the issue by creating a new Pod with the affected PVC and verify if it can successfully mount and unmount the PVC. If the issue persists, consider whether there are any local configuration issues on the affected node that might be causing the problem. f) In some cases, it may be helpful to temporarily disable dynamic volume provisioning (if enabled) and create a manually-provisioned PVC for the affected Pod. This can help determine if the issue is related to the automatic provisioning process itself. ||| The issue appears to be related to multiple pods trying to access the same persistent volume with ReadWriteOnce (RWO) access mode, which does not work well in this scenario. To resolve the issue, try restarting your toolsession pods after changing the persistent volumes' access mode to ReadWriteMany (RWX). However, it seems that you cannot change the accessMode of existing PVCs directly. Instead, recreate the affected PersistentVolumeClaims (PVCs) and reload the data to apply the necessary changes. ||| The user can implement an NFS server pod that attaches the PVC (geoedf-staging) and makes it accessible from multiple namespaces. Here's a high-level outline of the steps to achieve this:
   - Deploy an NFS server in one of the namespaces, e.g., geoedf-jupyter. You can use an existing Helm chart or create your own with the necessary configuration for an NFS server. Ensure the PVC (geoedf-staging) is attached to this pod. - After deploying the NFS server, users in both namespaces should be able to connect to that NFS server using their respective client applications (e.g., Jupyter notebook or web portal). To do so, use an NFS mount command within your Kubernetes pods:

```bash
mount -t nfs <nfs_server_ip>:/<nfs_share> /path/to/local/mountpoint
``` ||| To resolve this issue, the network policies need to be disabled for allowing communication from singleuser pods to other k8s pods in the same namespace. This can be done by setting `enabled` to `false` in the `networkPolicy` section of the user's `values.yaml`. Example:

```yaml
networkPolicy:
  enabled: false
``` ||| To resolve this issue, the user can try switching to the older version of Anaconda (2021.05-py38) by loading it directly using the following command:

   ```
   module load anaconda/2021.05-py38
   ```

   After loading the older Anaconda version, the user may encounter a new error related to the MFiX environment not being found. To list all discoverable environments, use the following command:

   ```
   conda info --envs
   ```

   If necessary, create or activate the desired MFiX environment and make sure that `./configure` and `build_mfixsolver` are accessible in the user's path. For more detailed assistance, it may be beneficial to discuss this issue over a virtual meeting through Purdue University's Anvil Support Hour: [Anvil Support Hour](https://www.rcac.purdue.edu/anvil/anvil-support-hour) ||| Incorporate mpirun for NAMD simulations in the Anvil submission script by modifying the executable line as follows:

```bash
module load ""${NAMD_MODULE}""
mpirun --mca btl_openib_allow_ib true -np 8 namd3 +devices 0 complex_solv_ion-PROD-0.conf > complex_solv_ion-PROD-0.out
```

It is important to note that using mpirun in this manner will set the number of processes (np) to 8, which might impact the performance of your simulation. For optimal results, it would be best to experiment with various process numbers and find the ideal configuration for your specific use case. Additionally, the NAMD you are using on Anvil is for single node only. If you wish to discuss optimizing your calculations workflows or code performance, I would recommend contacting MATCH program at https://support.access-ci.org/match/overview. ||| To resolve this issue, follow these steps:
   - Log into the Purdue Anvil service with your credentials. - Run the command `modules avail SU` to list available Software Units (SUs). - If the newly exchanged SUs are not listed, try running the command `module refresh` to update the module database. - If the issue persists, contact the Purdue Anvil support team for further assistance. For more information, refer to the [Purdue Anvil documentation](https://www.rcac.purdue.edu/documentation/anvil). ||| To resolve this issue, it is recommended to perform a Kubernetes (K8s) node restart or reinstallation. Here are the steps to perform a node restart:

   - Identify the problematic node from the cluster using the command `kubectl get nodes`. - Once identified, use the following command to drain the node of its pods and mark it as unschedulable:

     ```
     kubectl drain <node-name> --delete-local-data --ignore-daemonsets
     ```

   - After draining, you can safely stop the node. - Once the node is back online, rejoin it to the cluster using the command:

     ```
     sudo rke up <node-name>
     ```

   - Lastly, use the following command to check if the node is back in the cluster and ready to schedule pods:

     ```
     kubectl get nodes
     ```

   If the issue persists after trying these steps, consider performing a K8s node reinstallation. For more detailed instructions on this process, please refer to the official Rancher Kubernetes Engine (RKE) documentation here: https://rancher.com/docs/kubernetes/en/latest/cli/ ||| - To resolve the SSH issue, it seems that the user does not have any active jobs on node a198. To access the node again, submit a new job on the same or another available node using the SLURM batch system. For instance, you can use the following command to submit a job:

```
sbatch your_script.sh
```
   - Where `your_script.sh` is the script containing your workload. - To avoid the ""disc exceeding"" error due to the almost full HOME directory, the user should clear up some space in their HOME by deleting unnecessary files or transferring data to another storage location like scratch. If necessary, check the quotas and limits for both home and scratch spaces using the following command:

```
quota -u <username>
``` ||| - First, verify if the issue is with the composable name as suggested by Application Engineer for Scientific Computing. If necessary, update the composable name accordingly. - Since issues with the cluster were resolved earlier, check if these resolutions have addressed the current problem(s). - To facilitate communication during the workshop, consider using email or Teams for direct messages instead of the ticketing system if it provides a better response. - If the issue with Harbor persists, contact name directly for further assistance. ||| To resolve the issue, it appears that the user's information might not have been properly set up on the Anvil cluster. You should run the following command to confirm your username and check if it has been added correctly:

```
nginx\_stage --help | grep -i user
```
If your username is not listed, please contact the system administrator for assistance in setting up your account on the Anvil cluster. For more information about using the Anvil cluster, you can refer to the official documentation: [Anvil Cluster User Guide](https://example.com/anvil-cluster-user-guide). ||| The issue was caused by a cleanup script that was wiping some docker directories on the batch nodes (converted to k8s nodes). To resolve this, the script has been modified to exclude /tmp/docker. It's recommended to use /tmp/docker instead of /var/lib/docker as /tmp is the only mounted filesystem on the node. The OS and associated directories just run in a rootfs memory filesystem. ||| Modify the script as follows:
```bash
#!/bin/sh -l
#SBATCH --account=cis220065
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --partition=standard
#SBATCH --time=24:00:00
#SBATCH --mem=40GB
module load geos
```
Then, submit it with `sbatch gen_gdd.sub`, which should resolve the problem. ||| - To set up the SSH key for Anvil, remove all contents of /home/x-abdlmwla/.ssh/. * and then copy and paste your public key into /home/x-abdlmwla/.ssh/authorized_keys. This should solve the issue with password authentication. For more information, refer to the user guide: https://www.rcac.purdue.edu/knowledge/anvil/access/login/sshkeys

- The user has access to three directories:
	+ /home/x-abdlmwla with 25GB of space. Try using this space for setting up your environment, etc. Ensure this directory has some space; otherwise, you'll have problems connecting to OnDemand. + /anvil/projects/x-mss190008 with 5TB of data. This space is shared with group members. + /anvil/scratch/x-abdlmwla with 100TB of space. Remember our purge policy: files older than 30 days will be purged. Use this as temporary space for up to 30 days. For more information, visit https://www.rcac.purdue.edu/index.php/knowledge/anvil/storage/filesystems

- For submitting LAMMPS jobs, please visit our user guide at https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/lammps/lammps_job_submit_script. Try the sample script and modify it as needed. ||| To resolve this issue, the user should load the necessary modules in the terminal before opening RStudio:
   - Open the Desktop Interactive session and run the following commands:
     ```
     $ module load gcc
     $ module load gdal
     $ module load proj
     $ module load r
     $ module load rstudio
     ```
   - After loading the modules, open RStudio and attempt to install the 'sf', 'raster', and 'terra' packages. - If the dependencies 'udunits2' and 'sqlite3' are required, load these modules first:
     ```
     $ module load udunits/2.2.28
     $ module load sqlite/3.35.5
     ``` ||| The user should try running the interactive job again on ANVL after verifying if there are still issues with Anvil's storage system. If the error persists, they can attempt the following steps:
   - First, unload any modules using the command `module purge`. In this case, the user specifically mentioned that the module xalt/2.10.45 was not unloaded, so they should use the command `module --force purge xalt/2.10.45` to forcibly unload it. - Next, try running the sinteractive job again with the necessary options and parameters: `sinteractive -N1 -n32 -A ear170001`. - If the issue still persists, they can consult the Anvil documentation for troubleshooting tips or contact their local support team for further assistance. ||| To achieve this, you should create a ClusterIP service and map the required port(s) for access from the Pod. Once done, you can connect to the Pod using the ClusterIP name instead of the IP address. Here's an example of how to define a ClusterIP service in YAML:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-db
spec:
  selector: app=my-db
  type: ClusterIP
  ports:
    - name: postgresql
      port: 5432
      targetPort: 5432
```

Apply this configuration using the `kubectl apply -f my-db.yaml` command. Replace `my-db.yaml` with the actual file path to your YAML file containing the service definition. You can then access the Pod from other pods in the same namespace using the ClusterIP name, e.g., `psql -h my-db`. ||| To resolve this issue, contact our engineer for further investigation. In the meantime, Nikhil should verify that he has access to PHY24006 account by checking his account balance using the `mybalance` command:

```bash
@login01.anvil:~ $ mybalance x-nbisht1
```

If access is still not granted, consult the support team for assistance. Ensure that you have the correct account and allocation (PHY24006) in this command. ||| To resolve this issue, add user 'Matin Mostaan' to both {{vasp5}} and {{vasp6}} unix groups on Anvil. After the membership has been updated, refer to the RCAC Purdue University Anvil User Guide about how to submit VASP jobs on Anvil (<https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp>). ||| Check the ANVIL login again as there was an issue with the account that has since been resolved. No action is needed for adding the public key manually, as it seems that the original issue has been addressed. ||| The scheduling on the Anvil cluster was paused for approximately 3 hours due to an issue. It is now expected to be back to normal operations. The user is advised to check if their jobs are running as usual. If not, they may need to resubmit their jobs. Apologies for any inconvenience caused by the lack of communication during this maintenance period. ||| To resolve the issue, update the rsync command with the '-e' option followed by a modified ssh command that uses the specified local ssh key file for authentication. The correct command would look like this (replace 'ssh_key_name' with the name and path to your local ssh key file):

```
rsync --progress -r -e 'ssh -i ~/.ssh/ssh_key_name' -u *:/anvil/scratch/x-aglcher/StagYY/ . ``` ||| - To set up the requested allocation, update the qos (quality of service) as follows when submitting jobs with PHY230172:
     ```
     {{#SBATCH -q wide-highmem}}
     ```
   - The reservation will end on Oct 8, 2024. - The researcher needs 8 nodes at a time. If the current rules for wide-highmem (up to 16 nodes and 2 days) are sufficient for his jobs, keep those rules. Otherwise, create a new rule if necessary. - Ensure that any existing jobs running under allocation mch220029 using wide-highmem are changed accordingly. ||| To resolve this issue, it is recommended to first down the affected nodes using the following command: `scontrol update NodeName=<node_name> State=down Reason=name_proc`. After that, resume the nodes with the following command: `scontrol update NodeName=<node_name> State=resume`. This process can be automated by creating a script similar to the one provided by Doug. The script iterates through all affected nodes and applies the down and resume commands. To create this script, replace `<node_name>` with each individual node name in the loop. A sample of the script is provided below:

```bash
for i in $(sinfo -p shared -t COMP -o %n | tail -n +2); do
  echo scontrol update NodeName=$i State=down Reason=name_proc;
  echo scontrol update NodeName=$i State=resume;
done
``` ||| Provide user with the correct training link, which is <https://www.rcac.purdue.edu/training/anvil101> and confirm that it works as expected with two videos available on the page. ||| To resolve the issue, follow these steps:

- Ensure that your Python environment is using the correct version of NumPy compatible with Macs2. To check the current version of NumPy installed, run `pip show numpy`. If it's not 1.19.5 or higher, you may need to update it. To upgrade NumPy, use the following command:
```bash
pip install --upgrade numpy==1.19.5
```
- After updating NumPy, try running Macs2 again with the provided commands in your Jupiter notebook cell. If the issue persists, consider creating a new environment specifically for bioinformatics analysis and ensure that both Biocontainers (Macs2 and NumPy) are installed there. ||| The user should contact the site support team at Florida State University and ask them to consider enabling LaunchParameters=use_interactive_step in Slurm configuration. This will allow for interactive parallel capability within the Anvil environment. For more information about this behavior change, please refer to the following link: <https://slurm.schedmd.com/faq.html#prompt>. ||| Refresh the page in your web browser when encountering this issue to reattempt using the scRNAseq RStudio on Anvil on-demand dashboard. If further problems persist, please try again at a later time as it seems that the job scheduler has been experiencing minor faults recently. ||| To assist the user in running batch jobs on the Anvil cluster, first, ensure that the shared node is available. If it's not, consider using a different node or queue for job submission. Below is an example of an sbatch file for submitting a job on the Anvil cluster using the qsub command. ```bash
#!/bin/bash
#SBATCH --job-name=my_job
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --time=0:30:00
#SBATCH --partition=shared
#SBATCH --output=%x-%j.out

# Insert your job commands here
echo ""Hello, this is my batch job!"" ```

In the sbatch file above, `--nodes=1`, `--ntasks=1`, and `--time=0:30:00` specify the number of nodes, tasks, and time limit for the job. Replace `my_job` with a more descriptive name for your specific job. The `--partition=shared` option is used to submit the job to the shared partition on Anvil. Finally, `--output=%x-%j.out` specifies the output file for the job's standard output and error messages. ||| To resolve the issue of a missing home directory on Anvil OOD, follow these steps:
   - Log in to your account on Anvil OOD. - If your home directory is not found, it may need to be created. You can create your home directory by using the command `mkdir ~`. - Ensure that the home directory mount is available. If it's not, contact support for assistance in making it available. - Once your home directory is available or created, try restarting your web server. ||| The issue is related to the filename extension. The user's filename has a '.txt' extension, and it should be added at the end of the filename for proper job submission. The corrected command would look like this: `sbatch --nodes=1 --ntasks=1 myjobsubmissionfile.txt`. ||| The issue was caused by an issue with the deployed MPI and UCX on Anvil. A fix has been deployed. Users experiencing this issue should try running their simulations again. If further issues arise, reach out for assistance. ||| The user needs to enroll in DUO with a mobile device or computer, install the DUO App, and configure authentication to use DUO Push. They can find the steps for enrollment at this link: [https://operations.access-ci.org/identity/manage-mfa](https://operations.access-ci.org/identity/manage-mfa). It was noted that there is no enrollment in ACCESS DUO found for 'lpetrus' as of 2025-01-15. ||| The user can submit a Supplement for the second half of their ACCESS Credits for the Explore Allocation (EES240132). They should provide a Progress Report that describes how the PI's current/previous allocation was used and summarizes the findings or results. - To do this, follow these steps:
  1. Log in to allocations.access-ci.org. 2. You'll see your project information, or a list of your projects if you have multiple. 3. Click on the ""Credits + Resources"" tab. 4. Click ""Request More Credits."" 5. A ""Manage Your Project"" box will then appear. 6. Click ""Request a Supplement."" 7. Provide the reason you need additional credits. 8. Select ""ACCESS Credits"" as the available resource. 9. Upload your progress report. 10. Submit the form. 11. You will receive an email notification confirming your successful request. Once a decision has been made, you will receive a separate notification. - For detailed instructions, visit: https://allocations.access-ci.org/how-to#request-more-credits
- If the user has any questions in the future, they can submit a ticket at: https://support.access-ci.org/help-ticket ||| To resolve this issue, first, ensure that the updated allocation has been received by the system. If it hasn't, resend the allocation AMIE package to Halcyon for processing (command or steps not specified). After verifying that the allocation has been sent, check the cluster balance again. You can view the current allocation by logging in to the cluster and running the command `ssh username@cluster_address` followed by the command `balance account name`. The account name will be replaced with the appropriate account name (e.g., che240218). If the issue persists, reach out to the support team for further assistance. ||| The issue is due to a bug in the usage reporting process (ATS-5527). It has been confirmed that the name usage reported by the local command `mybalance` should be correct. Users are advised to use this command for accurate balance information. For more details, please refer to the linked communication: [ATS-5527](https://access-ci.atlassian.net/browse/ATS-5527) ||| The exact resolution for this issue is not provided in the ticket message. However, it's escalated to our scientists for further investigation and they will contact the user with a solution when available. In the meantime, users may find helpful resources on the following links related to segmentation fault errors and CAMx compilation process:
   - [CAMx Documentation](http://www.camx.com/resources-documentation)
   - [IFORT Compiler Documentation](https://software.intel.com/content/www/us/en/develop/documentation.html)
   - [OpenMPI Documentation](https://www.open-mpi.org/documentation/)
   - [Libc Documentation](https://man7.org/linux/man-pages/man3/malloc.3.html) ||| The issue appeared to be resolved as the user recompiled and re-ran their code today, and it now seems to be running smoothly. It is unclear if this was related to maintenance or something with the machine. The user did not provide specific details on their workflow for reproducing the error. If the user encounters further issues or requires more assistance, they should reach out again. ||| The issue was resolved by fixing a backend problem that allowed user Chapparapu to continue consuming the allocation's SUs after being removed. The system has been updated, and Chapparapu has been successfully removed from the cis230083-gpu allocation. A refund of 201 SUs consumed by this user post removal has also been processed. It is recommended to monitor the system for any further issues and take necessary actions if concerns arise. ||| The issue experienced by the user is related to a recent unscheduled outage of the Anvil cluster, which caused the 'NODE\_FAIL' error. As the outage has been remedied, the user should now be able to run batch jobs successfully. ||| To optimize the network for faster user performance, consider the following steps:
   - Monitor CPU usage on the exechost and adjust resource allocation as necessary to prevent overloading. - Gradually roll out tool sessions to manage user load and avoid overwhelming the system. This can be done by distributing tool session creation among groups of 10 to 15 students. - Investigate potential network bottlenecks. If needed, consult with networking experts or refer to relevant documentation available at http://nanohub.org for guidance on troubleshooting network performance issues. ||| To resolve this issue, the user should restart all pods that are using this problematic PVC. This seems to be a known bug in the current version of Rook being used. After restarting the pods, the volume should properly resize and the NodeExpandVolume errors should stop appearing. ||| Investigate node g-008 for possible hardware failure. If the issue persists, consider requesting a replacement or repair of the node to resolve the ""Uncorrectable ECC error"". ||| To investigate the issue, first, check your account balance and job status on the Anvil dashboard. If the problem persists, contact Purdue IT for further assistance. For general inquiries and support, you can refer to the [Purdue IT Service Portal](https://service.purdue.edu/). If you need immediate assistance, re-open this ticket within the next 7 days if required. ||| To reproduce and troubleshoot the issue, follow these steps:
   - Launch the ""Windows 11 Professional"" VM on https://ondemand.anvil.rcac.purdue.edu:
   - Enter the password ""rcacuser23""
   - Double click on the Anvil Home directory shortcut
   - The error can be observed if access to the home or scratch directory shortcuts is not working as expected. - Please note that issue ATS-6916 might provide some useful information regarding this problem. - Once the team has fixed the problem, please try again and let them know if you need more help. ||| To access Tecplot software on Anvil using a floating license, confirm that your Tecplot license is a floating one which allows Anvil to connect. If your current license is on a local computer with private IPs, Anvil may not be able to connect to it. ||| Check if the users can log in to their respective accounts on Purdue Anvil using their assigned usernames 'x-arezvaniboroujeni' and 'x-kxu6'. If they are still unable to access the dashboard, have them confirm that they are entering the correct username format. If the issue persists, contact the support team for further assistance. ||| The user can use Globus to transfer files from Anvil to the new allocation on NCSA Delta. It's important to note that older files in the Anvil scratch space (older than 30 days) will be removed automatically according to the purge policy. Users are advised to refer to the following links for more information about Anvil's filesystems and storage:
- https://www.rcac.purdue.edu/knowledge/anvil/storage/filesystems
- https://www.rcac.purdue.edu/knowledge/anvil/storage/scratch ||| To resolve this issue, the user should ensure that the executables (e.g., `gamess.00.x`, `ddikick.x`) are located in their project space and not just their home directory. The user should also explicitly export the paths in their job script as follows:

```bash
export PATH=/path/to/your/gamess/bin:$PATH
export LD_LIBRARY_PATH=/path/to/your/gamess/lib:$LD_LIBRARY_PATH
```

The user is also advised to drop by the *Anvil Support Hour* if anything is unclear. ||| The user should review the provided documentation on SLURM job scripts and submission methods at [https://www.rcac.purdue.edu/knowledge/anvil/run](https://www.rcac.purdue.edu/knowledge/anvil/run). They are advised to convert their PBS script into a SLURM script and reach out if they encounter any issues during this process. ||| To accommodate the user's request for running two jobs simultaneously during the reservation period, modify the SLURM job submission script (if possible) by removing or adjusting the limit of one job per submission using ""#SBATCH -q btripathi"". If this is not feasible, the user can submit one job at a time. The SLURM job submission script provided by the user should work as it did when 512 nodes were reserved in the past, using the ""_wholenode_"" partition. The user also requests guidance on any special considerations required for this reservation. ||| To resolve the issue, first, create a reservation on Anvil Compute as requested with 20 nodes and GPU with 2 nodes. Ensure that the scigap community logins and appropriate shared partitions are set up for the user. Configure Cybershuttle Computing Continuum for the Anvil reservation. If possible, update Amber to Amber22 on Anvil for the workshop. For the network connectivity issue on RCAC systems, inform the user that engineering team is working on it and apologize for any inconvenience this may have caused for their workshop. Provide them with a link to the latest news about the issue: https://www.rcac.purdue.edu/news/7096

Lastly, address the conda-forge block causing issues with the user's notebook downloads by requesting that the block be lifted as soon as possible. ||| ANSYS is not available on Anvil as it is a nationally shared resource (NSF ACCESS) requiring a commercial license, and thus cannot be provided as requestable software on a shared national HPC system. Users with a valid ANSYS license can only use it on one of the Purdue Compute Clusters. ||| Run the following command to navigate to the project directory:

    ```
    cd /anvil/projects/x-phy250136
    ```

   This issue was caused by an incorrect project group association that has now been resolved. You should be able to access the directory without any issues. If you still encounter problems, please reach out again for further assistance. ||| The issue was resolved by the engineer team. To test if the problem has been fixed, try running the following command on a back-end node to ping www.purdue.edu: `ping www.purdue.edu`. If it works properly, you should see a response from the server and no packet loss."
3,1,28,28,"The user is experiencing issues with their access to the Purdue Anvil machine's Job Composer and Shell Access. The issue started around 3pm EDT on Tuesday and has persisted despite being able to log in through the dashboard. ||| User cannot login to Anvil for award MCH220017 because they are unable to use the ssh command due to not having set up their ssh keys. ||| User cannot access TAMU FASTER and Purdue Anvil GPU portals due to ""failed to map user"" error. ||| Unable to map user account during first-time login to Pursue Anvil CPU onDemand ||| User is unable to map user account when logging into ANVIL using ACCESS account after DUO verification ||| User cannot assign users or see the allocated resources under ""Manage Users"" on Purdue Anvil CPU, and is unable to log in due to an error message. ||| The user's allocation transfer from ACCESS to Purdue Anvil under dmr160063 isn't being processed, resulting in incorrect usage information on both portals and a balance showing as n/a on Purdue Anvil. ||| User (x-tyokokura) unable to login to Purdue Anvil due to user not found error. ||| Unable to log into Purdue Anvil system with access ID x-yge due to an error related to the Unix group associated with the project PHY240119 ||| User unable to login to Purdue Anvil cluster via Open OnDemand due to non-existence of their username on the cluster. ||| User is unable to log into Anvil Open OnDemand due to account propagation issues. ||| The ""myquota"" command on Anvil is returning a syntax error for user x-jbamber. This error is caused by issues with line 37 and 38 of the user's .bashrc file. ||| Error while logging into Purdue Anvil On Demand with username x-abernard ||| User unable to log in to Anvil GPU with usernames ""aanand3"" and ""anand211"", keeps receiving error messages as failed to map user. ||| User is unable to log into the Purdue Anvil GPU cluster due to a failed user mapping error. ||| Users within an ACCESS project (cis240473) are not appearing as eligible users in Purdue Anvil allocations, affecting 9 out of 14 users. ||| User was unable to access Ondemand for data analysis using Jupyter Notebook, but could still log in to Anvil server. ||| User is experiencing login and session issues with Anvil OOD since Friday 12/6/24. ||| User needs help setting up an account on Purdue's RCAC (Research Computing and Cyberinfrastructure) for using transferred Anvil credits for scaling tests. The user may have an existing username ""x-pxdaniel"" but has not used it before. ||| Student was unable to SSH into the Anvil cluster despite following instructions for generating SSH keys correctly and having an ""authorized\_keys"" file in the correct location. ||| User needs SSH key to log into Purdue Anvil CPU with username x-vrothenb ||| User is unable to access OpenDemand portal on Purdue Anvil supercomputer due to a 403 Forbidden error. ||| User unable to login to Open OnDemand on Anvil due to error message ""can't find user for x-hhale"". ||| The user is unable to access the Interactive Desktop on Anvil cluster. ||| User is unable to log in to Anvil HPC cluster (OnDemand or SSH) due to a possible outage or configuration issue. ||| Difficulty logging into Anvil clusters at Purdue (allocation DMR110007). The user reports that they and others in their research group have been experiencing issues since Sunday, April 6, with no notifications or visible outage information from RCAC at Purdue. ||| User is unable to log onto Anvil for two weeks and receiving the error 'Failed to connect to a061.anvil.rcac.purdue.edu:57590'. ||| User cannot access VASP 6.3.0 on Anvil and is unable to submit jobs due to an error message about vasp\_ncl not being found.","The issue appears to be caused by the user's home directory being full (101% used). To resolve this, the user should remove some files from their home directory. Here's a command example on how to check the current quota and usage of the home directory: `$ myquota x-ddickson`. After identifying the excessive files, they can be removed using common Unix commands such as `rm` or `find`. ||| To logon to Anvil, the user should first set up their ssh keys using Open OnDemand by following this guide: [Open OnDemand User Guide](https://www.rcac.purdue.edu/knowledge/anvil/access/login/ood). Once they have done that, they can use the ssh command to log in. The user can find a guide on setting up their ssh keys here: [Anvil SSH Keys Guide](https://www.rcac.purdue.edu/knowledge/anvil/access/login/sshkeys). ||| - For TAMU FASTER, the user was missing being added to the resource by their PI. After being added, they should be able to access the portal. - For Purdue Anvil GPU, the user's account was not yet created. The user should ask their PI to add them to the allocations on Anvil via https://allocations.access-ci.org/user\_management. After being added, they can log into Anvil Open OnDemand again. ||| Try logging into Pursue Anvil CPU onDemand again and check if the error persists. If you still encounter issues, please reopen this ticket with any additional error messages or details for further assistance. ||| To resolve this issue, please verify that the correct module has been loaded for the software environment required by ANVIL. If not already done, load the appropriate module using a command like `module load <module_name>`. Additionally, ensure that your user account is correctly configured in the ANVIL system. If the problem persists, it might be helpful to consult the ANVIL user documentation at <documentation_URL> for troubleshooting steps or contact your system administrator for further assistance. ||| The user should try logging into Anvil Open OnDemand again (ondemand.anvil.rcac.purdue.edu). It might take some time for the ACCESS website to list the resource providers in the ""Manage Users"" section. Additionally, name Burrell and name Johnsen should also have access now, but name's account is still being set up. If the user encounters any issues, they are encouraged to reach out to the support team again. ||| Loop in ACCESS support to resolve the issue with the allocation transfer from 11/30/23. Once the transfer has been processed, it will be picked up and propagated by Purdue Anvil, correcting the usage information on both portals and ensuring the balance shows correctly. To check on the status of this request, please refer to the ACCESS Allocations link provided in the Private Note section: [ACCESS Allocations](https://access-ci.atlassian.net/jira/people/team/bb58a2c1-b952-4323-ae42-fddf8975f846?ref=jira$&src=issue). ||| The issue appears to be that the account has not been fully set up yet on the ondemand.anvil platform. The user should try logging into the Anvil Open OnDemand gateway again (https://ondemand.anvil.rcac.purdue.edu/pun/name/dashboard) to see if the problem persists. If errors still occur, it is recommended to contact HPC Support for further assistance. ||| To resolve this issue, perform the following steps:
   - Log in to the Anvil Open OnDemand gateway again by navigating to its webpage. - If you still encounter errors, try running the command `nginx\\_stage --help` to see a full list of available command line options as suggested in the error message. However, if the issue is resolved, this step is not necessary. ||| The user has confirmed that they have OnDemand files under their home directory and are currently experiencing no issues. It was mentioned that account propagation can take several days. ||| The user was advised that their account was not propagated to the Anvil system and the support staff re-added them. They were informed to allow another day or two for the system propagation. No specific technical details were provided in this response. ||| To resolve this issue, user x-jbamber should comment out lines 37 and 38 in their .bashrc file which are causing bash line editing issues:

   Before:
   ```
   bind '""\eA"": history-search-backward'
   bind '""\e[B"": history-search-forward'
   ```

   After:
   ```
   #bind '""\eA"": history-search-backward'
   #bind '""\e[B"": history-search-forward'
   ```

After making this change, user x-jbamber should be able to run the ""myquota"" command without encountering a syntax error. Additionally, the ticket has been modified to ensure that users' startup files won't interfere with the output of the myquota command in the future. ||| The issue was due to an improper account setup on the Anvil OnDemand gateway. The support team has resolved this and suggests the user attempts to log in again to confirm the error is no longer present. ||| The user can login to Anvil with ssh, however they are still experiencing issues logging in. Suggestions for resolution include scheduling a virtual meeting to discuss the issue or registering for one of the Anvil Support Hour sessions at https://www.rcac.purdue.edu/anvil/anvil-support-hour. The link provided offers more information on available support hours. ||| The user should try reloading their modules using the following command: `module load <module_name>`. If the issue persists, it may be necessary to contact the cluster administrators and request them to investigate further. ||| Run the following commands to view the affected users:

```bash
sacctmgr show assoc account=cis240473 format=user -p User
sacctmgr show assoc account=cis240473-gpu format=user -p User
```

The initial issue was resolved, and the user should now see the 500 balance for the allocation. If further issues arise with this allocation, please contact support. ||| A patch has been deployed that should resolve the issue of inaccessibility to OnDemand for affected users. ||| The issue reported by the user appears to have been resolved as it is now working. The original problem was an issue with Anvil OOD that occurred on December 6, 2024, which has been fixed. Users experiencing similar issues in the future are advised to try logging in again. If the issue persists, they can reach out for further assistance. ||| To set up an account on RCAC, Purdue, follow the steps below:

- Visit the RCAC User Portal (https://rcac.purdue.edu/user-portal)
- Click on the ""Create a new account"" button if you do not have an account already. - If you believe your username is ""x-pxdaniel"", you may use that, but it's recommended to first check if that account exists by attempting to log in at the RCAC login page (https://login.rcac.purdue.edu). - If you have forgotten your password or need assistance, click on the ""Forgot Password"" link during registration and follow the instructions provided to reset it. - Once logged in, ensure that you have access to your Anvil credits for scaling tests by completing any necessary account setup procedures within the RCAC User Portal. ||| The student should create an SSH key using ssh-keygen on their local computer and then copy the content of the generated key-name.pub file to the ~/.ssh/authorized\_keys file on the Anvil cluster. ||| To log into Purdue Anvil CPU using SSH, you need to generate a new SSH key or add an existing one to your account. Here's how to do it:

   1. First, ensure that you have no existing keys by deleting any ~/.ssh/id_rsa and ~/.ssh/id_rsa.pub files. 2. Run the following command to generate a new SSH key: `ssh-keygen -t rsa -b 4096 -C ""your-email@purdue.edu""`. Replace ""your-email@purdue.edu"" with your email address associated with your Purdue account. 3. After generating the key, locate the newly created ~/.ssh/id_rsa.pub file. This is your public key. 4. Add the public key to your Anvil account by logging into the Anvil User Management Interface (https://anvil.rcac.purdue.edu/user-management/). Click on ""Add SSH Key"" and paste the content of the ~/.ssh/id_rsa.pub file there. 5. Now, you should be able to log into Anvil CPU using SSH without needing a password. Run the command `ssh -l x-vrothenb anvil.rcac.purdue.edu`. ||| Try logging into the OpenDemand portal again. If the user still encounters the 403 forbidden error, the issue should have been resolved by now and it may be safe to try again. ||| Log back in to Open OnDemand at https://ondemand.anvil.rcac.purdue.edu/ using your ACCESS username and password. If you encounter the error again, it is recommended to try logging in again as there was an issue with the backend for populating user data onto Anvil which has now been resolved. ||| To use the Interactive Desktop on Anvil cluster, you may need to first establish an SSH connection to the node using a terminal application such as PuTTY or Terminal (for macOS and Linux users). Once connected, use the module command followed by the name of the environment you wish to activate. For example:

```bash
module load <environment_name>
```

After activating the desired environment, you can start the Interactive Desktop using a specific command that depends on your cluster configuration. Please consult the documentation provided by your HPC administrators or contact them for guidance regarding the exact command to use in your case. If you encounter issues while connecting or starting the Interactive Desktop, ensure you are using the correct SSH credentials (username and password) and verify that your client is configured properly. Additionally, check for any network connectivity issues between your local machine and the Anvil cluster. ||| The user experienced an outage with the Anvil HPC cluster on Monday which coincided with the ticket submission. Since the outage has since been resolved, if the user is still facing this issue, they are encouraged to reach out for further assistance. ||| The issue experienced by the user was an outage on Anvil clusters at Purdue which coincided with when the ticket was submitted. The issue should have been resolved according to support response. ||| The user should check their network connection to ensure they can access the specified URL (http://a061.anvil.rcac.purdue.edu:57590). If the network connection is fine, it is recommended to contact the HPC support team for further assistance as there might be an issue with the login server for Anvil. ||| The user needs to modify the modules part of their job script to include `module --force purge` before loading gcc/11.2.0, openmpi/4.1.6, and vasp/6.3.0. Here is the modified script:

```bash
#!/bin/bash
#SBATCH --job-name=PrRuSn_SOC
#SBATCH --output=PrRuSn_SOC.o%j
#SBATCH --error=PrRuSn_SOC.e%j
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=64
#SBATCH --time=02:00:00
#SBATCH --partition=standard
#SBATCH --account=PHY250050
#SBATCH [--mail-user=: mailto:]
#SBATCH --mail-type=END,FAIL
module --force purge
module load gcc/11.2.0 openmpi/4.1.6 module load vasp/6.3.0
... (rest of the script)
```

The user is also advised to refer to the Anvil user guide for VASP calculations: [Anvil User Guide for VASP](https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp) and [Submitting Jobs on Anvil with VASP](https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp|smart-link)."
1,0,23,23,"User is having trouble configuring and installing a specific version (3.4.5) of PETSc on Anvil's intel19 compilers due to an error related to the FFTW library. ||| User is having trouble compiling BerkeleyGW on Anvil HPC system due to a non-functional arch.mk file. The user requires flags for LAPACK and ScaLAPACK, as well as the ability to compile using HDF5 if possible. ||| The user is unable to run cmake with the Intel compiler on Anvil, a Fortran MPI code system. The issue occurs intermittently and is related to the Intel license server load. ||| Long and inconsistent compile times for FORTRAN code on the HPC system using Intel compilers. ||| User Strahinja Trecakov seeks assistance in installing the ESMI_IB_Library as a dependency for the Variorum tool on Anvil HPC system. ||| Aborted work on Anvil due to missing libraries causing LD errors. ||| User is unable to install the latest version of GPAW package due to a failure in building the prerequisite Libxc library using the gpaw-buildenv script on Anvil HPC system. The user encounters an error when trying to compile with both gcc and intel compilers. ||| User could not find required hdf5 and MKL libraries and optimizations for their Fortran code in the Anvil system, and needed guidance to resolve this issue. ||| User is unable to locate libjasper.so.1 on Anvil HPC system. ||| User is experiencing errors while running MPI-based code on Anvil HPCC due to an issue with the MPI setup, specifically involving the HYDRA library and intelMPI. ||| User is unable to compile a MPI-based computational fluid dynamics Fortran code using autotools with Intel compiler, mvapich2, mpc, and mpfr modules. The error occurs during the bootstrapping of makefile fragments for automatic dependency tracking. ||| The user is unable to run a previously compiled LAMMPS executable due to the compiler not being able to find the mpi libraries and the executable hanging without output. They also have trouble compiling PLUMED due to the same issue with the mpi libraries. ||| User requires libpng library (version 1.2 or later) to compile C++ code on Anvil HPC system. ||| User is unable to install Cairo.h on Anvil HPC system, despite having used other HPCs where it was included by default. ||| User cannot find wannier90 lib file when compiling vasp+wannier90 on Anvil and is requesting a make.inc file to compile wannier90 independently. ||| The user's project requires PETSc version 3.12.4 or later, but it seems that the required libraries GLIBCXX\_3.4.21 and CXXABI\_1.3.9 are not present in the latest intel module on Anvil. The user experiences segfault errors when trying to link against the libstdc++.so.6 from a different location, and alternative PETSc versions fail to configure properly. ||| User is encountering an error while running VASP on Anvil HPC, and seeks guidance on how to install Gaussian 16, GaussView, and compile VASP with Wannier90 and VASPsol. ||| User is experiencing issues with Fortran compiler while running an MPI program on Anvil, specifically regarding the namelist command in Fortran when using OpenMPI module with gcc/11.2.0. ||| User needs help compiling and running a code (StagYY) on Anvil HPC cluster due to lack of Euler access. The user is experiencing errors during module loading for PETSc installation. ||| User is encountering an assertion error while running Abinit compiled with Intel compiler on Anvil HPC system and seeking a solution for the issue. ||| The user is unable to compile JDFTx on Purdue Anvil due to header errors related to the source code, specifically with `ManagedMemory.h`. ||| User is unable to compile and run a Fortran-based software on Anvil due to issues with the required libraries and modules, specifically the version of intel compilers. ||| Unable to compile Nektar software version 5.7.0 on the HPC system","- To resolve this issue, try using the existing FFTW module available on Anvil instead of downloading it in the install_ANVIL.sh file. This might help avoid the error encountered during configuration and installation of PETSc. - Review and modify the install_ANVIL.sh script to replace the download command for FFTW with a line that loads the Anvil's FFTW module, such as `module load fftw`. - After making these changes, reattempt the installation process of PETSc using the updated script and check if the error related to FFTW still persists. If it does, consult the output file (slurm-2582935.out) for further clues on the issue and seek additional assistance as needed. ||| First, use the following command to obtain the necessary paths for HDF5 on Anvil:
   ```
   module spider hdf5
   module show hdf5/1.10.7
   ```
   Add these obtained paths to the arch.mk file in the appropriate sections (e.g., #HDF5DIR, #HDF5LIB). This should resolve the issue with compiling BerkeleyGW using HDF5. However, keep in mind that LAPACK and ScaLAPACK are already included in the MKL library provided in the shared arch file. Therefore, no additional steps are required for those flags. Hope this helps! ||| The user should use the default modules (GNU compiler) when running cmake on Anvil as an alternative to the Intel compiler. If the Intel compiler is required, the user may experience slow compilation times due to heavy load on the Intel license server. This is because there are a limited number of licenses available, and multiple users using the Intel compiler can cause fewer licenses to be available. The issue should not occur with GCC unless there is a filesystem issue. The user is encouraged to keep us informed when experiencing slowness. ||| The long compilation time might have been related to intermittent slowness of the project filesystem. The engineers have brought that filesystem back to its optimal performance. It is recommended to retest the compile time now. If issues persist or further questions arise, please reach out again. ||| Unfortunately, at this time, we are unable to directly install third-party dependencies like ESMI_IB_Library on the Anvil for users. It is recommended that you explore alternative methods to track performance metrics of your jobs without relying on this specific library. If you have further questions or require assistance with other matters, please do not hesitate to contact us again. ||| The issue with the project filesystem should be fixed now. Users are advised to try their job submission again and verify if the error persists. ||| Try installing GPAW using Spack, as it seems to work for some users without encountering the same issue. Below are the suggested commands:
   ```
   $ spack env create -d ./gpaw
   $ spack env activate `pwd`/gpaw
   $ spack add py-gpaw %
   $ spack install
   ```
   If you're not familiar with Spack, follow the tutorial (https://spack-tutorial.readthedocs.io/en/latest/index.html) to get started. ||| To load the necessary hdf5 libraries, use the following command: `module load hdf5`. For the MKL libraries, use the `module load intel-mkl` command. To find more software on Anvil, review the Cluster 101 and Cluster 201 tutorials at [this link](https://www.rcac.purdue.edu/training) and visit [the software knowledge base](https://www.rcac.purdue.edu/knowledge/anvil/software). ||| The user can try using command `locate libjasper` to find related libraries. However, only libjasper.so, libjasper.so.4, and libjasper.so.4.0.0 are available under /usr/lib64/ on Anvil. The user is advised to check if these can work for their specific workflow. ||| To resolve this issue, users can try modifying their command to run via a Slurm job with the specified MPI (e.g. `srun --mpi=pmi2 ./flow_ifs`). Additionally, adding the following command before running the code may help: `export I_MPI_HYDRA_BOOTSTRAP=ssh`. This configuration change should ensure that the correct intelMPI is used when running the code. ||| The user should try re-running the `configure` command with the '--disable-dependency-tracking' option to bypass the issue. However, this will disable automatic dependency tracking and might cause issues later. To compile the code correctly, ensure all necessary libraries are installed and accessible. The source code for this package can be found at [https://auburn.box.com/s/s5uenng2zui29o68db74zih5ehdx3kff](https://auburn.box.com/s/s5uenng2zui29o68db74zih5ehdx3kff). To generate the configure & make file and compile, use the following commands:
   - `sh autogen.sh`
   - `./configure`
   - `make` ||| To resolve this issue, follow these steps to correctly compile LAMMPS and PLUMED:
   - Load the Intel and IMPI modules (module load intel/19.0.5.281 and module load impi/2019.5.281)
   - Update your makefile with the correct options for LAMMPS compilation:
     ```bash
     CC = mpiccc -std=c++11 -diag-disable=10441 -diag-disable=2196
     OPTFLAGS = -march=core-avx2 -O2 -fma -ftz -fomit-frame-pointer \
       -fp-model fast=2 -no-prec-div -qoverride-limits \
       -qopt-zmm-usage=high
     CCFLAGS = -qopenmp -qno-offload -ansi-alias -restrict \
       -DLMP_INTEL_USELRT -DLMP_USE_MKL_RNG $(OPTFLAGS) \
       -I$(MKLROOT)/include
     SHFLAGS = -fPIC
     DEPFLAGS = -M
     LINK = mpicc -std=c++11 -diag-disable=10441 -diag-disable=2196
     LINKFLAGS = -qopenmp $(OPTFLAGS) -L$(MKLROOT)/lib/intel64/
     LIB = -ltbbmalloc -lmkl_intel_ilp64 -lmkl_sequential -lmkl_core
     SIZE = size
     ARCHIVE = ar
     ARFLAGS = -rc
     SHLIBFLAGS = -shared
     ```
   - Compile LAMMPS via the make command
   - Before compiling PLUMED, check if mpiccc and mpiicpc can find the mpi libraries:
     ```bash
     mpicc -h
     mpiicpc -h
     ```
     If there is an error saying ""/usr/bin/ld: cannot find -lmpi"", try loading the Intel and IMPI modules again. - If the checks are successful, compile PLUMED using cmake. ||| To resolve the issue, the user should update the path for libpng in their Makefile to include ""/apps/spack/anvil/apps/libpng/1.6.37-gcc-11.2.0-ndy5dug"". Additionally, the team will try to push this as a module for future use. ||| To install the Cairo library and header files (Cairo.h) on Anvil HPC, follow the instructions below:

   a. Open a terminal session. b. Update your package list by running `module purge` followed by `module load Anaconda3`. c. Activate the conda environment with the required dependencies using the command: `conda activate <your-environment>` (replace ""<your-environment>"" with the name of your desired Conda environment). d. Create a new conda environment specifically for Cairo by running `conda create -n cairo cairo`. This will install the latest stable version of Cairo and its dependencies. e. To access the newly created environment, activate it using the command: `conda activate cairo`. f. Verify that Cairo is installed correctly by checking for the presence of the Cairo header files using the command: `ldd <path-to-cairo-binary> | grep libcairo`. Replace ""<path-to-cairo-binary>"" with the path to your desired Cairo binary file. g. If you encounter any issues during this process or need further assistance, please reopen this ticket and provide more details about the error messages you're experiencing. Additional Information: For comprehensive documentation on Cairo and its usage, visit https://cairographics.org/documentation/. ||| To resolve this issue, provide the user with the appropriate make.inc file for compiling wannier90. Here's an example of where they can obtain it from:

   - Download the latest make.inc file from the Spack repository (https://github.com/spack/spack-recipes/tree/master/cuda/wannier90). Save it as `~/.spack/compilers/gcc/11.2.0/wannier90/make.inc`. - Once the make.inc file is in place, they can compile wannier90 by running the following command on Anvil:
     ```bash
     spack install wannier90@3.1.0 --force-reinstall
     ```

   After a successful installation, the user should be able to find the wannier90 lib file in the expected location (`/apps/spack/anvil/external/vasp/wannier90/3.1.0`). ||| To resolve this issue, the user can try switching to using the GCC compiler or installing the Intel oneAPI compilers on their project space as a non-root user. If they must use the Intel compilers, download and install the latest version (19.1.3.304) from the Intel website, available at <https://software.intel.com/content/www/us/en/develop/tools/oneapi>. Instructions for non-root installation may be found in the provided guide or by contacting Purdue IT for assistance. ||| - To utilize the VASP5 module on Anvil, the user needs to be added to the vasp5 unix group. - Currently, there is no Gaussian or GaussView installed on Anvil due to licensing restrictions. The user can install these tools in their own project space and use them on Anvil. - To compile VASP with vaspsol and wannier90, the user should follow the instructions provided on the vaspsol page for installation. Adding the PATH of wannier90 to the makefile.include is also necessary during compilation. - To be added to the vasp5 unix group, provide your email address associated with the VASP license, not someone else's. Your supervisor should add you to the VASP license using your email address. ||| Temporarily resolve this issue by using the Fortran compiler provided with the OpenMPI installation instead of gcc/11.2.0. To do so, modify your script to include the following command before compiling and running your MPI program:

```bash
module unload gcc/11.2.0
module load openmpi/<version> fortran
```

Replace `<version>` with the appropriate OpenMPI version you are using. This change should help resolve the issues with the namelist command in your Fortran program. ||| - To load the necessary modules in Anvil, run the following command:
     ```
     module purge
     module load modtree/cpu
     module load openmpi/4.1.6
     module load hdf5
     module load openblas
     module load libpng/1.6.37
     module load cmake/3.20.0
     ```
   - The user should adapt the MAKEFILE for Anvil by updating the PETSc version to 3.15.3 available on Anvil (PETSc 3.20.1 is not available). It's unclear what changes are needed in the makefile, but here's a suggested place to start:
     ```
     # CMake settings
     -DCMAKE_INSTALL_PREFIX=/usr/local -DPETSC_DIR=/usr/local
     ```
   - The user should verify that they use 'module load modtree/cpu' instead of 'module load rcac' on Anvil. - The user is advised to contact the StagYY code developer for further information regarding manual PETSc installation and whether using petsc from the cluster directly would also be okay. - The user should test if they encounter any issues with their edited makefile and reach out for assistance as needed. ||| The user was suggested to run their workflow using `srun --mpi=pmi2 -n xxx ./mycode` (where xxx is the number of processors needed for the run). If this does not work, re-compiling the application with gcc/openmpi or using containerization might be a solution. Alternatively, checking if anyone has previously compiled Abinit on Anvil and sharing their configure files could help resolve the issue. If these steps do not work, escalating the ticket to the Anvil Applications team is recommended for further investigation. A link to the Anvil Applications Jira page was provided for additional resources (https://access-ci.atlassian.net/jira/people/team/0f5fcf8a-26ef-4d24-a346-4a2ef3a7afda?ref=jira$&src=issue). ||| To resolve this issue, it is recommended that the user contacts the JDFTx code developer for assistance with the header errors. In the meantime, the user can attempt to add the missing include statement (`#include <cmath>`) at line 35 in `BlasExtra.h` file located at `/anvil/scratch/nshan/apps/jdftx/jdftx-1.7.0/jdftx/core/`. The corrected line should read:

```c++
#include <cmath>
``` ||| To resolve the issue, the user should try using one of the available versions of the intel compilers on Anvil: * intel/19.0.5.281 * intel/19.1.3.304 * intel/2024.1. Modify the compiler bash file located at ""/home/x-armkian/boundarylayer\_DNS/v013/compile.sh"" to adjust for the new version. If necessary, seek assistance from Anvil Application Team or visit Anvil Support Hour for further guidance. ||| To resolve this issue, follow these steps:

   - Load the necessary modules required for compiling Nektar. This includes Intel, CMake, Intel MKL, FFTW, Boost and enable MPI (Message Passing Interface) and FFTW (Fastest Fourier Transform in the West) during the compilation process:

     ```
     module load intel cmake intel-mkl fftw boost
     export I_MPI_DEBUG=2
     ```

   - Once the necessary modules are loaded, navigate to the Nektar software directory and attempt to compile it using CMake. For example, if you have downloaded the software into a directory called `nektar-v5.7.0`, execute these commands:

     ```
     cd nektar-v5.7.0
     mkdir build && cd build
     cmake -DCMAKE_BUILD_TYPE=Release ..
     make -j8
     ```

   - If the compilation still fails, check for any error messages that indicate specific issues with the compilers or dependencies. For instance, you may need to reload the required modules or update certain software components. It is also recommended to refer to the Nektar documentation (<https://www.nektar.info/getting-started/repository/>) for detailed information on building and installing the software."
1,1,23,22,"User wants to run Gaussian jobs on Anvil but is unable to due to the absence of Gaussian license. ||| User needs to install Quantum Espresso 7.2 and EPW 5.7 for project work. ||| User requesting installation of Joe-Editor on Anvil HPC cluster ||| A student researcher at New Mexico State University was unable to find the necessary modules required for compiling a custom version of Quantum Espresso 6.7+ on Purdue Anvil's supercomputer. ||| User requesting installation of Apptainer on Anvil for container building, and installation of prerequisites if not possible. ||| User requires CP2K + PLUMED for simulations but the current version on Anvil does not contain PLUMED support. The user requests a private compilation of CP2K+PLUMED in their home directory (/home/x-msun3/Codes/). ||| User requires access to the computational chemistry software Gaussian for their HPC resource, but is unsure if it is available and pre-installed on Anvil. ||| User wants to install IDL in their $WORK directory on Anvil without using modules, but requires sudo permissions or root access for license activation and doesn't know how to proceed. ||| PhD Student at University of Rhode Island needs assistance in compiling OpenFOAM v2006 on Anvil, but the administrative team might not be able to deploy this version due to software installation policy. ||| User (Komal) wanted to install PLUMED on Anvil HPC system and needed clarification if it was allowed and how to proceed. ||| Request to add jq command-line program to the list of modules for Anvil cluster OS ||| User wants to install the GALBA pipeline as a module on Anvil. ||| User needs Abaqus 2022 or higher installed on Anvil HPC system for their project space in order to run jobs under an access allocation. ||| User requests installation of BerkleyGW and Yambo software in their project space. ||| The user is encountering an issue while installing a program called Aspect using Apptainer, and the installation seems to be having trouble finding a specific file. ||| User requests installation of a newer version of Gromacs than the current one (version 2021.2) on Anvil HPC system. ||| User needs assistance in installing MOPAC and ORCA software packages on the ANVIL High Performance Computing (HPC) system. ||| User Uttam needs to install Tesseract command-line software on Anvil system for an OCR project. ||| User reports that GAMESS is not installed on ANVIL cluster and requests its installation. ||| User encountered issue with software installation on HPC cluster. ||| User inquired about making the Globus CLI, SDK, and REAST APIs available without installation in user home or project space. ||| User needs guidance on how to install software on Clusters ||| Request to install Quantum Espresso (QE) version 7.4 on Anvil HPC cluster by user bbrogdon.","To run Gaussian jobs, transfer some credits to Expanse where Gaussian is available. Ask Joan Shea (PI for MCA05S027) to submit a supplement for allocation time from Expanse. Here are the steps to submit a Supplement:
   - Login to ACCESS: <https://allocations.access-ci.org>
   - Click on ""Manage Allocations."" - Within ""Manage Allocations,"" click on ""Manage My Projects."" - Starting at the List of Allocation Request Page, go to the List of ACCESS Allocations Requests page: <https://allocations.access-ci.org/requests>
   - Look for the Allocation you would like to take action on and click the ""Choose New Action"" button. - Choose the option: Supplement. - Once we receive this and get it reviewed, you will receive a notification of the reviewer's decision. ||| To install Quantum Espresso (QE) version 7.2, the user can follow these steps:

   a. Load the required modules using the following command:
      ```
      module load gcc openmpi intel-mkl fftw
      ```

   b. Download and build QE from source. You can find the instructions here: https://www.quantum-espresso.org/downloads/installation.html

   For EPW 5.7 installation, you can follow these steps:

   a. Load the required modules using the following command:
      ```
      module load intel-mkl openmpi
      ```

   b. Download and build EPW from source. You can find the instructions here: https://www.cp2k.org/en/documentation/installation-epw/ ||| Although the Joe-Editor cannot be installed centrally on Anvil as it is not a frequently requested software, users can install it in their own space, such as their HOME or project folder. To do so, follow these steps:

   - First, ensure you are logged into your account on Anvil. - Create a directory for the Joe-Editor using the command `mkdir ~/joe`. - Download the latest version of Joe-Editor from its official website and save it in the created directory with a suitable name (e.g., `joe`). - To make the downloaded binary executable, use the command `chmod +x ~/joe/joe`. - Finally, add the directory to your PATH environment variable so that you can run Joe-Editor from anywhere on Anvil. Here's how to do it:
     ```bash
     export PATH=$PATH:~/joe
     ```
     To make this change permanent, add the line above to your `~/.bashrc` or `~/.bash_profile` file and reload the profile with the command `source ~/.bashrc`. ||| The user can access the installed {{quantum-espresso/6.7}} module by running the following commands:
   ```bash
   $ module load gcc/11.2.0 openmpi/4.0.6
   $ module load quantum-espresso/6.7
   ```
   If the user needs to install a custom version of Quantum Espresso, they should check its required libraries for compilations. ||| The requested application, Apptainer, is currently under consideration for installation on Anvil, but an estimated time of availability (ETA) has not been provided. If users want to install it in the meantime, they are recommended to use Spack (<https://spack-tutorial.readthedocs.io/en/latest/>). This tool can help with the installation process. Users should refer to its documentation for detailed instructions. ||| The user can check if Gaussian is available on Anvil by visiting the following webpage: [https://www.rcac.purdue.edu/knowledge/applications](https://www.rcac.purdue.edu/knowledge/applications). Since Gaussian needs a license, it is not pre-installed on Anvil. However, if the user has their own license, they can install Gaussian in their project space. ||| To install IDL in the desired directory (/anvil/projects/x-phy160032/idl89) on Anvil, you need either sudo permissions or root access to activate the university license by running the command ""{{envi\_idl\_license\_admin}}"". Since the user doesn't have sudo permissions or knows the password for it, the best course of action is to request sudo privileges from an administrator or system support staff. The relevant documentation can be found at: [IDL Installation Guide](https://www.nv5geospatialsoftware.com/docs/idl-install.html). ||| The PhD student can try to compile OpenFOAM v2006 following the official guide within their own spaces since an older version of gcc (compiler) is working for them now. If they encounter further issues, they should reach out with more questions. ||| The user should be able to install PLUMED in their space. Before installation, they are required to load the GCC module using the command `$ module load gcc`. The instructions for installation can be found at this link: [PLUMED Installation Guide](https://www.plumed.org/doc-v2.9/user-doc/html/_installation.html). ||| The support team is planning to install jq on Anvil. It will be added to the pipeline and the user will be notified once it's available. No action is required from the user at this time. ||| To install the GALBA pipeline as a module on Anvil, follow these steps from the Purdue RCAC software installation guide: [https://www.rcac.purdue.edu/training/software-installation](https://www.rcac.purdue.edu/training/software-installation%7Csmart-link) | The user can follow the steps on this page to successfully install GALBA as a module on Anvil, enabling its usage for genome annotation in non-model species where RNA-seq data is not available. ||| - The user should confirm with RCAC that the necessary licensing information is correct and then follow up with them for further assistance. - If Abaqus 2022 is not available, the user can request for Abaqus 2023 or 2024 to be installed on their project space. - To install Abaqus, the user's allocation will need to temporarily allow access for a system administrator. - After the installation, the user can load the Abaqus module with the following command: `module use /anvil/projects/x-mch240064/etc/modules; module load abaqus`
- To check if Abaqus is correctly loaded, the user can run the command: `which abaqus`
- The license server has been set up for the installed version of Abaqus. The user should ensure that they have been added to ECN's license pool. If not, they need to contact ECN for assistance. - To open the Abaqus CAE GUI, the user can use the command: `abaqus can -mesa` ||| To install BerkleyGW and Yambo, the user's group should follow the Anvil software request policy available at this link: <https://www.rcac.purdue.edu/knowledge/anvil/policies/software_installation_request_policy>
After following the installation process according to the policy, they can install BerkleyGW from its official site: <https://berkeleygw.org/> and Yambo from its official site: <https://www.yambo-code.org/> If they encounter any issues during the installation, they are encouraged to reach out for further assistance. ||| Pull the Aspect Docker image as a Singularity/apptainer image by executing the following commands in a terminal:

```bash
git clone https://github.com/geodynamics/aspect.git
cd aspect
singularity pull docker://geodynamics/aspect
```

After pulling the image, create a build directory and navigate to it:

```bash
mkdir build
cd build
```

Execute the following commands within the build directory:

```bash
singularity exec -e ../aspect_latest.sif cmake ..
singularity exec -e ../../aspect_latest.sif make
```

Now you should be able to compile and install Aspect without any problems. When working with Aspect, use the ""singularity exec -e ../aspect_latest.sif"" command instead of ""docker run -it"". This is necessary because Apptainer/Singularity must be used on clusters where Docker cannot be run due to admin permissions. ||| The most recent available Gromacs version is 2024, which can be tried as an alternative to the older version currently installed on Anvil. It is suggested that the user tests this version to determine if it meets their requirements. If a newer version is expected in February, users will be notified accordingly. ||| To install MOPAC and ORCA on ANVIL HPC system, follow these steps. First, log in to your ANVIL account using the command `ssh username@anvil.rc.uk`. After logging in, load the necessary modules with the commands: `module load anaconda3/2019.10` and `module load mpi/openmpi/4.0.5`. Now, activate your Anaconda environment with the command `source activate`. To install MOPAC, run `conda install -c conda-forge mopac` and for ORCA, use `conda install -c conda-forge orca`. Once installed, you can access the software from your system's PATH. For more information on using ANVIL, please refer to the [ANVIL User Guide](https://www.rc.uk.net/documentation/anvil). ||| - First, the support team verified that the user was working on the Anvil system. - Then, they provided the installation command `sudo dnf install tesseract` to the user. - However, upon running the command, the user encountered an issue with Tesseract not found (`bash: tesseract: command not found`). The support team resolved the issue by creating a local folder for software installation and activating a condo environment with `tesseract -v` command. - To make sure that Tesseract was installed, they suggested the user to check the version using the command `tesseract -v`. If everything works well, the user should see the Tesseract version output. - Later, they informed Uttam that they had created a module for easier usage with the command `module load tesseract/5.5.0`. Additionally, they also created a Tesseract environment named `tesseract_env`, which is now available as a Jupyter kernel in Open OnDemand. ||| To install GAMESS on ANVIL, the user was asked to follow these steps:
   - Log into the ANVIL cluster using ssh (not provided in the message)
   - Navigate to the scratch folder `sinteractive -N1 -n128 -t 8:00:00` and create a directory for GAMESS if not already present
   - Download the GAMESS source code from the specified location (not provided in the message) using `tar xvf gamess-current.tar.gz`
   - Navigate into the downloaded directory `cd gamess/`
   - Unload any existing module related to compilers, MPI, and MKL using `ml --force purge ml gcc/11.2.0 openmpi/4.1.6 intel-mkl/2020.4.304`
   - Load the required modules for the build process: gfortran, mpi, and openmpi (not provided in the message)
   - Run the configuration script `./config`, entering the target machine name as 'linux64', GAMESS directory, build directory, and version number
   - Provide MKL pathname and MPI library location (specified in the message), and choose options for building GAMESS with OpenMP thread support and other optional features. - Run `make ddi` to compile GAMESS, followed by `make -j128` to speed up the build process using multiple cores. ||| The user should follow these steps to install the required software on the HPC cluster:

   a. First, log into the HPC cluster using ssh. b. Navigate to the home directory (~) using the command `cd ~`. c. Load the appropriate module for the software by typing `module load [software_module_name]`, replacing ""[software_module_name]"" with the actual name of the software module. If you're unsure about the correct module name, consult the HPC documentation or contact the system administrators. d. After loading the module, initiate the installation process by executing the relevant command for your operating system and software package. For example, if using a Linux distribution and installing the GNU Compiler Collection (GCC), you can use the command `sudo yum install gcc`. Again, replace ""gcc"" with the appropriate software name when applicable. e. Verify that the installation was successful by checking the version of the installed software. For example: `gcc --version` or any other suitable command for the specific software you're using. ||| In accordance with Purdue RCAC's software installation policy (<https://www.rcac.purdue.edu/knowledge/anvil/policies/software_installation_request_policy?all=true>), Globus CLI, SDK, and REAST APIs are considered self-installable software in a user's home directory or project space rather than software that the support team makes available to all users. If assistance is required for installation, the user is encouraged to ask questions about the process. ||| Use the tutorial provided at https://www.rcac.purdue.edu/training/software-installation for instructions on installing software on clusters. If you require assistance during the process, please do not hesitate to ask for help. ||| To install Quantum Espresso (QE) version 7.4 on the Anvil HPC cluster, submit a support ticket with the necessary details to the user support team. They will review and make a decision based on your request. If you still need QE 7.4 on Anvil, specify any specific functions required. You can download the latest release of Quantum Espresso from this link: [QEF/q-e GitLab releases](https://gitlab.com/QEF/q-e/-/releases)."
2,0,21,21,"The user is experiencing an Out of Memory error when running VASP on ANVIL. Despite increasing the number of nodes, the issue persists. The problem occurs after approximately 5 minutes, but the job continues to run until the allocated time ends. ||| User needs guidance on specifying the correct allocation and filename in VASP submission script for Anvil. ||| The user, Mouyang, is unable to load VASP5.4.4 through a batch submission job (slurm's sbatch command) in the allocation dmr160156. ||| User needs access to VASP 5 module on HPC and is unable to do so because they are with MedeA. They are self-compiling VASP using Intel compilers but seek guidance on optimizing the makefile.include parameters for Anvil. ||| User inquires about availability of VASP 5.4.4 compiled for GPU usage and requests a sample script. ||| User inquired about the availability of VASP 5.4.4 compiled for GPU usage on Anvil HPC system. ||| User is experiencing issues running vasp on compute nodes using sbatch scripts while it works when run interactively or on the head node. The issue appears to involve a potential mismatch in libraries on the compute nodes. ||| User is unable to run VASP 5.4.4 due to a missing shared library libmkl_gf_lp64.so during execution. ||| The user cannot build bvasp5.4.4+wannier90 on ANVIL due to a permission error when linking the libwannier.a library. ||| VASP version on Anvil does not compile with vtst code and cannot run NEB calculations. ||| User is experiencing memory errors when running VASP calculations on Anvil with more than 8 nodes using OpenMPI versions prior to 4.1.2. The user suspects the issue may be due to a memory leak in ScaLAPACK calls present in the older versions of OpenMPI. ||| User needs help with compiling VTST Tools for solid state CI-NEB calculations in VASP6 source code on Anvil. ||| User requires assistance in compiling OpenACC version of VASP for GPU node on HPC system, specifically seeking environment and compiler options. ||| User is unable to run VASP jobs due to the program not finding the executable vasp\_std, despite having permissions for VASP5.4.4. ||| The user is experiencing dependency issues while trying to install 'fs' and 'httpuv' packages in R (v 4.3.2) using conda, specifically in an environment called R5. This error occurred after the dependencies for 'Seurat' package stopped working, and the user was unable to load it with the library(Seurat) command. The user suspects a conflict with GNU compiler version might be causing this issue. ||| User is unable to compile VASP.5.4.4 with Wannier90 and experiences slow job execution without it, along with out of memory errors during the run. ||| User is experiencing an error during VASP simulation using Intel MPI library, which is resolved by recompiling VASP with updated versions of Intel, OpenMPI, and Intel-MKL libraries. ||| User needs to install the vtvt tool for VASP 5.4.4 on Anvil HPC cluster. ||| User encountered an error while running vasp calculations using module spider, unable to find ""vasp"" or specific versions (5.4.4.pl2 and 6.3.0). ||| Request for building a new module for VASP 6.4.3 due to the lack of recent updates in the current version on Anvil (6.3.0). The tarball is provided in the user's home directory at ""/home/x-jcappola/vasp.6.4.3.tgz"". ||| Lmod error with VASP module and mpirun unable to find specified executable file","The user should check their INCAR file for each tag and run a small job to see if it can run successfully. If the small jobs can run, it indicates that some setup for large jobs needs to be adjusted. ||| The user can use `#SBATCH -A myallocation` for the allocation and does not need to change the `#FILENAME` line. The account name (replacing 'myallocation') can be found using the `mybalance` command on Anvil. For more information about submitting jobs with SLURM, the user may refer to the documentation at [SLURM sbatch documentation](https://slurm.schedmd.com/sbatch.html). ||| To resolve this issue, the user should ensure that the required modules for VASP5.4.4 are loaded before submitting a batch job using the module command. For example, the following command can be used to load the required modules:

```bash
module load vasp/5.4.4
```

It is important to note that this command should be executed before running any scripts or commands related to VASP5.4.4. If the issue persists after loading the modules, check the job script for any potential errors in specifying the correct path for the vasp_std file or other necessary files. Additionally, verify that the batch job submission parameters are correctly set up to include the required resources and environment variables. ||| For using VASP on Anvil, users are advised not to join Anvil VASP groups if they are with MedeA. Instead, users can compile it on their own or work with Materials Design about how to use MedeA workflow on Anvil. The user guide for VASP compilation on Anvil is available at: [https://www.rcac.purdue.edu/knowledge/anvil/software/installing\_applications/vasp](https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp)

For building VASP 5.4 with different compilers, including Intel compiler, a guide is provided at: [https://www.rcac.purdue.edu/knowledge/anvil/software/installing\_applications/vasp/build_your_own\_vasp_5](https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp/build_your_own_vasp_5). Users are recommended to use GNU compiler on Anvil instead of Intel compilers. ||| To use VASP 5.4.4 on GPUs, you need to have the correct modules loaded. Here's an example script for running a calculation using VASP with GPU support:

```bash
module load vasp/5.4.4-gpu
cp input_file $SCRATCH/input_file  # Replace input_file with your actual input file name
cd $SCRATCH
vasp_std -pot . POSCAR OUTDIR &
```

Please note that you should replace ""input\_file"" with the actual name of your input file. Also, ensure that you have the correct GPU-enabled version of VASP installed and the necessary modules loaded before running this script. If you encounter any issues, please consult the [VASP documentation](https://cms.mpcdf.mpg.de/vasp/) for additional guidance. ||| VASP 5.4.4 is not currently installed with GPU support on Anvil HPC system. We are considering adding it to our installation list but there is no estimated time of availability at this moment. ||| To investigate and resolve this issue, follow these steps:
   1. Examine the error log (err.3490321.xz) for more details about the errors encountered during the sbatch script execution. You can do so using a text editor or command-line tools such as `less` or `cat`. 2. Check the versions of the OpenMPI and Intel modules installed on both the head node and compute nodes to ensure they match. If not, reinstall/update the modules to match. For example:
      ```
      module avail openmpi
      module avail intel
      module load openmpi/x.y.z
      module load intel/x.y.z
      ```
   3. Verify that the correct modules are loaded in your sbatch script. If not, add appropriate `module` commands at the beginning of the script as follows:
      ```
      #!/bin/bash -l
      module unload openmpi
      module load intel
      ...
      ```
   4. Ensure that the paths to the vasp binary and any necessary libraries are correct in your sbatch script and in the working example provided (/home/x-coses/scratch/test). 5. Check if there are any environment variables affecting vasp execution only when running via sbatch scripts, such as LD\_LIBRARY\_PATH or PATH. Set these variables appropriately in your sbatch script. 6. If necessary, contact the system administrators for further assistance, especially if additional information or troubleshooting is required. Also, provide them access to your vasp license if needed. ||| To resolve this issue, the user needs to download and install the necessary MKL (Math Kernel Library) library. Here is a step-by-step guide on how to do it using the Intel Parallel Studio XE application (which includes MKL):

   - Visit the following website to download the software: https://software.intel.com/content/www/us/en/develop/tools/oneapi/components.html
   - Select the appropriate version for your system, accept the license agreement, and follow the installation instructions provided. - After successful installation, run Intel Parallel Studio XE Command Line Compiler (ipxc) by adding it to your PATH or using the command `module load intel` if you have LMOD module manager installed. - Run the following commands to build and link MKL libraries with VASP:

```bash
mkdir -p $VASP_DIR/libmkl
cp /opt/intel/compilers_and_libraries_2021.5.314/linux/mkl/lib/libmkl_gf_lp64.so $VASP_DIR/libmkl/
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$VASP_DIR/libmkl
```
   - Replace `$VASP_DIR` with the actual path to your VASP installation directory. ||| To resolve this issue, ensure that you have the necessary permissions to access the required library by running the following command as root or using `sudo`:

```bash
sudo ln -s /apps/spack/anvil/external/vasp/wannier90-3.1.0-gcc-11.2.0/libwannier.a /usr/local/lib
```

Before building the bvasp5.4.4+wannier90, verify that you have loaded the correct modules for both VASP and wannier90 using the `module load` command:

```bash
module load vasp/[version]
module load wannier90/[version]
``` ||| The Senior Computational Scientist recommends installing the necessary NEB version of VASP in the user's own project space. If assistance is required for installation, the user should reach out to the team for help. ||| Compile VASP5 and VASP6 with OpenMPI version 4.1.6 or later on Anvil to resolve the memory issues related to the memory leak present in OpenMPI versions 4.0.4-4.1.1. Use the modules available on Anvil for compilation of vasp\_ncl, as it has been recompiled with the new version of OpenMPI. ||| To compile vtst with vasp6, modify the main.F file according to guidance on the vtst website. After modifying the file, follow our user guide to build VASP on Anvil (<https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp/build_your_own_vasp_6>). ||| To compile OpenACC version of VASP for GPU nodes on the HPC system, use the NVIDIA HPC SDK (nvhpc) for compilation. At the moment, the current nvhpc module (nvhpc/21.7) does not work because it lacks the mpi library. The user can either wait for the deployment of the new nvhpc/23.3 module on the HPC system or install the nvhpc with MPI on their own project space. For the installation, follow these steps:
- Load the NVIDIA HPC SDK (nvhpc) module by running this command in your terminal: `module load nvhpc/23.3`
- Install the necessary MPI library for your system (e.g., Intel MPI or OpenMPI) in your project space. Regarding the makefile, you can set up tags in makefile.include for GPU VASP installation by using:
```makefile
%if (${ARCH} == 'gpu')
  OPENACC = -acc
  CC = pgc++
  CFLAGS = ${CC} ${CFLAGS} -fopenmp -fopenacc
  F77 = pgfortran
  FFLAGS = ${F77} ${FFLAGS} -fopenmp -fopenacc
  LDSHARED = pgc++
#endif
```

Replace `'gpu'` with the appropriate architecture identifier in your makefile if needed. For more information, please refer to the provided link (<https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp/build_your_own_vasp_6>) for building your own VASP on Anvil. ||| The issue was resolved by fixing some permission issues with the vasp\_std executable. The user should now be able to use it for their calculations. If any further questions or issues arise within 7 days, replying to this ticket will re-open it. ||| To resolve this issue, first, ensure you have the required packages installed on your system. In this case, the user needs to install elfutils and libelf along with binutils. Unfortunately, the provided commands ({sudo pacman -U http://archlinux.arkena.net/archive/packages/e/elfutils/elfutils-0.174-1-x86\_64.pkg.tar.xz <http://archlinux.arkena.net/archive/packages/e/elfutils/elfutils-0.174-1-x86_64.pkg.tar.xz>  http://archlinux.arkena.net/archive/packages/l/libelf/libelf-0.174-1-x86\_64.pkg.tar.xz <http://archlinux.arkena.net/archive/packages/l/libelf/libelf-0.174-1-x86_64.pkg.tar.xz>  sudo pacman -U http://archlinux.arkena.net/archive/packages/b/binutils/binutils-2.30-5-x86\_64.pkg.tar.xz <http://archlinux.arkena.net/archive/packages/b/binutils/binutils-2.30-5-x86_64.pkg.tar.xz>) are only applicable for Arch Linux and might not work on your system. Instead, you can try to update the required packages within your conda environment using the following commands:

   - Activate your R (v 4.3.2) environment (R5)
   ```
   conda activate R5
   ```
   - Update elfutils and libelf packages
   ```
   conda update elfutils libelf
   ```
   - Reinstall binutils package
   ```
   conda install --force-reinstall binutils
   ```
   After updating the required packages, try reinstalling the 'fs' and 'httpuv' packages using the initial commands provided by the user:
   ```
   R CMD INSTALL fs
   R CMD INSTALL httpuv
   ```
   Once these steps are completed, you should be able to load the Seurat package in your environment without encountering errors. ||| - Add the '--mpi=pmi2' argument to the 'srun' command to see if it resolves a NUMA error mentioned in the discussion. - Membership to the vasp5 unix group on Anvil has been added and will be ready within 4 hours. If the user cannot load the vasp.5.4.4.pl2 module tomorrow, they should let the support team know. ||| To resolve the issue, recompile VASP using updated Intel (19.1.3.304), OpenMPI (2019.9.304), and Intel-MKL (2020.4.304) libraries and load them for the job execution. The user can follow these steps to recompile VASP:

   a. Uninstall existing versions of Intel, OpenMPI, and Intel-MKL using appropriate package managers or binary installers. b. Install updated versions of Intel, OpenMPI, and Intel-MKL from their respective download pages (e.g., https://software.intel.com/content/www/us/en/develop/tools/oneapi/compilers.html)

   c. Compile VASP with the updated libraries using appropriate commands for your system (e.g., `mpicxx -O3 -openmp -I$MKL_INC_DIR -DHAVE_MKL_BLAS -DMKL_BLAS_ILP64 -DSCALAPACK_USE_BLAS=1 -o myvasp myvasp.f90`). d. Submit the compiled VASP to the HPC system for job execution. ||| The vtvt tool can be installed on both VASP 5 and VASP 6 versions. To install it on VASP 5.4.4 on Anvil, add the following commands to your submit script:

```bash
module --force purge
module load gcc/11.2.0 openmpi/4.1.6
module load vasp/5.4.4.pl2-vtst
``` ||| To resolve the issue, try reloading the module environment by executing the command `module purge` followed by `module spider vasp`. If the error persists, it's possible that the issue was temporary and has now been resolved. The user can attempt running their calculations again. If further issues arise, they should contact support for assistance. ||| To create a new module for VASP 6.4.3, follow these steps:

   a. Log in to the HPC system as your account (e.g., `ssh user@anvil.rcac.mydomain`). b. Navigate to the shared modules directory (e.g., `cd /opt/modulefiles/apps`). c. Download the VASP 6.4.3 source code from the given location using the command: `wget <source_code_URL>`. d. Extract the downloaded tarball using the command: `tar -xvf vasp.6.4.3.tgz`. e. Create a new modulefile for VASP 6.4.3 by copying an existing one (e.g., `cp vasp.6.3.0 vasp.6.4.3`). f. Modify the copied modulefile with the appropriate path to the source code directory and other necessary configurations. g. Submit the newly created modulefile to the modulesystem (e.g., using `moduleproc add <modulefile>` or the appropriate command for your system). h. Once processed, load the new VASP 6.4.3 module using the command: `module load vasp/6.4.3`. ||| - The issue reported is that the Lmod software has detected a license error for the VASP module, and that users need to be confirmed by the VASP team before access can be granted. - Additionally, the mpirun command was unable to find the specified executable file (vasp\_std). - To resolve this issue:
  - Ensure that the user has been approved for use of the VASP module under the appropriate license. - Double-check the spelling and correct syntax of the mpirun command, particularly with regards to any parameters or options specified. - Confirm that the vasp\_std executable file is located in the expected directory (/opt/spack/cpu/openmpi/4.1.6-745pfv4/gcc/11.2.0/vasp/6.3.0) and is accessible by the user running the mpirun command. - If necessary, update the module load command to ensure it correctly specifies the vasp/6.3.0 module and loads any required dependencies (e.g., openmpi/4.1.6, gcc/11.2.0)."
3,3,20,20,"User needs assistance in accessing the Anvil HPC system at Purdue University, as they are a Primary Investigator (PI) for a project and need to upload data into their system. ||| User requires access to Purdue Anvil HPC system for VASP (Version 5.4.4) with grant number geo170003 and is unable to submit jobs due to an error related to file 'runTime.txt' not found, as well as a license issue. ||| User is experiencing an error while loading OpenFOAM modules and having issues with command execution, specifically with snappyHexMesh, on Anvil HPC resource. User also seeks guidance on parallel speed for OpenFOAM simulations. ||| Unable to run IDL (Interactive Data Language) on Purdue Anvil due to lack of available module. ||| User encountered an error running Pytorch codes on Anivil due to a missing file or directory for the Conda module in Lmod. ||| Unable to install a specific application (name not provided) on Anvil due to missing MPFR library version requirement. ||| User with an ACCESS allocation on Anvil seeks access to the FORTRESS tape library at Purdue for long-term storage of large simulation data (20+ TB). ||| User on Purdue Anvil (x-mjarcari) cannot find the module to load NAMD3.0, and is unsure if the installed version has been updated to the official release. ||| Access to G16 software on Anvil HPC not listed on the ACCESS website ||| User requests installation of ABySS vs. 2.3.8 as a module to replace or complement existing versions (ABySS 2.3.2 and ABySS 2.3.4) ||| Error executing ABySS script on SLURM batch system due to invalid identifier 'ABYSS-P'. ||| User requires assistance on downloading, installing, and running MitoHifi BioContainer on Purdue Anvil as it is not one of the pre-installed BioContainers. ||| The user needs the CGAL module (specifically version 6.0.1) to run Hy2Foam, as it has been removed in the latest HPC update and the research cases are now halted due to this. ||| User is experiencing issues with the 'purge_dups' module on Purdue Anvil, as the required executables are not precompiled and need to be turned into executables for use on the platform. ||| Request to add Anvil resource to project EES210015 for running *anvi'o* tool for microbiome research ||| User (x-hli16) cannot access project directories bio240060 and bio240291 on Purdue Anvil due to permission denied error. ||| User requires a newer version of the Nextflow module on Purdue Anvil to run the blobtools2 pipeline as the currently available versions do not meet the minimum requirements. ||| The user is unable to run the ""Screen the genome"" step of FCS-GX genome decontamination efficiently on Purdue Anvil due to insufficient RAM. ||| Inability to access the Purdue Anvil Open OnDemand portal (https://ondemand.anvil.rcac.purdue.edu) from the Kansas State University network for molecular dynamics simulations. ||| User needs to use FFmpeg on Purdue Anvil and seeks guidance on how to proceed due to sudo privileges requirement for installation.","To use Anvil, the user must be under an active allocation or be the PI of an active allocation. The user should either contact the PI of the existing project on Anvil to add them, or their group should apply for a new allocation on Anvil. The user can find instructions on how to submit a proposal and register on ACCESS (Allocation Committee for Computing, Storage and Data Services at Argonne National Laboratory) by visiting the following links:
   - Prepare Requests Overview: https://allocations.access-ci.org/prepare-requests-overview
   - Learn how to submit a proposal: https://allocations.access-ci.org/prepare-requests-overview ||| To resolve the issue, add the user to the VASP license by verifying their registered email addresses for the VASP license. Here are the provided email addresses: (1) [mailto:primary_email], (2) [mailto:additional_email]. Once added to the license, re-open the support ticket if further issues occur. The user should then be able to use vasp5 and submit jobs without encountering the mentioned errors. ||| To resolve the issue with loading OpenFOAM modules, it appears that the user has figured it out already. For ensuring all commands work, the user should run scaling tests for their system to determine the optimal number of cores for their calculations. This can be done by running a few iterations with different numbers of cores (e.g., 16, 32, 64, 128, 256) and recording the time used for each calculation. The efficiency can then be calculated according to the tests with different numbers of cores. Regarding parallel speed, it is recommended that the user runs scaling tests to maximize the efficiency of their OpenFOAM simulations on Anvil HPC resource. The user should note that SLURM will find available resources for each job based on priority, and small jobs will start quicker than big ones due to this scheduling behavior. With scaling tests, the user can determine how many cores they should use to maximize efficiency and guarantee they do not request too many cores and wait too long in the queue to get a job started. Example job submission script for OpenFOAM:
```bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=16
#SBATCH -n 16
```
or
```bash
#SBATCH --ntasks=16
```
For running a scaling test, the user could modify the above script to run with different numbers of cores (e.g., 4, 8, 16, 32) and submit it multiple times to observe the performance with various core counts. ||| If you have a valid license, you can install IDL in your project space or home directory on Anvil. To do this, follow the installation instructions provided by the IDL documentation. You can find the official documentation here: <https://www.harrisgeospatial.com/docs/idl/>. Once installed, you should be able to access and run IDL on your Purdue Anvil account. ||| The user should reload the Conda module by executing the following command in the terminal:

```bash
module load anaconda/2024.02-py311
```

After loading the module, the user can attempt to run their Pytorch codes again. If the error persists, it may be necessary to confirm that the Conda environment with Pytorch is activated using the following command:

```bash
conda activate py311
```

Replace ""py311"" with the appropriate Conda environment name if different. The user should also verify that the Python executable (python) is present in their system's PATH and ensure there are no typos or incorrect paths in their script. If the issue still cannot be resolved, it may be helpful to consult the relevant documentation for Anvil, Lmod, Spack, Conda, and Pytorch. [Documentation links would be added here if available] ||| The user can attempt to download and install the latest version of MPFR (4.2.1) from https://www.mpfr.org/mpfr-current/ by themselves in their own space and use it with the application. However, please note that since this is an open source software, a central upgrade may not be possible unless requested by many users. ||| It is possible to grant access to the FORTRESS tape library by setting up a Globus share, which will allow the user and their group to store the desired amount of data (minimum 20 TB) for long-term use while enabling follow-on analysis and comparison with later simulations. The HPC support team can send an invite for access once they have confirmed if it is intended for the user or their entire group. ||| To load the NAMD3.0 module on Purdue Anvil, use the following command in your terminal: `module load namd/version_number`, replacing `version_number` with the correct version number of NAMD3.0. You can find the correct version number by visiting the official NAMD website (<https://www.ks.uiuc.edu/Research/namd/>) or checking the Anvil system documentation. If you are unsure about the installed version on Purdue Anvil, please contact your system administrator for assistance. ||| To clarify, the user is having trouble accessing the G16 software on the Anvil HPC because it is not explicitly listed on the ACCESS website. However, upon further investigation, it has been confirmed that the G16 software is indeed available on the Anvil HPC. Users can find more information about using G16 on this link: [Gaussian 16 documentation](https://purduercac-applications.readthedocs.io/en/latest/Applications/gaussian16.html). For a list of available software on the RCAC website, including Gaussian16, visit: [RCAC Gaussian16 page](https://www.rcac.purdue.edu/knowledge/applications/gaussian16). ||| ABySS 2.3.8 has been made available on the Anvil login nodes, with plans to roll out to the compute nodes over the next few hours. The user is advised to check if there are any issues with the installation after this rollout. ||| Modify the shebang line in the submission script from `#!/bin/sh -l` to `#!/bin/bash`. If this does not resolve the issue, contact support for further troubleshooting. Additionally, when running ABySS on multiple threads, use the 'j' flag on a single node. For more information, consult this ABySS guide: [http://sjackman.ca/abyss-activity/](http://sjackman.ca/abyss-activity/). ||| The user needs to follow the tutorial provided at this link: https://rcac-bioinformatics.readthedocs.io/en/latest/mitohifi.html. To use the container, Singularity or Apptainer (known as Docker on Anvil) should be installed, which is available on Anvil. The user can pull the mitofish biocontainer to their scratch folder on Anvil. If there are any questions or specific topics the user would like to see covered in the documentation, they can reach out for further assistance. ||| The user is advised to load the openfoam module using the command `module load openfoam`. It should include the required CGAL module if it is part of the openfoam module. If loading the openfoam module does not resolve the issue, the user is encouraged to either reopen this ticket or submit a new one for further assistance. ||| The user can resolve this issue by loading the updated 'purge_dups' module using the following commands:

```
module load ml
module load biocontainers
module load purge_dups/1.2.6_fixed
```

These changes should take effect by tomorrow. If the user still cannot find the executables after that, they should contact support for further assistance. ||| To add the Anvil resource to the specified project, follow these steps:

   - Log in to allocations.access-ci.org using your credentials
   - Click on the ""Credits + Resources"" tab
   - Request multiple resources within one exchange by clicking ""Add a resource to your exchange."" - Select the Anvil resource from the list of available resources that appears
   - Enter the required amount of units (e.g., hours or days) for the requested resource
   - Provide a brief justification for requesting the resource, such as the intention to run *anvi'o* tool for microbiome research
   - Click ""Submit for Approval""
   - Confirm the amounts being requested are correct and click ""Submit""
   - You will receive an email notification confirming your successful request. Once a decision has been made, you will receive a separate notification. For more information about Exchange Requests, please refer to: https://allocations.access-ci.org/how-to#exchange-credits

If you have any questions along the way or in the future, please submit a ticket at https://support.access-ci.org/help-ticket ||| To resolve this issue, you need to verify and modify the permissions for the affected project directories (/anvil/projects/x-bio240060/ and /anvil/projects/x-bio240291/). As a system administrator or project owner, execute the following command on the Anvil cluster to change the owner of the directories:

```bash
chown -R x-hli16:anvil /anvil/projects/x-bio240060/
chown -R x-hli16:anvil /anvil/projects/x-bio240291/
```

After executing the above commands, the user should be able to access their project directories without encountering the permission denied error. For more information on file permissions and ownership, refer to the [Anvil User Guide](https://docs.rc.purdue.edu/anvil-user-guide/#file-permissions). If the issue persists, please reach out again for further assistance. ||| To install the latest version of Nextflow in your home directory, follow these steps:
   ```
   module load openjdk
   cd ~
   curl -s https://get.nextflow.io | bash
   mkdir -p $HOME/.local/bin/
   mv nextflow $HOME/.local/bin/
   nextflow -version
   ```
   This will place Nextflow in your local `$HOME/.local/bin/` directory, allowing you to manage updates independently. ||| To resolve this issue, it is recommended to use the highmem partition on Purdue Anvil, which offers nodes with ~1 TiB RAM. The user can submit their job using a SLURM script similar to the following:

```bash
#!/bin/bash
#SBATCH --job-name=FCS_GX
#SBATCH --partition=highmem
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=48  # Adjust as needed based on the number of CPU cores available on your node
#SBATCH --mem=512000  # In Megabytes, adjust as necessary to suit your exact requirements

module load singularity
singularity exec --nv fcs-gx.sif fcs-gx <arguments>
``` ||| The issue appears to be related to a potential network restriction from Kansas State University (KSU). At this moment, there is no known workaround other than suggesting using an alternative network or connection method. The Purdue team is currently investigating the reported connection issues with campus networks and will update when more information is available. ||| To use FFmpeg on the Purdue Anvil cluster, load the module directly using the command `module load ffmpeg/4.2.2`. This command will make the specified version of FFmpeg available in your environment without requiring sudo privileges to install the software."
5,2,22,22,"The user is experiencing an issue with the filesystem of Anvil where they cannot copy a file to a new-created directory in the specified path. The problem seems to be affecting directories under /anvil/projects and /anvil/scratch. ||| User is experiencing intermittent issues with Anvil's GPFS filesystem, specifically listing files, determining folder sizes, and using vim editor. The stalling occurs approximately 5 times during a 10-day period. ||| User cannot read or write files generated by simulations from the shell or Python scripts in specified directory on HPC system. Error messages include ""Input/output error"" and ""OSError"". ||| User is unable to edit or view certain text files on Anvil using the ""nano"" editor due to a magic_file(parameters.prm) failed error. ||| User is unable to copy files from a specific directory to another due to Input/Output error ||| User is unable to access files in scratch directory on login04 due to a ""stale file handle"" error. ||| User Hannah Bish is encountering an issue with uploading files to her home directory using ACCESS OnDemand with the error ""Upload failed"". ||| Filesystem commands (ls and rm) are hanging on a specific directory in the Purdue Anvil project filesystem for user x-phy230179. ||| User cannot transfer files through command line using SSH authentication on Anvil due to unsupported password-based SSH authentication; user successfully transfers files with SSH keys using MobaXterm ||| User is unable to change directory into `/anvil/scratch` due to a stale file handle error. The issue occurred around 2am EST today and was resolved by engineers in the morning, but the user still encounters the problem. ||| The user is unable to change directories (cd) into any of Anvil's directories (scratch, etc.), and receives a ""stale file handle"" error even when attempting to cd into the root directory of Anvil (/anvil). ||| User encountered a stale file handle error when using Anvil and is unable to execute commands like cd or ls. The user suspects that the issue may be related to recent model compilation attempts on Anvil, and is concerned about potential data loss due to lack of backups. ||| User Seyyedfaridoddin cannot write or edit files due to their home folder being full (23.6 GB out of 25 GB). ||| User accidentally deleted contents of their home directory with the command `rm -rf /home/x-vsharma6/`. ||| Permissions on Anvil home directory have been accidentally changed, resulting in loss of access to the home directory. ||| User Wenliang was unable to access home directory on OnDemand due to an unavailable mount or needing creation. ||| Unable to access https://ondeomehmand.anvil.rcac.purdue.edu/ due to ""Home directory not found"" error ||| User Moaz cannot access Anvil through OpenDemand due to a missing home directory error. ||| User is unable to copy files from a specific directory on Anvil due to permission denied error. ||| The user (Doug) reports that the ""myquota"" issue on ATS-12974 is causing issues with filesystems on Anvil, resulting in an inability to launch certain applications, such as ondemand applications and Firefox, which enter device wait and never fully launch. Additionally, all commands hang and new logins cannot be made. ||| PhD student under the NAIRR program is unable to create or save files in their home directory due to intermittent NFS server connectivity issues with zfs.anvil.rcac.purdue.edu. ||| User was unable to upload documents to AnvilGPT","The engineering team is working on resolving the glitches impacting the Anvil GPFS filesystem. In the meantime, several tweaks have been made to Anvil's file system to improve its performance. The user should experience normal performance at this moment. Further updates will be provided when the issue is fixed by vendors. ||| The reported issue appears to be related to the Anvil GPFS filesystem. While our engineering team is still investigating the root cause, temporary workarounds have been implemented that should improve performance at this time. Users can expect further updates once the vendors resolve the underlying problem. In the meantime, it is recommended to regularly save work and exit applications such as vim when not in use to minimize potential stalling issues. The affected folder is /anvil/scratch/x-samlobo/SARS1/01\_NPT\_equilibration. ||| The issue was caused by a glitch on the filesystem that has since been fixed by the engineering team. User is advised to try accessing the files again to verify the resolution. ||| The issue seems to be related to file permissions on the specific directories containing the parameters.prm file. To resolve this, navigate to the affected directories (/anvil/projects/x-dmr110007/PRISMS\_name/phaseField/applications/alloySolidification\_DSI-R\_full and its subdirectories) using the command line, and then change the permissions recursively to make them writable for the user. Here's an example of how to do it:

```bash
cd /anvil/projects/x-dmr110007/PRISMS_name/phaseField/applications/alloySolidification_DSI-R_full
chmod -R 755 . ```

In the above command, `chmod -R 755 .` changes the permissions to be writable by the user (7) and readable and executable for others (+x). Replace the directory path with the paths of the other affected directories. Alternatively, you can change the file permission using a GUI file manager if available on your system. Make sure to set the permissions on the files and their parent directories recursively (using the ""Recurse into subdirectories"" option). After applying these changes, the user should be able to edit and view the parameters.prm file without encountering the magic\_file(parameters.prm) failed error. ||| To resolve the issue, the user should investigate the cause of the Input/Output error in the specified directories (`pbe-scf/WAVECAR`, `pbe-scf/czs.3195898`, and `pbe-scf/CHGCAR`). The following command can be used to recursively copy the 'pbe-scf' directory to the destination:

```bash
cp -r pbe-scf/ hse
```

If the issue persists, it is recommended to check file permissions, validate file integrity, and ensure that the source and target directories have enough storage space. If necessary, consult the user's HPC documentation or seek additional assistance from the support team within 7 days. ||| The issue has been fixed by the support team. Users should now be able to access the files in their scratch directory on login04. ||| To resolve this issue, Hannah can delete some files from her home directory if it's nearly full and then try uploading again. Alternatively, she can use scp (Secure Copy) to transfer the files directly to the Anvil system instead of relying on ACCESS OnDemand for large files. Additionally, it is recommended to utilize project space for storing larger files due to a 5TB quota, as compared to the home directory's 25GB quota. To navigate to her project space, Hannah can use the command:

```bash
cd /anvil/projects/x-phy230181
```

Hannah should always check the storage quotas using 'myquota' command on Anvil to understand the available space for each storage type. For more information about different storage spaces on Anvil, please refer to this user guide: [Anvil Storage - Filesystems](https://www.rcac.purdue.edu/knowledge/anvil/storage/filesystems) ||| Run the command `rm -rfv /anvil/projects/x-phy230179/wham/run/20240226.BEAM.nz256.tester-anvil/particles/T.0` to remove the specified directory and its contents. If the problem persists, try removing files individually instead of recursively using the command `rm -vf /anvil/projects/x-phy230179/wham/run/20240226.BEAM.nz256.tester-anvil/particles/T.0/*`. If you still encounter issues, please contact the support team for further assistance. ||| To transfer files using SSH on Anvil, follow the instructions provided in this link for setting up SSH keys: https://www.rcac.purdue.edu/knowledge/anvil/access/login#with\_ssh. After setting up SSH keys, you can use a tool like MobaXterm to transfer files. If you encounter any issues or require further assistance, do not hesitate to reach out for a virtual meeting. ||| To resolve this issue, try reconnecting to the nodes `\_g005.anvil.rcac.purdue.edu` and `\_g008.anvil.rcac.purdue.edu`, and then attempt to change directory into `/anvil/scratch`. If you still encounter the same error, it is recommended to contact the support team for further assistance. There was an issue with Anvil storage in the early morning (~2am EST today), but engineers were able to isolate and fix the underlying problem. However, it's possible that the user may need to refresh their session or restart their application to reflect the changes made by the engineers. ||| Engineers at Purdue University have identified an issue with Anvil storage that occurred around 2am EST today. The underlying problem has been isolated and fixed first thing this morning. Users experiencing this issue are encouraged to verify if the problem persists. ||| To resolve the stale file handle error, it's recommended to run the command `rm -rf .Slurm_XXX` in the user's current directory (replace ""XXX"" with the job ID of the last failed Slurm job). If the user is unsure about the job ID, they can find it by running `ls -ltrh | grep slock` to list all lock files sorted by time and looking for the most recent one. After running the above command, the user should be able to execute commands like cd or ls again. However, since the user is concerned about data loss due to unsaved results, it's essential to regularly backup the data stored on Anvil. Users can use tools like rsync, scp, or Globus to transfer files between their local system and Anvil for backup purposes. Here's a brief example using `scp`:

```
scp -r username@anvil-login.example.com:/path/to/local/backup /path/to/remote/backup
``` ||| To resolve this issue, user Seyyedfaridoddin needs to move or delete some files in their home directory. They can check the disk usage with the 'myquota' command. ||| To recover these files, use the 'flost' command. Browse snapshots using the following command: `flost -w /home`. Specify the date that you deleted the files and it will search for a snapshot from around that time. More details about 'flost' can be found here: [https://www.rcac.purdue.edu/knowledge/anvil/storage/recover/flost](https://www.rcac.purdue.edu/knowledge/anvil/storage/recover/flost) ||| To resolve this issue, you can change the permissions back to their original state using the `chmod` command with an appropriate value (e.g., 700 or 755 for directories, and 644 or 640 for files). First, navigate to your home directory using the `cd ~` command. Then, run the following command to change the permissions of the entire directory (in this case, replace the number with an appropriate value such as 755):
```
chmod 755 /path/to/your/home/directory
```
If you would like to change specific files or subdirectories within your home directory, navigate to their location and run the appropriate `chmod` command (e.g., for a file with a 640 permission value):
```
cd /path/to/specific/file
chmod 640 filename
``` ||| Deploy a patch that should resolve the issue for users. ||| A patch has been deployed that should resolve the issue for users experiencing this problem. ||| The user should attempt to restart the web server on Anvil by clicking the provided button once the issue has been resolved (assuming it was an intermittent problem as stated in the message). If the user continues to encounter issues, they can try to re-mount their home directory or create a new one by following these steps:
   - On OpenDemand, go to the Home Directory section of your profile settings. - Check if your home directory is already mounted and accessible. If not, attempt to manually mount it using the appropriate command for your system (e.g., `mount -t nfs <server_ip>:/<path> /home/<username>`). - If the user's home directory needs to be created, use the `mkdir -p /home/<username>/` command on the Anvil server to create a new one. - In both cases, ensure you have the necessary permissions to perform these actions. If not, contact your system administrator for assistance. ||| The user does not have access to the TDM group on Anvil. It is recommended that the user contacts The Data Mine group (mailto:[TheDataMineGroupEmail]) to request for their account to be granted access to the group. Once access is granted, the user should be able to access the specified directory without receiving a permission denied error. ||| To resolve the issue, it is recommended to perform a deep filesystem check (fsck) on the affected filesystems. On Anvil, this can be done by following these steps:
   - Log in as the root user or use sudo commands if necessary. - Navigate to the root of the problematic filesystem using the command `cd /path/to/filesystem`. - Run the deep filesystem check with the fsck command, for example, `fsck -p /path/to/filesystem` (replace '/path/to/filesystem' with the actual path to the affected filesystem). This command will automatically correct any found errors. - After the filesystem is checked and repaired, try launching applications again and monitor for any further issues. If problems persist, consider rebooting the system or seeking additional assistance from HPC support. ||| Investigate the NFS server connectivity problems by reviewing system logs using `dmesg` command for repeated entries indicating the server is not responding (e.g., ""nfs: server zfs.anvil.rcac.purdue.edu not responding, still trying""). Address any network issues that may be causing the NFS server to become unresponsive. To ensure successful write operations to the home directory, try restarting the NFS server or contact your system administrator for assistance. If possible, temporarily switch to another home directory or use a local filesystem for writing data during this resolution process. Additionally, keep monitoring the server status and log entries for any improvements. ||| The issue has been fixed by the support team and the user can now upload files without any problems. No additional assistance is needed."
8,0,30,30,"Requirement for a significant increase in quota for Anvil scratch space files due to large number of simulation data cubes generated by each processor. ||| Configuring S3 Storage on Anvil Composable for Access Information Sharing ||| User on the anvil cluster at Purdue has exceeded the maximum number of files allowed in their project due to a large number of images in their dataset and requires a limit increase for accurate model training. ||| User needs guidance on how to store data generated during turbulence simulations on Anvil HPC system. ||| User needs more storage for a specific experiment on the Anvil cluster as their home directory has only 25 GB of available storage. ||| User wants to change the storage quota for their users from 25GB to more on Anvil HPC cluster. ||| Request to increase file number limit (quota) on the Anvil scratch partition for Image classification model research. ||| Requested setup of S3 storage for namespace axin-hub on Anvil for Triton server deployment; issue with provisioning new users in existing Anvil S3 storage. ||| Data Mine students exceeding their /home quota on the Anvil HPC system without realizing it, causing issues with data management and workflow. Currently, there is no way for TDM staff to check individual student quotas proactively. ||| User requested an increase in their ANVIL scratch folder quota for 90 days due to large data sets and optimized workflow. ||| User Eeshan needs an increase in the file number limit and scratch storage space on Anvil HPC for running multi-year, multi-domain WRF-Chem simulations as a part of his PhD research. ||| File limit per user on Anvil resource is hindering efficient upload of datasets due to counting of small files towards file limit. ||| User needs an increase in their Anvil project storage from 5TB to 40TB due to large unstructured data sets required for a global climate simulation and neural network training. The data is needed for a period of about a year. ||| User needs to increase their file number limit on Anvil due to current batch of simulations writing a large number of files and running into the 100,000 file count limit quickly. ||| User inquiry about accessing Anvil persistent volume storage from applications running on Geddes and sharing those persistent volumes across systems. ||| User inquired about storage allocation upon obtaining an Anvil allocation to determine if it is a suitable replacement for the expiring Rockfish resource. ||| User requires assistance in requesting storage on the ANVIL HPC system. ||| Request to increase disk space for project ees240038 on Purdue Anvil system and inquiry about data management policies for scratch directories. ||| User needs assistance in increasing the temporary storage capacity on their Anvil account. ||| User Wenliang requires an extension of their file number and disk size quota on Anvil for six months due to ongoing project work. ||| Request to increase file number limit on user's Anvil directory to 5 million files for a suite of simulations in Enzo. ||| User is unable to download the 'Places365' dataset due to a file limit count exceeding the limit on Anvil GPU clusters. The user requires an increase in the limit or suggestions for smaller datasets. ||| The user is experiencing an issue where the {{myquota}} command does not display the relevant information properly in the 'tests' directory on Anvil HPC system. ||| Request for an increase in storage allocation for /anvil/projects/cis220051 from 20TB to 30TB. ||| The user's HPC storage limit was exceeded during the installation of an ollama model, preventing them from accessing any apps on Anvil. ||| A graduate student at Purdue is unsure if the Anvil HPC cluster can handle large-parameter models (e.g., Llama 3.1 405b and Deepseek r1 670b) for fine-tuning, and wants to know if it's possible to finetune these models on the Anvil cluster itself instead of uploading them from another source. ||| First-time Anvil user is unable to access Scratch Anvil and Projects X-bio250049 storage, and their quota is insufficient for required tasks. ||| User wants to exchange 200,000 Anvil core hours for temporary Anvil storage but can't find a way to do it in Resources. ||| User requests an increase in file storage for project 520052 from 5TB to 10TB to accommodate the Waymo Dataset (~4TB), nuScenes Dataset (~0.5TB), checkpoints (~1TB), and processed files (~2TB). ||| Request for increase of instance quota from 6 to 10 for G3.L flavor on Jetstream2 HPC cluster due to course requirements.","To address the user's requirement, you can consider increasing their Anvil scratch space quota. The current limit of 1,000k files is not sufficient for the user's workload, which amounts to about 6.2e6 files per simulation. It would be best to increase the quota to accommodate this number, or, as a temporary solution, provide them with a higher limit for the time being. Note that each file is relatively small and does not require additional storage space at this time. ||| To configure S3 storage on Anvil Composable and share the access information, follow these steps:

   a. First, make sure you have AWS CLI installed and configured with your credentials (access key ID and secret access key). If not, refer to the [official documentation](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html) for installation and configuration instructions. b. Next, set up your S3 storage bucket by executing the following command in your terminal:

      ```bash
      aws s3api create-bucket --bucket <your_bucket_name> --region <your_region>
      ```

   c. After creating the bucket, grant Anvil Composable access to it using AWS Identity and Access Management (IAM). Create a new IAM user with necessary S3 permissions and attach the policy to the user. For detailed instructions, refer to the [official documentation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html#id_roles_create_for-user_prereqs). d. Once done, share the access and secret keys of the created IAM user with your support team for further configuration on Anvil Composable. ||| The user was advised to consider using archive formats (`.zip` or `.tar`) for their image dataset and also to process images from their personal scratch directory (`/anvil/scratch/x-ldenoncourt`), as it is more performant than the group's project space. Additionally, the file number limit of the user's project (`x-bio230069`) was increased to 2M. The user can check the updated limit using the `myquota` command and should be aware that inactive files in their personal scratch directory will be purged after 30 days if not accessed. ||| On Anvil, users receive a personal scratch directory (quota of 100TB and 1M files) for temporary storage of working data. For long-term data storage and sharing, users get a project directory with a quota of 5TB and 1M files per project. The user should check the quotas and usage of directories using the `{{myquota}}` command on Anvil. The user's accounts and directories may take some time to be completely set up, but once ready, they can access their scratch directory at `/home/x-senocak` and project directory at `/anvil/projects/x-ees230052`. For regular backups, the user should be aware that files in the scratch space will be purged if not accessed for 30 days. To avoid data loss, it is recommended to back up the data on a regular basis. ||| The user's scratch directory on Anvil has a quota of 100TB storage and 1M files, which should be sufficient for their data storage needs. They can check the usage and quotas with the `{{myquota}}` command on Anvil. For more details, they can refer to the RCAC - Knowledge Base: Anvil User Guide: File Systems at this webpage: [RCAC - Knowledge Base: Anvil User Guide: File Systems](https://www.rcac.purdue.edu/knowledge/anvil/storage/filesystems) ||| To modify the storage quota for each user, run the command `myquota <username>`. For example, to check the current quotas of the user 'x-vsharma6', you can run the following command:

```bash
$ myquota x-vsharma6
```

This will display the storage usage and quota for specified users. To change the quota, please follow these steps:

1. Run `myquotactl -g <username>` to set a new group quotas or `myquotactl -u <username>` to set user quotas. 2. Set the 'hard limit' and 'soft limit' for the required storage allocation (e.g., for a user with 50GB quota, use `myquotactl -u x-vsharma6 --hard-limit=50G --soft-limit=50G`). 3. Verify the new quotas by running the command again: `myquota x-vsharma6`. Please be aware that changing file system quotas may affect existing files if they exceed the new quota limits. To ensure smooth operation, please plan accordingly and monitor usage regularly. For more information on storage options in Anvil, refer to the documentation at this link: [Anvil Filesystems](https://www.rcac.purdue.edu/knowledge/anvil/storage/filesystems) ||| To address this issue, you will need to contact the Anvil HPC support team to request an increase in your file quota on the Anvil scratch partition. Please provide them with details about your current usage (e.g., 2 Million file limit) and the reason for the request (improving Image classification model research). Once you have submitted this request, the Anvil HPC support team will review it and respond with instructions for increasing your quota. ||| To set up S3 storage for the namespace axin-hub, execute the following command: `anvil s3 create axin-hub`. If immediate access is needed, provision keys for RCAC object storage. For problems with existing S3 storage user provisioning, wait for Sam's resolution or use the provided RCAC keys instead. The new S3 keys have been sent via filelocker. ||| Implement a cron job on zfs.anvil that runs the following command as root:
   ```
   zfs userspace -p tank/home | grep -w -f <(getent group x-cis220051 | sed -e 's/^.\\*://; s/,//g') > /anvil/projects/tdm/log/tdmquotas
   ```
   This command will output the quota usage for each member of the x-cis220051 group in the specified log file, which is only accessible by TDM staff. With this information, scripts can be written to send email alerts to students who are over quota or perform other necessary actions. Note: The location where the output is written isn't critical; it could be within /anvil/projects or even in /tmp on zfs.anvil. However, accessing the file from other locations would require an extra ssh connection. An alternate suggestion is for students to run ""myquota"" commands themselves or use helper scripts that check their quota usage and perform sanity checks like checking for excessive Python package usage in ~/.local. It was also mentioned that email quota alerts could potentially be re-enabled on the Anvil HPC system in the future. ||| - The default policy is that files older than 60 days on the Scratch folder will be purged, but if the file is used every day or frequently, it will remain. Anvil is not designed for long-term storage, but computing, and users have HOME (25GB), project (5TB), and scratch (100TB) for storage and computing purposes respectively. - The storage team recommended optimizing the workflow to avoid storing large datasets in one place, as extending the scratch quota is not a permanent solution. They also suggested being aware of the file number limit (1000k) on Scratch. The current request for a 200TB increase in scratch quota for 90 days has been acknowledged and will be forwarded to the storage team. However, it's essential to note that there is a limit of 1000k files on Scratch due to performance considerations. ||| The user's files quota has been extended to 20M and he now also has 200T blocks quota available. This increase will last until the end of the calendar year, after which it will be reset back to 1M. Users are advised to monitor their usage accordingly. ||| To alleviate this issue, it is recommended to use scratch directories for storing working files and transfer results to a long-term storage space. This could help avoid the file count limit issue while maintaining performance and flexibility in data management. Additionally, sharing more details about the datasets (e.g., their size, duration of usage, users involved, programs run, and data management strategies) would allow for further optimization recommendations. ||| Our storage experts have increased the user's project quota limit to 40TB until the end of their current allocation (approximately 2025-03-18). The user should check if there are any other questions and contact us if needed. ||| The user's file quota has been increased from 1,000,000 to 5,000,000 files. ||| Currently, Geddes and Anvil each have their own Ceph and Persistent Volume Claims (PVCs) cannot be accessed between resources. A new central Ceph resource is being deployed that will be available across both systems, but it is not integrated yet. The team plans to integrate this central Ceph resource in the future. ||| To find details about the Anvil file system, access the following link: [https://www.rcac.purdue.edu/knowledge/anvil/storage/filesystems](http://example.com) The allocated storage for personal home directory is 25GB, personal scratch is 100TB, and project space is 5TB upon obtaining an Anvil allocation. ||| To request storage on the Purdue Anvil resource, a compute award must be made first. Once this is done, the Anvil staff will work directly with you and your storage needs. No specific steps are provided for requesting storage via the ACCESS program since ANVIL does not allocate their storage through it. For more information about ANVIL, please refer to their official website or contact the Anvil staff. ||| The request to increase the disk space for the project ""ees240038"" on the Purdue Anvil system has been granted, with a new limit of 20-25 TB. As for the data management policies for scratch directories, it appears that data in these directories may be purged. However, specific details regarding this policy were not provided. If authorization from the PI is required, the user should arrange for necessary communication with them. ||| The user's home directory is full, but they still have plenty of free space in {{/anvil/scratch/x-lintravaia}} and {{/anvil/projects/x-bio240052}}. To check the quotas and usage of these spaces, run the `myquota` command:

    ```
    $ myquota x-lintravaia
    ```

   For more information about the file systems on Anvil, please refer to the following documentation:

   - https://www.rcac.purdue.edu/knowledge/anvil/storage?all=true
   - https://www.rcac.purdue.edu/knowledge/anvil/storage?all=true|smart-link

   Be aware that the scratch directory is for short-term storage only, and inactive files there are purged on a regular basis. To avoid possible data loss, ensure your data is backed up promptly. ||| To extend the quota for user x-wangwl, please provide the following information to your PI for approval:
   - User Name: x-wangwl
   - Project ID: x-mcb130189
   - Directory: /anvil/scratch/x-wangwl
   - Old quota: 100T/10M
   - New quota: 500T/20M

Once approved, submit the request to the deployment team via this link: https://urldefense.com/v3/__https:/access-ci.atlassian.net/servicedesk/customer/portal/2/ATS-11622?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ0Z3QiOiJhbm9ueW1vdXMtbGluayIsInFzaCI6ImFhNmVkMDFhNDNmMmQ5NmRjZGIzMzRjMzk0YjI2MjVlYzk3YWUzNGQ0ZDU4MzE5NjQ2Y2NkNmI2YjdlYjFiZWMiLCJpc3MiOiJzZXJ2aWNlZGVzay1qd3QtdG9rZW4taXNzdWVyIiwiY29udGV4dCI6eyJ1c2VyIjoiMTI5MjYiLCJpc3N1ZSI6IkFUUy0xMTYyMiJ9LCJleHAiOjE3MzIxMjg0NDUsImlhdCI6MTcyOTcwOTI0NX0.mnP4bSKE98kf0mV4kwYN97f9iCdEgN7Luf8SWArVBIE&sda_source=notification-email__;
The deployment team will then address the extension request. If further help is needed, do not hesitate to ask. Best, name ||| The file quota has now been increased to 5 million files for the user's scratch directory at /anvil/scratch/x-rabatinb. If further issues arise, please contact Eli from Purdue IT via their service portal at https://service.purdue.edu or reach out directly to them at their email address provided in the messages. ||| The user can propose steps to resolve this issue by providing information about the popularity and use of the Places365 dataset in research, particularly its relevance to scene classification training. The user has also provided links to the dataset repository on GitHub and MIT website:
   - http://places.csail.mit.edu/
   - https://github.com/CSAILVision/places365

In addition, it's important to note that the user is focusing on using this dataset for testing the robustness of an adversarial attack defense framework rather than direct scene classification accuracy rate. ||| To troubleshoot the issue, it is recommended to check if there are any potential issues with files or permissions within the 'tests' directory causing the myquota command to malfunction. Additionally, running the command outside of the 'tests' directory can help determine if the problem lies within that specific directory:

   ```
   myquota home (or any other valid directory)
   ```

   Here is an example of the expected output format for the command:

   ```
   Type Location Size Limit Use Files Limit Use ==============================================================================
   home x-kkeshavarz 221.6MB 25.0GB 0.87% - - -
   scratch anvil 0KB 100.0TB 0% 0k 1,000k 0.00%
   projects x-cis240640 0KB 5.0TB 0% 0k 1,048k 0.00%
   projects x-ees240058 4.9TB 5.0TB 99% 0k 1,048k 0.04%
   projects x-ees240082 16.6TB 160.0TB 10% 276k 1,048k 26%
   ```

   If the issue persists after checking other directories and investigating any possible issues with files or permissions within the 'tests' directory, please reopen this ticket for further assistance. ||| The Anvil support team has updated the storage with an increase for the specified directory. The new size limit for the project directory x-cis220051 is now 30TB. Users can check their storage usage by navigating to the home or project directories using the command ""cd"" and verifying the size limit using the command ""du -sh"". ||| To resolve this issue, the user needs to free up some space in their HPC storage. They can do this by deleting unnecessary files or temporarily moving files to another location. However, specific steps may vary depending on the file system and tools used. For more information about managing storage on Purdue's HPC system, please refer to the documentation available at: https://service.purdue.edu ||| The student should be able to request a large model to be uploaded to AnvilGPT at <https://anvilgpt.rcac.purdue.edu/auth/>. However, any large models (such as Deepseek-671b) need to be reviewed by the Lead Data Researcher, name. If the student is able to work with moderate-sized 70b models provided on Anvil, there would be no issue. For finetuning a Llama 3.3 70B model on the Anvil cluster itself, the support team will get back to the student once they have more information about what's available on AnvilGPT and the specific model details. It's important to note that Purdue RCAC prevents users from storing HIPAA data on non-encrypted or common access storage solutions such as Data Depot. For more info, see <https://www.purdue.edu/legalcounsel/HIPAA/>. ||| To access the Scrchat Anvil and Projects X-bio250049 storage, navigate to your home directory by using the following command: `cd ~`. To view the directories for Scratch Anvil and Projects X-bio250049, use the `ls` command. If the user's quota needs to be increased, they can submit a ticket to RCAC Support at https://service.purdue.edu. It may take up to a couple of business days for a response. ||| To request the exchange of 200,000 Anvil core hours for temporary Anvil storage, please follow these steps:
   - Log into your Anvil account on the HPC system. - Navigate to the ""Storage"" tab instead of the ""Resources"" tab. - In the ""Storage"" tab, you should see an option to exchange your Anvil core hours for temporary Anvil storage. - Enter the number of hours you wish to exchange (200,000 in this case) and submit the request. - Once the request is processed, the requested temporary Anvil storage will be available to you. If you encounter any difficulties during this process, do not hesitate to reach out for further assistance. ||| To address this issue, follow these steps:
   - Log in to the HPC system and navigate to the project directory: `/anvil/projects/ai250052/`
   - Use the following command to expand the storage limit:
     ```bash
     salloc --adjust=storage=10T /bin/bash -l
     chmod -R a+w . edquota -t user -p disk -u 520052 10T
     ```
   - Ensure that the new storage limit takes effect by logging out and logging back in or running any command that requires file system access. ||| To resolve this issue, please contact the Jetstream2 support staff at Indianapolis University as they manage the compute provider. For further assistance, refer to their official support website or documentation for submitting tickets and contact information."
1,2,23,23,"User is unable to compile and submit the hydrodynamical code ENZO on Anvil, despite having successfully run it on Stampede2 with specific modules loaded. The user has managed to compile the newer version of ENZO but is still having trouble submitting the job successfully. ||| BerkeleyGW software jobs aren't running as expected on Anvil, leading to errors. ||| User is experiencing issues with the login03 node. ||| User needs to use hdf5/1.8 instead of the currently available hdf5/1.10 on Anvil while running FLASH code. ||| User is encountering an error when trying to submit a sbatch job on Anvil with multiple nodes and tasks using WRF (WRFU_TimeSet(startTime) FAILED), but the job runs successfully with 1 node and 1 task. ||| User is trying to analyze output with matplotlib but getting an error due to the version needed. The user is unsure if the issue is related to the Python version available through Anvil and needs guidance on installing a different Python version if necessary. ||| Code running successfully despite the warning ||| User is experiencing slow performance with the Nek5000 solver and seeks guidance on optimal compiler configuration for improved performance. ||| User encounters a ""404 Bad Request"" error when accessing the data mine Jupyter lab. The issue seems to be related to an oversized request header field. ||| Loading Anaconda on Anvil fails due to a missing file error in the conda script. ||| User is experiencing performance issues with Nek5000 code on the Anvil Cluster, including OpenFabrics warnings and slow simulation speeds. ||| User Eoghan encountered an issue while compiling Quantum Espresso and another software using the Intel libraries on Anvil, as the jobs fail when requesting more than 32 cores on a node. ||| User requires OpenFOAM version 11 on ACCESS sites (Anvil or Darwin) for viscoelastic flow calculations. ||| Unable to run IQtree on Anvil cluster due to 'command not found' error despite loading the iqtree module ||| The user is experiencing issues with running a specific version of 'name' on the Anvil cluster due to an out-of-memory (OOM) error when using packages and permission denied error while installing packages on version 'name 1.9.3'. ||| User is unable to invoke the samtools module in their Perl script on Purdue Anvil, causing the script to fail. ||| The user is experiencing slow run times due to a WARNING related to OpenFabrics device initialization and UCX performance degradation or data corruption. ||| User is encountering an error initializing OpenFabrics devices in jobs running on HPC Anvil. The issue appears consistently across all jobs for a specific project. No job history or error details were found using slurm commands. ||| User cannot run a program on ANVIL that requires java as it appears not to be installed and the JAVA_HOME variable is not set in the environment. ||| User is trying to install FLAIR (https://github.com/BrooksLabUCSC/flair) on Anvil following the documentation provided at https://flair.readthedocs.io/en/latest/requirements.html ||| User requires GPU-accelerated GROMACS 2025.1 for running molecular dynamic simulations and utilizing the COLVARS package, but the latest available GPU version on ANVIL is 2023.2. ||| The user is experiencing a crash when running a dihedral scan using ORCA 5.0.1 on a protein-ligand system with MP2 and a 6-31G(d) basis set, and the error suggests an issue with OpenMPI Calling Command. ||| User is experiencing issues installing and running OVITO for visualizing large-scale atomic simulation results on Windows via the Anvil supercomputer's OnDemand sessions due to graphics dependency requirements and limited space for storing large data snapshots.","To resolve the issue, create a Python 2 environment using conda and add it to `$PATH` and/or `$PYTHONPATH`. The user can do this by executing:

    ```
    conda create -n xxx python=2.XX
    source activate xxx
    ```

   After creating the Python 2 environment, try submitting the job again using the new submission script located at `/anvil/scratch/x-jsullivan1/EnzoRunJul17`. If the problem persists, consider submitting a new help request or continuing the discussion in an existing ticket (ATS-1960) for further assistance. ||| To resolve the issue, Rongjing should try adding `--mpi=pmi2` to the `mpirun` line in the job script. The corrected line would be:

    ```bash
    mpirun --mpi=pmi2 -np $SLURM_NTASKS $BGW/epsilon.cplx.x
    ```

   Also, Rongjing should ensure that the necessary modules are loaded before running the job script:

   ```bash
   module load intel impi intel-mkl hdf5 libszip
   ```

   And modify the `$QE` and `$BGW` variables in the script to point to the correct directories containing the QE and BerkeleyGW executables, respectively. For example:

   ```bash
   QE=/home/x-rg47749/qe-7.2/bin
   BGW=/home/x-rg47749/BerkeleyGW-3.0.1-hdf5/bin
   ``` ||| To troubleshoot the issue with the login03 node, it is recommended to perform a thorough investigation of the node's status and check for any potential errors or malfunctions. Additionally, relevant documentation related to HPC nodes maintenance can be found at <documentation_URL>. ||| To solve this issue, the user can install hdf5/1.8 in their project space on Anvil. This can be achieved by using the following command:

```bash
module load hdf5/1.8
mpicc -o my_flash_code my_flash_code.c -lhdf5_hl -L/path/to/your/library
```
Replace `my_flash_code` and `/path/to/your/library` with the appropriate file names and paths for your specific project. This command loads the hdf5/1.8 module, compiles your FLASH code, and links it with the required hdf5 libraries. ||| The user is advised to use `srun --mpi=pmi2 -n xxx ./wrf.exe` command, where `xxx` should be replaced by the desired number of tasks. If the user does not specify `-n xxx`, it will use the total number of processor cores requested from SLURM by default. The user mentions that this was already attempted without success, and suggests that the issue may be related to the version of the model. It is recommended to escalate the question to the application team for further investigation. ||| To resolve this issue, the user should create a custom Conda environment and install the required packages there. ||| Run the code, as the warning is not critical enough to prevent its execution. If any issues arise due to the warning in future, further investigation will be required. ||| To optimize performance for the Nek5000 solver, it's recommended to perform scaling tests tailored to the user's system. This is crucial because various simulating systems can behave differently even when running on the same machine. Factors affecting calculating performance include I/O speed and file system. It's also noted that different compilers may affect calculations slightly, so if optimization is desired, scaling tests with your own system are advised. ||| To resolve this issue, the user should ensure their request headers do not exceed the server limit. This can often be achieved by reducing the size of data sent in each request or optimizing the request format to minimize header size. As a workaround, the user could consider using an alternative Jupyter Notebook environment within Anvil, if available and compatible with their specific use case. For more information on optimizing requests for better performance, please refer to this guide: [HTTP Request Optimization](https://developer.mozilla.org/en-US/docs/Web/HTTP/Optimizing_performance) ||| To resolve this issue, it is recommended to manually install or reinstall the Anaconda module using Spack. Here's how you can do that:

   First, open a new terminal and source the Spack environment:

   ```
   module purge
   module load spack
   source $(which spack activate)
   ```

   Next, reinstall the Anaconda module with the correct version (in this case, 2024.02):

   ```
   spack install -f anaconda@2024.02-py311
   ```

   After installation, load the updated Anaconda module:

   ```
   module load anaconda/2024.02-py311
   ```

   The issue should now be resolved, and you should be able to use Anaconda on Anvil without encountering the mentioned error. If you continue to experience issues, please let us know so we can assist further. ||| To resolve the performance degradation and potential data corruption due to the ""UCX is unable to handle VM\_UNMAP event"" warning, add ""--mca opal_common_ucx_opal_mem_hooks 1"" to the mpirun/oshrun command line. Additionally, it's recommended to run benchmarking/scaling tests for the user's model on Anvil to determine the optimal number of cores for efficient performance. The scaling test should involve running a few iterations (e.g., 4-5) with different numbers of cores (16, 32, 64, 128, 256) and recording the time used for each calculation to calculate efficiency and choose an appropriate number of cores for the system. Here's a modified jobscript incorporating the recommended changes:

```bash
#!/bin/bash -l
#SBATCH --nodes=16
#SBATCH --ntasks-per-node=128
#SBATCH --time=24:00:00
#SBATCH --export=NONE
#SBATCH [--mail-user=]
#SBATCH --mail-type=ALL
#SBATCH -e error_file_%j.e
#SBATCH -o logfilei
#SBATCH -A PHY240099
#SBATCH -p wholenode
module purge
module load intel/19.1.3.304
module load openmpi
module load cmake
srun --mca opal_common_ucx_opal_mem_hooks 1 -n $SLURM_NTASKS ./nek5000 > logfile 2>&1
``` ||| The user should modify the compile line to replace ""-DCMAKE\_C\_COMPILER=mpicc"" with ""mpiicc"". This is applicable when using Intel compiler+impi. Also, ensure that the version of Quantum Espresso used for compilation interfaces better with the other software in question. If this modification resolves the issue, the user should be able to compile successfully and run jobs requiring multiple entire nodes for calculations without erroring out. ||| The software installation policy at Anvil only supports high-demand software, and the most widely used version of OpenFOAM they currently support is 8-20210316. As an alternative, the user can attempt to install OpenFOAM themselves in their home directory or within their allocated project space. If successful, a Singularity container could be beneficial for using the preferred version of OpenFOAM. RCAC provides a tutorial on containers (<https://www.rcac.purdue.edu/training/containers101>) which may assist with this process. Pre-built OpenFOAM Docker images are also available on Docker Hub (<https://hub.docker.com/search?q=openfoam>), and these can be converted to Singularity containers using the steps outlined in the tutorial. ||| Load the `iqtree` module and run the software by calling it as `iqtree2`. User guides for each biocontainer module can be found at https://www.rcac.purdue.edu/knowledge/biocontainers. ||| To resolve the OOM issue, the user should request more memory in their Slurm job script by adding the `#SBATCH --mem` or `#SBATCH --mem-per-cpu` directives and specifying the required memory (e.g., `#SBATCH --mem=50G`, `#SBATCH --mem-per-cpu=4G`). To fix the permission denied error on version 'name 1.9.3', it is recommended to run commands with elevated privileges by using `sudo` (e.g., `sudo name pkg install <package_name>`) or ensuring that the user has the necessary permissions for the file path where the packages are stored. If the issue persists, please consult your group administrator for further assistance. Also, it is worth noting that if the code modification is not significant, it might be ok to use the currently supported version of 'name' on Anvil until a specific version can be installed. For detailed documentation and instructions about using 'name' on Anvil, refer to the [Anvil user guide](http://I.name Anvil support team). ||| To resolve this issue, update the Perl command with the path to the singularity-run samtools binary. In the bash script (arima_mapping_3A.sh), add the following lines after loading modules and source ~/.bashrc:

```bash
SAMTOOLS=""/usr/bin/singularity run /apps/biocontainers/images/staphb_samtools:1.9.sif samtools""
perl $COMBINER $FILT\_DIR/${SRA}\_R1.bam $FILT\_DIR/${SRA}\_R2.bam ""$SAMTOOLS"" $MAPQ\_FILTER
```

Please note that this solution assumes the path to the singularity-run samtools binary is ""/usr/bin/singularity run /apps/biocontainers/images/staphb\_samtools:1.9.sif samtools"". Adjust the command accordingly if the path is different. Additionally, refer to the following user guide for further information about using samtools on Purdue Anvil: https://www.rcac.purdue.edu/knowledge/biocontainers/samtools

If you encounter any other issues or have questions, please don't hesitate to reach out. ||| To resolve this issue, the user should add the flag `--mca opal_common_ucx_opal_mem_hooks 1` to their mpirun/oshrun command line when submitting a job using ""srun -n $SLURM_NTASKS exe"". The specific job ID mentioned is 4864168. ||| Investigate the issue further by confirming if the same set of modules (Intel, OpenMPI, and cmake) are loaded during both compilation and job execution. If not, the user should ensure that these modules are loaded consistently for both stages to avoid discrepancies leading to the error messages. The user can try recompiling their application from a fresh copy after loading the necessary modules, then run it again to see if the issue persists. ||| To resolve this issue, load the conda module and create a virtual environment to install Java. This can be achieved using Conda from the {{conda-forge}} community channel. Here's how you can do it:
   - Load the conda module: `module load conda`
   - Create a new virtual environment (replace ""myenv"" with your preferred name): `conda create --name myenv java openjdk`
   - Activate the created environment: `source activate myenv`
   - Verify that Java is now installed and accessible: `java -version`

For more information about using Conda, visit [the official documentation](https://docs.anaconda.com/anaconda/user-guide/tasks/manage-environments/). ||| Follow the steps outlined in the Flair documentation (<https://flair.readthedocs.io/en/latest/requirements.html>) to install it independently. If you encounter any errors during installation, reach out for further assistance. ||| The user needs to install GROMACS 2025.1 from source if they require this specific version, as it's not currently available within NGC containers on ANVIL. Additionally, the user should ensure that any prerequisites such as CMake (3.28) and CUDA toolkit versions (12.8) are installed in their project space before installing GROMACS 2025.1. ||| The user needs to install their own version of OpenMPI 4.1.6 on Anvil to resolve the issue. Here are the steps for installation:
   - Login to Anvil: `ssh`
   - Navigate to the specified directory: `cd /anvil/projects/x-mch240042/`
   - Create a software directory and navigate into it: `mkdir -p software/src && cd software/src`
   - Download OpenMPI 4.1.6: `wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.6.tar.gz`
   - Extract the downloaded tarball: `tar -xzvf ./openmpi-4.1.6.tar.gz`
   - Navigate into the extracted directory: `cd openmpi-4.1.6/`
   - Configure OpenMPI with the specified prefix and enable MPI Fortran: `./configure --prefix=/anvil/projects/x-mch240042/software/openmpi-4.1.6 --enable-mpi-fortran CC=gcc CXX=g++ FC=gfortran`
   - Compile and install OpenMPI: `make -j 16 && make install` (use the number of cores you want for installation)
   - Once OpenMPI is installed, download ORCA 6.0.1 from the provided link: https://orcaforum.kofo.mpg.name/app.php/dlext/?view=detail&df_id=235 and move it to Anvil: `scp /path/to/download/orca_6_0_1_linux_x86-64_shared_openmpi416.run :/anvil/projects/x-mch240042/software`
   - Finally, install ORCA 6.0.1 on Anvil: `cd /anvil/projects/x-mch240042/software && chmod +x ./orca_6_0_1_linux_x86-64_shared_openmpi416.run && ./orca_6_0_1_linux_x86-64_shared_openmpi416.run`
   - After installation, the job script should be modified to load the installed OpenMPI module and run ORCA as follows:
     ```
     #!/bin/bash -l
     #SBATCH --job-name=orca_job
     #SBATCH --nodes=1
     #SBATCH --ntasks=32
     #SBATCH --time=00:30:00

     module load openmpi-4.1.6
     /anvil/projects/x-mch240042/software/orca_6_0_1/bin/orca your_input_file.inp
     ``` ||| - To install OVITO, follow the software request policy on Anvil and install it in your own project space: https://www.rcac.purdue.edu/knowledge/anvil/policies/software_installation_request_policy
   - For X11 forwarding issues, download ThinLinc to your computer and use it to login Anvil: https://www.rcac.purdue.edu/knowledge/anvil/access/login/thinlinc
   - To run OVITO, use the following commands after logging in through ThinLinc:
      ```
      module --force purge
      module load ovito
      ovito
      ```
   - It is recommended to start an interactive job when using Ovito instead of running it on the login nodes."
2,1,49,49,"User needs to be added to vasp6 license and instructions on using the VASP software on Anvil ||| - Postdoc requires VASP license confirmation on Anvil HPC and guidance on setting up X11 display for gnuplot usage.
   - Supervisor needs to add postdoc to the VASP license list.
   - Incorrect email address associated with postdoc's information in the VASP license list. ||| User (Zekun name) added to VASP5 group on Anvil and instructed to check user guide for VASP calculations on Anvil ||| Request for VASP access and addition of another user (zli18) to the research group with VASP access ||| User Requests Access to VASP6 on Anvil HPC System ||| User requires access to VASP on Anvil and has attached their license file. ||| User request for access to VASP on Anvil with provided license number. ||| User wants to access VASP on Anvil after purchasing the license ""23-0266"". The user's username for Anvil is x-yao1 and email address used for Anvil is [mailto:]. ||| User Nick requires approval and access to VASP 5.4.4 on Anvil. ||| User (Gillian) needs VASP license approval on Purdue's Anvil HPC system for submitting VASP jobs ||| Access to VASP software on ANVIL for the group and verification of one member's license status. ||| User is unable to access VASP 5.4.4.pl2 on compute nodes despite being granted access and updating groups. ||| User requires access to VASP software under the advisor's VASP license on Anvil platform ||| A new user needs to access VASP (Vienna Ab initio Simulation Package) on Anvil HPC system with license number 23-0067. ||| Requests verification of VASP license from Materials Design Inc and assistance in compiling VASP on Anvil. ||| User is unable to load VASP module or see the vasp5 Unix group after being added to it on Purdue Anvil. ||| Request for access to VASP 5.4 on Anvil ||| A PI from UC name needs to be added to the VASP 6.3 license on Purdue Anvil for users elee2, jspencer, and sto. ||| User request for access and loading instructions for VASP on ANVIL cluster ||| ACCESS/ANVIL account for PhD student Cappola at Arizona State University requires access to run VASP on ANVIL due to a purchased license from ASU. ||| User needs to add VASP version 6.4.3 to Anvil for use and requires updating their email address in the VASP portal as the current one is associated with a different license (vasp5). ||| The user is unable to start the WES Toil service on HPC due to a network permission error when using unprivileged users with Apptainer. They need to access a rabbitmq server on loopback port for specific benchmarks. ||| User requires access to VASP on Purdue Anvil due to an active license but the system could not verify their email address associated with the license. ||| User needs access to VASP on their x-scasanova account on Anvil ||| User requires access to VASP.5 software on Anvil computer with a valid license until June 30th, 2019. ||| User was unable to use VASP (versions 5.4.4.pl2 and 6.3.0) on Anvil due to licensing issues. ||| Request for addition to VASP users group on HPC system ||| User Mayur cannot find or load VASP installation in Anvil HPC system due to license issue after being added to the vasp users group. ||| User Mayur needs access to VASP at ANVIL, and is an authorized user with an associated email address. ||| User Kethamukkala (username ""x-kkethamukkal"") needs access to VASP for the Anvil ACCESS project MAT210034 at Arizona State University. ||| User could not find VASP module installed on the Anvil system, required it for their research, and requested its installation. ||| User requires access to VASP on Anvil HPC system and needs assistance with adding a license. ||| User needs assistance with using VASP on the Anvil server and has provided a license number (22-0315). ||| User needs access to VASP5 and VASP6 software on the Anvil HPC cluster, and has provided an email address for license verification. ||| Visiting Scholar at Purdue University needs VASP license added to ANVIL clusters ||| User Gokhan Sensoy is unable to access the VASP code on the cluster due to a Lmod license error. ||| User needs access to VASP 6.0 on Anvil under Prof. name's VASP license. ||| Unable to run Abaqus jobs on Anvil due to a licensing error despite having access through ECN (Purdue Grad Student). ||| Postdoc at University of Delaware is unable to run VASP jobs on Anvil HPC at Purdue through NSF Access credits due to missing VASP access. ||| Request for access to VASP on the Anvil cluster ||| User with ACCESS ID: wenhaosun requires permissions to submit VASP jobs on Anvil due to having a valid VASP license. ||| Error while loading VASP module on Purdue Anvil CPU cluster due to restricted license. ||| Request to add users Larry and [name] from Harvey Mudd College research team to the Anvil VASP group for running DFT simulations using the VASP software with license number 24-0365. ||| User is unable to access VASP on the Anvil server due to a local server maintenance issue and wants to know how to gain access using their university's VASP license. ||| User requires access to VASP 6 on Anvil HPC cluster. ||| User needs to execute VASP on Purdue ANVIL cluster after maintenance downtime. ||| Request for VASP 6 access on the Purdue Anvil system and inquiry about Gaussian 16 availability ||| Request to add users (Niusha Niknahad, Kanmodi, Shuprovo Sikder) to VASP on Anvil ||| Unable to access VASP on Anvil server using a local subscription. Requires assistance in linking professor's VASP license to multiple student accounts and resolving error when building and running VASP 6.","The user has been added to the vasp6 group on Anvil and their membership will be ready within a few hours. To use VASP6, make sure to add 'module load hdf5' to your submit script. For more information about using VASP on Anvil, please refer to this documentation: [VASP calculations on Anvil](https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp) and [Installing Applications on Anvil](https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications). ||| - For VASP, ask your supervisor (Professor Zunger) to add you into the valid VASP license and provide your email address for verification purposes. Let the support team know once verified so they can add you to the VASP groups on Anvil. - To fix the X11 display issue when using gnuplot, try using another SSH client such as ThinLinc, which is equipped with X11 and available for Anvil login. You can find more information about setting up both MobaXterm and ThinLinc in this guide: <https://www.rcac.purdue.edu/knowledge/anvil/access/login> and <https://www.rcac.purdue.edu/knowledge/anvil/access/login/thinlinc>. - To correct the email address associated with your information in the VASP license list, please notify the support team about the mistake so it can be updated accordingly. ||| To access and use VASP version 5 on Anvil, the user (Zekun name) should wait for their membership to be ready within a few hours, as they have been added to the vasp5 group. The user can refer to the user guide provided by RCAC Purdue for instructions on performing VASP calculations on Anvil: <https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp>. ||| - Haonan was granted access to the pre-installed VASP 5 and VASP 6 on Anvil by being added to the vasp5 and vasp6 Unix groups. - The following command can be used to verify the user groups: `$ groups <username>` (replace `<username>` with the actual username)
- The user Haonan was added to the x-zli18 group, granting access to zli18. - The modules vasp/5.4.4.pl2 and vasp/6.3.0 are available for zli18. - The link to the user guide on VASP (RCAC - Knowledge Base: Anvil User Guide: VASP) was shared with Haonan and will be shared with zli18. - To share information with another user, you can copy and paste the link in an email or a messaging platform. - If there are any further questions or requests, Haonan is encouraged to contact support again. ||| The user has been added to the vasp6 group on the Anvil HPC system. Their membership will be ready within a few hours. They are advised to refer to the user guide for VASP calculations on Anvil located at this URL: https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp
Additionally, they can find more information about using VASP on Anvil in this document: https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp|smart-link ||| The user has been added to the access of both vasp5 and vasp6 on Anvil. They are advised to check their account later today. For usage information, they should refer to our user guide which can be found at the provided URL: [Purdue VASP User Guide](https://purdueit.service-now.com/service_attached_file?id=c8f4b70e3d3f9610d5202e7c31a665b5) ||| The user's VASP license has been verified and they have been added to both {{vasp5}} and {{vasp6}} Unix groups on Anvil. To access the VASP modules, the user should open a new login session. For guidance on running VASP jobs on Anvil, refer to the following link: [https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp](https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp). Additionally, a previous training session recording on VASP is available online at [https://www.rcac.purdue.edu/training/using-vasp-on-anvil](https://www.rcac.purdue.edu/training/using-vasp-on-anvil). ||| The user has been added to vasp5 and vasp6 groups on Anvil. Membership should be ready within a few hours. To use VASP6, the user is required to add 'module load hdf5' command to their submit script as indicated in this link: <https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp>
For additional information on running VASP calculations on Anvil, please refer to this link: <https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp|smart-link> ||| Add the user 'mailto:[Nick's email address]' to the Unix group {{vasp5}} for access to the {{vasp/5.4.4.pl2}} module. To utilize the new access, the user should start a new login session so that the updated group memberships can be recognized. The command to check the group membership is: `$ groups x-njaegers`. If the user has further questions or encounters issues, they may reopen this ticket or submit a new one. ||| - The user has been added to the 'vasp6' group on Anvil, and membership is expected to be ready within a few hours. - The user is advised to consult the VASP calculations user guide available at this link: [VASP calculations on Anvil](https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp)
- When using VASP6, it's recommended to include 'module load hdf5' in the submit script to ensure proper functionality. Further details on running apps on Anvil can be found at this link: [Anvil run examples for VASP](https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp) ||| - The user (Nikolas) has been added to the {{vasp5}} and {{vasp6}} Unix groups on Anvil for accessing the pre-installed vasp modules. This should allow Nikolas to use the VASP software. However, it is necessary to start a new login session in order for the update to take effect. - There was an issue with verifying the eligibility of one member (name) using VASP. The user is advised to contact the support team regarding the current status of their license. Once the license status is verified, name will be added to the {{vasp5}} and {{vasp6}} Unix groups for accessing VASP on ANVIL. ||| Add the user to the vasp5 Unix group. The user should be able to load the vasp/5.4.4.pl2 module, but might need to start a new login session for the update to take effect. If the problem persists after starting a new login session, the user should wait for the system update to propagate and check again later. For any further questions or help regarding VASP issues, please contact our expert or reopen this ticket. ||| To grant access to the VASP software on the Anvil platform for the user, follow these steps:
   - Log into the Anvil platform using your credentials
   - Launch the VASP5 software by typing `module load vasp5` in the terminal if it's not already loaded. If you encounter any issues with loading the module, please consult the Anvil documentation at <https://anvil.itap.purdue.edu/documentation/>
   - Once loaded, proceed with using VASP as per your research requirements ||| To access VASP modules {{vasp5}} and {{vasp6}} on Anvil, you will need to login to the system and load the appropriate modules. This process may take a couple of hours after your account has been updated with the licenses. For more information about loading modules, please refer to the Anvil User Guide: [Anvil Module System Documentation](https://docs.it-cog.purdue.edu/anvil/module_system/) ||| To verify the VASP license status, have the NSF administrator reach out to <Chelsea Kemmerrer, M.S., P.E., mailto:> at Materials Design Inc. For compiling VASP on Anvil, refer to the user guide provided by Purdue University at https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp. ||| To resolve this issue, it's recommended to check your shell environment by using the following command: `source /etc/profile`. After that, try running the `id $USER` command again to verify if you are part of the vasp5 Unix group. If you still encounter problems loading the VASP module, consider reloading the modules by executing `module unload all && module load vasp/5.4`. Make sure your VASP license (5-2357) is up to date before running the VASP code. ||| The user has been added to the vasp5 group on Anvil. Their membership will be ready within a few hours. For information on running VASP calculations on Anvil, please refer to this user guide: [link](https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp). Additionally, the example files for using VASP on Anvil can be found here: [link](https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp|smart-link) ||| The user has been successfully added to the vasp5 and vasp6 Unix groups on Anvil. They should now have access to the vasp modules. For guidance on running VASP jobs on Anvil, please refer to the following documentation: https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp ||| To gain access to the vasp modules on Anvil, follow the instructions provided in the guide: <https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp> This should help you load the vasp modules on your account. If you have any further questions or issues, please feel free to reopen this ticket. ||| The user has been added to both vasp5 and vasp6 groups on Anvil. The membership should be ready within a few hours. Please refer to the user guide for VASP calculations on Anvil available at this link: [https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp](https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp) ||| The user should update their email address associated with their VASP license on the VASP portal. Once this is done, the user should contact support again so that they can be added to the vasp6 unix group on Anvil. After this step, the membership will be ready within a few hours. ||| Remove any `--network` options passed to `apptainer`. Docker would require a bridge network, but Apptainer is more permissive and allows direct use of the loopback interface on Anvil. Some RCAC-maintained apps are already using this approach. ||| The user needs to have Dr. [name Leuenberger] add their email address to the existing VASP license (license number 21-0457 5-498). After this step, the Senior Computational Scientist will be able to verify the new email address and grant access to the user on Purdue Anvil. This process may take a few hours. ||| The user has been verified for VASP license and added to the vasp5 and vasp6 Unix groups for accessing the vasp modules on Anvil. The command `$ groups x-scasanova` shows the updated group memberships. To recognize these changes, the user may need to start a new login session. For detailed instructions on running VASP jobs on Anvil, please refer to the following documentation: [RCAC - Anvil - Software - Installing Applications - vasp](https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp) and [RCAC - Anvil - Software - Installing Applications - vasp|smart-link](https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp). ||| To provide access to the requested VASP 5.4.4 software on Anvil computer, first ensure your license details are as follows:
   - License holder: Mikalai Artsiusheuski
   - License number: 5-1632
   - Email for license holder: mailto: [redacted]
   - VASP versions: version-5
   - Primary user: Mikalai Artsiusheuski
   - Primary user email: mailto: [redacted]

Once your license details are confirmed, you should now be able to use the VASP 5 module on Anvil. ||| Add the user to the appropriate VASP license groups. The changes will propagate through the system, and the user should be able to use both VASP5 and VASP6 by tomorrow. ||| To grant access to the VASP users group, first, it is necessary to identify the email address used to sign up for the VASP license for your research group. After obtaining this information, you can verify if the account has been successfully granted access by logging in and checking for the availability of vasp5 on the HPC system. The Purdue IT helpdesk provides assistance with this process: [Purdue IT Helpdesk](https://service.purdue.edu). In your case, Eli from Purdue IT has confirmed that access to VASP has been granted and can be verified through the Purdue IT Helpdesk website: [Purdue IT Helpdesk](https://service.purdue.edu/). ||| The issue may be due to the group update not propagating yet. The user is advised to wait for the group update to take effect. If the problem persists, it is recommended to reach out again for further assistance. In the meantime, the group has been changed from vasp5 and the original group x-mch240072-vasp5 has been removed which might have caused this issue. ||| Add Mayur ([Mayur's email provided in the original message]) to the managed vasp5 group after verifying his email. Additional Information: For more information about VASP process, refer to this link: [License](https://github.rcac.purdue.edu/RCAC-Staff/SupportKnowledgeBase/wiki/License) in the RCAC Support KnowledgeBase Wiki. ||| The license for VASP has been added to user's account. They should now be able to use VASP on Anvil. If additional assistance is needed, please reach out. ||| The Anvil support staff granted access to both vasp5 and vasp6 modules for the user. The user should verify that they can now access these modules. If the user still cannot access these modules, they are encouraged to respond to this ticket with the issue details. A user guide about Anvil is available at https://www.rcac.purdue.edu/knowledge/anvil for further information on using Anvil resources. ||| The user should contact RCAC (rcac@osu.edu) for issues related to Anvil and VASP. To obtain access, the user is advised to send an email to RCAC with the specific request. The user has been informed that their license will be added by a team member tomorrow. If further help is required, the user should let the team know. ||| To use VASP on the Anvil server, first ensure that you have your license key (e.g., 22-0315) ready. Log into the Anvil server via SSH and navigate to your home directory or the desired working directory. Load the necessary modules using L Modules:

```bash
module load vasp
```

Now, you can run VASP with your license key by executing the following command (replace `your_key` with your actual license key):

```bash
vasp.x -i in.vasp -x out
```

Replace `in.vasp` and `out` with the input file name and output directory, respectively. For more information about running VASP on Anvil server, refer to the [VASP documentation](https://cms.mpcdf.org/pdfdocs/VASP/Manuals/MANUAL-5420/manual_5420.pdf) and Purdue IT's HPC guide (https://it.purdue.edu/hpc). ||| The user's email `<mailto:>` was added to the VASP5 and VASP6 groups on the Anvil HPC cluster. Access will be available within a few hours. The user is advised to check the user guide for VASP calculations on Anvil at [this link](https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp). ||| The support team has successfully added the VASP license (license number 23-0006) to user 'x-ksamanta1' on the ANVIL clusters. It may take some time for the license to become effective. The user is advised to try again later if needed, and reach out for further assistance if required. ||| The issue appears to have been resolved due to an unscheduled outage at Anvil that has now been remedied. If user Gokhan Sensoy is still experiencing issues with accessing VASP, they should confirm their credentials and license information (full name: Gokhan Sensoy, affiliation: Recep Tayyip Erdogan University, user name: mgsensoy, vasp license number: LICENSE 5-1749, email: [mailto:](mailto:)) with the VASP team to ensure they are registered under that license. ||| The user was advised to register an ACCESS account and request Prof. name to add them to an active allocation. After doing so, they should have access to VASP on Anvil. The user confirmed they had an access account but had not asked Prof. name to add them yet. Later, the user was added to the vasp5 and vasp6 groups on Anvil, and their membership will be ready within a few hours. The user can find information about VASP calculations on Anvil in the following links:
   - [Anvil: Run Examples - Apps - VASP](https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp)
   - [Anvil: User Guide for VASP Calculations](https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp|smart-link) ||| Wait for a response from the ECN regarding the licensing issue. In the meantime, ensure that the installation of Abaqus is correct and functioning without issues for your account. If the problem persists after receiving the response from ECN, try reinstalling or resetting the license for your Abaqus installation on Anvil. ||| To resolve the issue, the user should provide their email address associated with their VASP license. The support team will then validate the license and add the user to the appropriate groups on Anvil (`vasp5` and `vasp6`) for accessing VASP. This process may take a few hours. In the meantime, the user can refer to the user guide for VASP calculations on Anvil at this link: [Anvil User Guide for VASP Calculations](https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp) ||| The user has been granted access to VASP on the Anvil cluster. They have been added to the vasp5 group and their membership will be ready within a few hours. The user is advised to check the RCAC Purdue University user guide for VASP calculations on Anvil at https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp. Another useful resource can be found at https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp|smart-link. ||| To resolve this issue, the user should have their PI add them into a valid VASP license first. Once added, the user's valid vase license can be verified on the VASP community portal, which will then allow Anvil to update the user's account with permissions for VASP job submissions. ||| The user needs to be added to the valid VASP license by their advisor. After being added, they can validate their status and use the VASP module. Once added, the user membership will be ready within a few hours. Here is the link for VASP calculations on Anvil: <https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp> |smart-link and for more information about using VASP on Anvil, please refer to this guide: <https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp>. ||| The users, Larry [mailto:] and [name] [mailto:], have been added to both {{vasp5}} and {{vasp6}} groups on Anvil. Their membership will be ready within a few hours. Users are advised to check the Purdue University user guide for VASP calculations on Anvil at this link: <https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp>. For more information about running VASP on Anvil, please visit this link: <https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp|smart-link> ||| Users on Anvil who want to use VASP need to provide an email associated with an active VASP license for validation. The support team will then add the user to the central vasp groups on Anvil after validating the license. It is not specified if a single license can be shared among multiple users, so it may be best to contact your professor or supervisor for more information regarding licensing agreements. ||| The user's VASP license has been validated, and they have been added to the vasp5 and vasp6 groups on Anvil. Their membership will be ready within a few hours. The user is advised to check our user guide for VASP calculations on Anvil at this link: [https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp](https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp). Additionally, they can find more examples and information on running VASP calculations on Anvil at this link: [https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp|smart-link](https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp%7Csmart-link). ||| To run VASP on Purdue ANVIL, follow the steps below. First, download the current VASP6 source code from this link: [http://my.materialsdesign.com/download/vasp6.2](http://my.materialsdesign.com/download/vasp6.2)
   - username: Saquib.name (or) : mailto:
   - password: [Hidden due to security concerns]

   Next, refer to the user guide about building your own VASP on Anvil provided by Purdue University here: [https://www.rcac.purdue.edu/knowledge/anvil/software/installing\_applications/vasp/build\_your\_own\_vasp\_6](https://www.rcac.purdue.edu/knowledge/anvil/software/installing_applications/vasp/build_your_own_vasp_6) ||| You ({{x-hgriffin}}) have been added to both vasp5 and vasp6 groups on Anvil. Your membership will be ready within a few hours. Please refer to the user guide for VASP calculations on Anvil for instructions: <https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp>. There is no Gaussian on Anvil as it is intended for national wide institutions. Purdue has a site license for Gaussian 16, but not Anvil. ||| To add the specified users to VASP groups on Anvil, their license status needs to be verified through the VASP portal using their individual emails. The PI should add them to one active VASP license with their emails. Once this is done, Niusha Niknahad, Kanmodi, and Shuprovo Sikder have been added to {{vasp5}} and {{vasp6}} groups on Anvil. Their membership will be ready within a few hours. Users can refer to the user guide for VASP calculations on Anvil at https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp and https://www.rcac.purdue.edu/knowledge/anvil/run/examples/apps/vasp|smart-link for further guidance. ||| - To utilize VASP on the Anvil server with your local subscription, register the license on the VASP portal at this URL: [https://www.vasp.at/sign\_in/registration\_form/](https://www.vasp.at/sign_in/registration_form/)
   - In order to allow your professor's VASP license access for multiple student Anvil accounts, the professor needs to register their license on the VASP portal. - For resolving the issue when building and running VASP 6, please ensure you are using the correct commands and file paths in your job script. Make sure the executable file is placed correctly and linked appropriately. If you still encounter errors, analyze the error file (err.txt) for further troubleshooting. - The Anvil support team cannot assist with 3rd-party license integration; only licenses registered through the VASP portal can be utilized on Anvil."
8,1,31,31,"Increase the file number quota for project x-cis230283 from the current limit to 5000k. ||| The user's SCRATCH directory quota has been exceeded due to a large number of output files generated during extensive jobs. ||| User is unable to open a new Jupyter notebook on Anvil due to a disk quota exceeded error, despite deleting files in the root directory. The user wants to move their project directory into their home directory for additional storage space. ||| User, Canchila, requires an increase in the file quota limit of their HPC project folder to accommodate a large dataset (over 1.3 million images) for deep learning model training. ||| User is experiencing an issue with a Model being stored in their home directory instead of the project directory, despite having limited space and more capacity in the project directory. ||| User Vivek Sharmas home directory is full and he needs to free up space as his data is in the scratch directory and he cannot increase his HOME quota. ||| Error saving Jupyter Notebooks on OnDemand due to exceeded file limit in the SCRATCH directory. ||| User is unable to run jobs due to file creation limit in project space and wants to increase the limit or find a workaround. The user also needs help creating a tarball file in Anvil. ||| User is unable to run jobs due to file creation limit in project space and wants to increase the limit or find a workaround for creating tons of small files during machine learning training process. ||| User has a large dataset (Imagenet) and is experiencing storage issues due to limited usage on home directory. Requests assistance in accessing the Imagenet data effectively. ||| User is experiencing simulation crashes due to out-of-disk space problems specifically related to file number limit. ||| The user requires an increase in the file quota on their storage location (/anvil/scratch/x-sajal105) due to a large number of open files generated by their MPI simulations. ||| User is facing an issue with expanding persistent volumes in the Kubernetes cluster and wants to find a solution that supports quick turnaround, minimizes downtime, and allows for user quota settings. They are also encountering a ""Permission denied"" error when trying to apply a size quota using setfattr command on a user's home directory. ||| User requires an increase in the maximum number of files they can store on their HPC system due to a limit of 1000k files that is preventing them from utilizing the full potential of their newly allocated 100TB storage. ||| The user is unable to write files due to a file limit of zero in their directory on the HPC allocation. ||| User is unable to download Conda packages due to insufficient storage on their Anvil home directory. ||| The command ""myquota"" does not work due to a complicated shell environment in user's ~/.bashrc file, causing an issue when the .bashrc gets sourced. ||| Disk quota exceeded error occurred while saving a file in scratch directory despite not being within the home directory. ||| User Wenliang needed an increase in the file limit and block quota on their scratch directory due to an upcoming large analysis that would generate a huge number of files. ||| User requests extension of storage time for their project space and scratch files (60T and 200T respectively). ||| The user is unable to access Open OnDemand due to an error: ""No space left on device"" and requests deletion of specific directories. ||| User has a project with a reported total quota of 200TB, but is experiencing ""No space left on device"" errors when reaching approximately 65TB. The user has confirmed that the additional 135TB of quota is not actually available for use. ||| Home directory of user x-wliu1 is full due to excessive storage usage in the .cache folder ||| User is unable to access quota information or use 'myquota' command in the $PROJECTS directory due to an error regarding disk quotas on the Anvil cluster. They suspect that the allocated storage space has been exceeded but are unsure since they cannot confirm using 'myquota'. They request additional 20T of storage in the $PROJECTS partition. ||| User's Anvil/home/x-snajafi/ quota limit is currently set at 25GB, requiring an increase to install required programs due to their storage needs. ||| The myquota command is reporting that home, scratch, and projects file system quotas are currently inaccessible. ||| User is unable to copy files due to insufficient space in home directory and wants to know how to resolve the issue or request for an increase in disk size. ||| Request for increase in file number limit in a project space for data transfer. ||| The user's HPC project is using more disk space than expected due to block size allocation inefficiency, and the increased quota is not solving the problem. ||| User x-mlesniewski is unable to access data located at /anvil/projects/x-che170062 from their shortcut in /home/x-mlesniewski due to permission errors. ||| User cannot see output from ""myquota"" command and is experiencing an error related to storage allocation during data transfer. Data will expire on May 12th, and the account and storage location are x-kchand1 and /anvil/projects/x-ees240013/krishan respectively.","Unfortunately, it was not possible to increase the file number quota as requested due to the user's specific workflow requirements. If the user has any other requests or questions in the future, they are encouraged to contact support again. ||| The HPC support team increased the quota of the user's SCRATCH directory to 3 million files. Users can check their new quota using the command `myquota x-whnfff`. Here is an example of how you might check your quota:

```bash
$ myquota x-whnfff
==============================================================================
home    x-whnfff          811.9MB        25.0GB      3%  -   -   -
scratch anvil            11.9TB         100.0TB     12%  999k  3,145k  32%
projects x-dmr100005     102.5GB        5.0TB       2%   255k  1,048k  24%
``` ||| To resolve this issue, create a symlink from the user's project directory to their home directory by running the command `ln -s /anvil/projects/x-cis230306/{user_folder} myprojects` (replace `{user_folder}` with the actual folder name). After that, navigate to the shared group space `cd /anvil/projects/x-cis230306/`, create a new directory for the user with the command `mkdir {user_folder}`. The user can then access their project directory from within their home directory by opening the Jupyter notebook and finding the `myprojects` folder. ||| The storage expert has increased the user's project folder quota to 2 millions files. This should help alleviate the issue faced by the user during their deep learning task. ||| The user can modify the `cache_dir` when downloading Hugging Face models to change the destination for the cache files. Additionally, the user can set up a symbolic link (symlink) to move the cached files to their scratch directory using the following commands:

```
mv ~/.cache $CLUSTER\_SCRATCH/cache
ln -sf $CLUSTER\_SCRATCH/cache ~/.cache
```

In this way, applications that want to read/write to the canonical `~/.cache` will use the Scratch filesystem (more capacity and performance), with the added benefit of automatic expiration. ||| Run the 'ncdu' command on the home directory to check what files have been occupied, decide if you want to keep them, and then move necessary files to either /anvil/project/x-cda090008 or /anvil/project/x-cis230306 project folders. Each project folder has a 5TB quota. For more information about storage options on Anvil, refer to the user guide: <https://www.rcac.purdue.edu/knowledge/anvil/storage/filesystems> ||| To resolve the issue, it is necessary to remove some files from the SCRATCH directory to free up inodes and allow for additional files to be added. The user can delete old or unnecessary files to meet the 1,000k file limit in the SCRATCH directory. For reference, navigate to the SCRATCH directory using the terminal (e.g., `cd /home/x-jsullivan1/SCRATCH`) and run the command `rm [filename]` to delete specific files or use the command `rm -r [directoryname]` to remove entire directories. It is recommended to back up any important data before deleting files. ||| - The recommended solution is to bundle large collections of small files, such as in machine learning training, using an off-the-shelf format like TAR or ZIP. This can be done without compression for better performance during training. - To create a tarball file (TAR) with the `tar` command, you can use:
     ```
     tar -cvf training_data.tar training_data/
     ```
   - To create a ZIP file, the creation command is similar:
     ```
     zip -0 -r training_data.zip training_data/
     ```
   - Note that the archive creation won't affect the source data, as it only touches the data in read-only fashion. - To keep your terminal running even when you get disconnected, use the Remote Desktop interface (ThinLinc) or utilities like SCREEN or TMUX. For better features, consider using TMUX. - When consuming the data while in a bundled format, you can list the files from where they lie inside the archive and pull them directly out if you have the benefit of writing your own code. In Python, this is a couple of lines change or a single line change if you use a third-party package to handle it for you. If you are stuck using a pre-existing application that you cannot change the interface to, you can trick it by exposing the archive file as a 'normal' directory using utilities like ratarmount: [https://github.com/mxmlnkn/ratarmount](https://github.com/mxmlnkn/ratarmount)
   - To use ratarmount, first create an empty folder (e.g., `other_dirname`) and then mount the archive as a directory using:
     ```
     ratarmount training_data.tar other_dirname/
     ``` ||| To resolve this issue, it is recommended that the user archives the files to a tarball or zip format to avoid producing/creating tons of small files. This can be done using standard Linux utilities such as `tar` or `zip`. To create a tarball file using `tar`, use the command `tar -cvf training_data.tar training_data/`. For creating a zip file, use the command `zip -0 -r training_data.zip training_data/`. The user should note that they do not need to compress the data while archiving; the purpose is only to bundle the files. The archive creation process will not affect the source data and can be done without worrying about losing any data. To avoid disconnection issues, it is suggested to use the Remote Desktop interface (i.e., ThinLinc) or utilities like `SCREEN` or `TMUX`. For consuming the archived data while in a bundled format, the user can list the files from within the archive and pull them directly out using their preferred programming language. If using Python, this process is straightforward with only a couple of lines of code change. Alternatively, if using pre-existing applications, they can trick the application by exposing the archive file as a 'normal' directory using utilities like [ratarmount](https://github.com/mxmlnkn/ratarmount). ||| The user plans to use the lmdb format for their dataset, which is an aggregate file format often used for databases. It's suggested that they try this format first. If further assistance is required or if other issues arise, the user can reopen this ticket or submit a new one. ||| To resolve this issue, the user can use the command `myquota x-otumuklu` to check their actual storage quota and usage on the scratch anvil location. The user is likely running into an out of file number issue as there are two limits for their project folder: a file size limit and a file number limit. To address this, the user should monitor their file count within their project folders located at `/anvil/projects/x-phy240018` and `/anvil/scratch/x-otumuklu`. It is also important to note that there are no limitations on file size or file number for the home directory (`home x-otumuklu`). ||| The user's request for increasing the file number quota on /anvil/scratch/x-sajal105 to 2M until the end of 2024 has been granted. This can be verified using the command `myquota x-sajal105`. The new quota will facilitate the concurrent execution of simulations and enhance the user's project efficiency. ||| - To expand persistent volumes quickly and minimize downtime, the user can consider resizing the Persistent Volume using anvil-block storage. This would require changing the storage class of the affected volumes to anvil-block, which supports volume expansion. However, development and testing might be needed for dynamically attaching different volumes to pods based on directory requirements. - Regarding the ""Permission denied"" error when applying a size quota with setfattr command, it is recommended to review the CephFS client authentication configuration as outlined in this documentation: <https://docs.ceph.com/en/latest/cephfs/client-auth/> and <https://docs.ceph.com/en/latest/cephfs/client-auth/>|smart-link. This error might not be related to root user's permissions exactly, but more related to CephFS authentication. The user should investigate further to confirm the issue's cause. ||| To resolve this issue, register for an Anvil Support Hour session (https://www.rcac.purdue.edu/anvil/anvil-support-hour) to discuss the file limit increase request. Additionally, during simulations, generating many small files (~10MB each) will quickly reach the current file limit on their scratch space. To avoid this problem, it is suggested to archive these small files into one tarball or zip file to reduce the number of files. During post-analysis, when untarring these archives, consider using the /tmp or /dev/shm folders for temporary storage (please note that these folders are cleaned daily). To read the files without untarring, it may be beneficial to consult with a data scientist regarding the use of tools such as nektar++ (https://www.nektar.info/) for analysis. ||| To resolve the issue, check which directory the user was using for the jobs and run the command `myquota` to see the current disk usage. After identifying the directory with the problem, increase the file limit by editing the appropriate line in the `/etc/profile` or `~/.bashrc` file for the user account. The command for increasing the soft limit (S) and hard limit (H) can be written as:

```bash
ulimit -S -n <new_soft_limit>
ulimit -H -n <new_hard_limit>
```

Replace `<new_soft_limit>` and `<new_hard_limit>` with appropriate values according to the user's requirements. For example, if a soft limit of 10,000 and hard limit of 25,000 are needed, use:

```bash
ulimit -S -n 10000
ulimit -H -n 25000
```

After setting the new limits, source the user's profile file with the following command:

```bash
source ~/.bashrc
```

Now, the user should be able to write files again without encountering the file limit error. ||| Run the command `ncdu $HOME` to determine that the directories ""~/.cache/"", ""~/.conda/pkgs/"", and ""~/.local/lib/python3.8/"" are taking up significant space. Delete files in these directories safely, particularly those within ""~/.cache/"", ""~/.conda/pkgs/"", and ""~/.local/lib/python3.8/"" if all Python packages are installed in Conda environments. If the home directory becomes too crowded, install Python packages in the group shared project directory (""/anvil/projects/x-cis230283/"") instead. ||| - Rename the existing .bashrc file with the backup extension (`.bashrc.bak`). ```sh
mv ~/.bashrc ~/.bashrc.bak
```
- Create a new simple .bashrc file. - Remove the ""echo"" command from the .bashrc file if it is causing the issue. - If necessary, move any `module load` commands and other environment variable settings to job scripts or separate scripts to be sourced later. ||| The issue was resolved by deleting excess files from the user's scratch and home directories to bring the number of files below the disk quota limit. Users should monitor their usage regularly to avoid such issues in the future. ||| Our storage expert increased the file number limit on Wenliang's scratch directory to 20M and extended the block quota increase from 100TB to 500TB. These changes will remain in effect until November 1st. The user can verify their quota using the command: `myquota x-wangwl`. To ensure the security of data, it is recommended to implement a good data management workflow, as inactive files in the scratch filesystem may be purged periodically. ||| The user can extend the duration of their project space by submitting a request to their PI (Principal Investigator) or Allocation Manager. However, as per the current status, the project space has 200+ days remaining. For the scratch files located at /anvil/scratch/x-wding2, the user should consider using the Globus transfer service to save and retransfer the data when needed, as it will get purged according to the filesystem policy. ||| - To resolve the issue with the Open OnDemand gateway, attempt to log in again using this URL: https://ondemand.anvil.rcac.purdue.edu/
   - If you still encounter the error, delete the NanoString directory located at /anvil/scratch/x-mcontrerasza/NanoString and the UCD4 directory in anvil/scratch/x-mcontrerasza/ATACseq/UCD4. To do so, use the following command:
     ```
     ssh username@anvil
     cd /anvil/scratch/x-mcontrerasza
     rm -r NanoString
     cd ATACseq/UCD4
     rm -r UCD4
     ```
   - Once the directories are deleted, try accessing Open OnDemand again using the provided URL. Please reach out if you encounter any further problems. ||| Check the quota on the specified directory ({{/anvil/projects/x-mcb130189}}) to confirm the block limit. If the user encounters the error again, they should report their name and location of the error (in the terminal or any output files). It is essential to note that the usage stays under 65 TB to avoid encountering the ""No space left on device"" error again. ||| To resolve this issue, remove the {{~/.cache}} directory as it is using 2.3 GB of space. This can be achieved by running the following command:

```bash
rm -rf ~/.cache
```

Please note that this command will permanently delete the content in the .cache folder. Ensure that you do not need any data within this directory before executing the command. Additionally, try to free up some space and avoid exceeding your disk quota by regularly cleaning up your home directory or temporarily moving large files to other storage locations like project directories or scratch spaces when they are no longer needed. ||| To resolve this issue, you can follow these steps:

   a. Check the quota limits and usage for each user on the Anvil cluster by executing the following command:

      ```bash
      quota -u username
      ```
      Replace 'username' with the actual username of the affected user (in this case, x-name). b. If the user has exceeded their allocated storage limit, you can extend the limit by modifying the disk quotas using the 'edquota' command:

      ```bash
      edquota -u username
      ```
      Replace 'username' with the actual username of the affected user. This will open an editor where you can adjust the soft and hard limits for the specified user. Set the soft limit to a value less than or equal to the current usage but greater than the required additional space (20T in this case), and set the hard limit to a larger value that allows room for future work. After making the changes, save and exit the editor. c. To apply the new quotas, run the following command:

      ```bash
      quotaon -au username
      ```
      Replace 'username' with the actual username of the affected user. d. Once you have extended the storage limit, the user should be able to access their quota information and use the 'myquota' command without issues. ||| The home directory quota for user x-snajafi on Anvil has been increased from 25GB to 40GB. ||| To verify if the issue has been resolved, re-run the `myquota` command and check if the quotas for the respective file systems (home, scratch, and projects) are accessible now. If the issue persists, please reach out again. ||| The user can try removing some data from their home directory by deleting unnecessary files or moving them to the scratch or project folder. Specifically, they should remove their ~/.cache directory which is taking around 5GB and their ~/.conda directory which is taking around 9GB. If they need their conda environments, they can move them somewhere else and try activating the environment from the new path. The user can also check hidden directories in their home directory using `ls -name` or `ncdu` command to identify large files for removal. It's important to note that increasing the disk size of home and scratch directories is typically not an option. ||| To resolve the issue, the user's request has been forwarded to the storage team. The user was informed that they should receive an answer by tomorrow morning. In case of any other questions, the user is advised to reach out again. ||| To resolve the issue of excessive disk usage, bundle small files into tarballs (compressed archives) to eliminate per-file block overhead. This could save terabytes of space immediately. Additionally, the user can find small files using the command: `find . -type f -size -1k -exec ls -lh {} \; | sort -k5h`. To remove users' files after they leave the group/project, the PI of the allocation needs to grant write permission. The storage team will have to address the quotas being out of sync as a long-term solution, but in the meantime, they have doubled the project folder quota to 10TB as a workaround. If the disk quota reaches 100% again, the user should reach out to the support team for further assistance. ||| The issue has been resolved by one of our engineers. Users experiencing the same problem can try accessing their data again. ||| Run the ""myquota"" command again to check the quota status. If the issue persists, try increasing the quota temporarily or consider transferring data to a different location with sufficient space before May 12th. For detailed guidance on adjusting quotas, refer to the Purdue IT documentation: https://www.rcac.purdue.edu/docs/allocation/quota/. If necessary, join the Anvil Support Hour (https://www.rcac.purdue.edu/anvil/anvil-support-hour/) for interactive assistance."
16,0,21,21,"User requesting UI and API access for AnvilGPT with incorrect allocation number and unclear ticket subject ||| User requests access to AnvilGPT UI and API for a research collaboration project with Dr Bernie from Southern Oregon University ||| PhD candidate at Northwestern University requires UI and API access to AnvilGPT with allocation MTH240048 ||| Request for access to anvilGPT for integration with Galaxy project ||| User Request for Access to AnvilGPT Platform for Hydrological Modelling Workflows ||| Request for Anvil GPT Access ||| User needs access to AnvilGPT for testing application via both API and UI. ||| User needs assistance in generating unit tests using AnvilGPT due to lack of sudo access, and requires help finding ACCESS allocation number for AnvilGPT. ||| User has exchanged ACCESS credits for Anvil GPU SUs but account activation is still pending. ||| User requested access to AnvilGPT under allocation CIE170004 (Globus Staff) for experimentation and API tests ||| Request for access to AnvilGPT UI and API by a Purdue student with pending ACCESS account activation ||| User requests access and information about AnvilGPT service, comparing it with their experience using Jetstream2. ||| User requests ANVILGPT access allocation number and intends to use both UI and API for research queries and mathematical reasoning tests. ||| Request for Access to AnvilGPT with Allocation Number TRA120044 and Access Limited to the UI ||| User requires access to AnvilGPT UI and API for Stratolaunch hypersonics research on allocation account number cis220051 ||| Request for access to AnvilGPT for UI and API usage by RCAC employee ||| User bkeene and mfakhan seek access to both UI and API of allocation CIS240473 for comparison purposes with AnvilGPT against other solutions. ||| User needs access allocation number and details about AI projects they will use ANVILGPT for. ||| User requesting an Anvil ACCESS Explore allocation to explore Anvil's services and develop resources for a Research, Computing, and Data (RCD) professionals Research Group through CaRCC and the National AI Research Resource (NAIRR). The user also has a NAIRR allocation on Vocareum for building Jupiter notebooks. ||| User is unable to send a query along with a text file to AnvilGPT via API due to an error that the input should be a valid dictionary or object to extract name from. ||| Request for AnvilGPT access allocation number","Approve user account with username 'tmiddelk'. Update documentation on the ticket creation process to provide clear instructions for selecting appropriate boxes and specifying a subject line as ""AnvilGPT Access Request"". Provide users with the correct allocation number for their project, which appears to be ""INI200001: Startup Allocation for Cloud based workloads"". Users can find more information about this project at the provided NSF EAGER award links: [NSF EAGER Award 2436057](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2436057&HistoricalAwards=false) and [NSF EAGER Award 2436057 (alternate link)](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2436057&HistoricalAwards=false). Users can find more information on the AnvilGPT at [AnvilGPT Knowledge Base](https://www.rcac.purdue.edu/knowledge/anvil/anvilgpt) (smart-link). ||| The user's request has been approved, and they have been given access to both the UI and the API for AnvilGPT. The user can start exploring the service and contact the support team if they have any questions or need further assistance. Their ACCESS allocation is CCR190024. ||| The user has been granted access to both the User Interface (UI) and Application Programming Interface (API) for AnvilGPT. They can start exploring the service and contact support if they have any questions or need further assistance. ||| The user's request has been approved, and they have been given access to both the UI and the API for AnvilGPT. To access the service, users should use the allocated ACCESS numbers CCR160022 (for development) and MCB140147 (for overall Galaxy project). ||| The user has been granted access to both the UI (User Interface) and API (Application Programming Interface) of AnvilGPT. The project number associated with this request is EES240082. ||| The user, Yueying, has been granted access to both the UI and API for AnvilGPT. To provide more information about the intended use, the user is encouraged to reply to the question posed in the ticket message. ||| Your request has been approved, and you have been given access to both the UI and the API for AnvilGPT. To start exploring the service:
   - For API usage, refer to the official documentation at [AnvilGPT API Documentation](URL_HERE)
   - For UI usage, navigate to the AnvilGPT platform using your preferred web browser (e.g., Chrome, Firefox). The link to access the UI can be found on our website under the ""Services"" section of RCAC's main page. ||| To generate unit tests using AnvilGPT, follow these steps if you do not have sudo access:

a. Access AnvilGPT API or UI by logging in to your account with the provided credentials. b. Follow the documentation provided on the AnvilGPT website (<https://www.anvilgpt.com/documentation>) to generate unit tests and provide rationale for them. c. If you are unable to find your ACCESS allocation number, contact your system administrator or refer to any previous communications regarding your account setup. Once you have the allocation number, you can use it when working with AnvilGPT services. ||| The user should attempt to log in to AnvilGPT again. ||| The support team has granted the user access to AnvilGPT. The user should test the access immediately and report back if there are any issues. The user confirmed that they can successfully login to AnvilGPT. ||| The user has been directed to login using this link: https://purduegpt.rcac.purdue.edu/ Accessing that will create a pending request for review and approval. The user is advised to check back once the official launch of AnvilGPT occurs in the coming weeks. In the meantime, the user may want to consider utilizing PurdueGPT which is currently available for beta users: https://purduegpt.rcac.purdue.edu/ ||| The user has been granted access to both the UI and API for AnvilGPT. They are encouraged to explore the service and seek assistance if needed. No specific technical details were provided in this ticket message. ||| To continue using ANVILGPT, an access allocation number is now mandatory. However, it's strongly recommended to transition to PurdueGENAI Studio instead, which provides similar capabilities and is specifically designed for Purdue users. Access the platform using your Purdue credentials at this link: https://genai.rcac.purdue.edu/ ||| To grant access to AnvilGPT with allocation number TRA120044 and limited to the UI, follow these steps:

    a. Login to the HPC system

    b. Load the necessary module for AnvilGPT (e.g., `module load anvilgpt`)

    c. Run the following command to set your allocation ID: `anvilgpt_allocation_set TRA120044`

    d. Launch the AnvilGPT UI with the appropriate command (e.g., `anvilgpt_ui`)

    Please note that you may need to replace the module load command and UI launch command with the correct ones for your specific HPC system. If you require assistance with this, please consult the system documentation or reach out to the HPC support team. ||| The user has been granted access to both the AnvilGPT UI and API. For documentation on how to use the API, please refer to the following link: [https://www.rcac.purdue.edu/knowledge/anvil/anvilgpt/api](https://www.rcac.purdue.edu/knowledge/anvil/anvilgpt/api) ||| Your request for access to AnvilGPT has been approved. You may now utilize both the user interface (UI) and API for coding purposes. For any questions or further assistance, please do not hesitate to contact us. [No specific technical details were provided in this message] ||| Provide access credentials for both the User Interface (UI) and Application Programming Interface (API) for allocation CIS240473 to users bkeene and mfakhan. Inform them that they can start using these resources, and advise them to reach out if they have any questions. ||| To obtain a user's access allocation number and learn about their intended AI projects using ANVILGPT, prompt the user to provide that information. This information can be obtained through either the HPC UI or API. There is no specific resolution required as it involves gathering user input. ||| To obtain an Anvil ACCESS Explore allocation, the user should visit the ACCESS website and submit a short proposal to ACCESS. Once approved, they will receive credits which can be transferred to Anvil Service Units (SU). For more information about different types of allocations, visit [https://allocations.access-ci.org/project-types](https://allocations.access-ci.org/project-types|smart-link). Regarding the NAIRR allocation, if it is associated with Purdue Anvil, the user should share their NAIRR project number. ||| The user's bash script is using the incorrect format for sending data in the API request. Instead of using -F ""file=@tests/test.txt"", the user should enclose their data in a JSON structure and set the Content-Type header accordingly. Here's an example using `jq` for creating a JSON object from the command line:

```bash
cat tests/test.txt | jq -R --arg query ""Test query"" '{query, (. as $file)}' | curl -X POST -H ""Content-Type: application/json"" -H ""Authorization: Bearer $ANVILGPT_JWT_TOKEN"" https://anvilgpt.rcac.purdue.edu/ollama/api/generate -d @-
```

In this example, the query and file contents are combined into a JSON object using `jq`. The pipe (|) symbol sends the output from `jq` to `curl`, which then sends the request to the API. Replace `$ANVILGPT_JWT_TOKEN` with the actual token obtained for authentication. If you don't have `jq` installed, you can create a JSON object manually and use that in your curl command:

```bash
cat tests/test.txt | tr -d '\r' > tmp_file.json
cat << EOF >> tmp_file.json
{
    ""query"": ""Test query"",
    ""file"": {
        ""content"": `cat tmp_file.txt`,
        ""filename"": ""test.txt""
    }
}
EOF
curl -X POST -H ""Content-Type: application/json"" -H ""Authorization: Bearer $ANVILGPT_JWT_TOKEN"" https://anvilgpt.rcac.purdue.edu/ollama/api/generate -d @tmp_file.json
``` ||| To obtain an access allocation number for AnvilGPT, no current project is required as the user is part of the ACCESS Allocations team. The user should now have access to ANVILGPT. If the user encounters any other issues or has additional questions, they should reach out again."
